var tipuesearch = {"pages":[{"title":"マネジメント1年目振り返り on 2024-12","text":"この文章はもともとはローカルで自分向けに書いていたもので、dumpしているだけのもの。 そのうちマネージャー業務のtopicごとに、自分なりにやっていることや、考えていることを書き起こしたい。 マネージャーになった ここでいうマネージャーとは、少人数のメンバーのマネージャーだったり、チームレベルのマネージャーで、 マネージャーとシニアマネージャーの違いとは | Indeed (インディード) あたりがイメージに近いかも。 現状の立ち位置: 2つのチームのマネージャーを兼務している - SREチームのマネージャーのうちの1人 - 近いところに同じレイヤーのマネージャーがいる - embedded SREのチームの一部とSRE CoE的なチームを担当 - 共通クラウドインフラチームのマネージャー - 2024年の途中から兼務 - \"共通クラウドインフラ\"でイメージされるような業務をいくつかのチームで分担していて、そのうちの1つのチーム - しかし、ここではスクラムのPOも担っている(暗黙的ではなく、明示的に兼務している) マネージャーになる前のイメージと実際の違い なんとなくこれまでのマネージャーの様子を見ていて、マネージャーの仕事についてイメージしていたことと、実際に今やっている業務には大きな乖離はない。ただ、やはり一部想定と違うところもある。 これは今の組織の話なので、一般論ではないものもあるが、 - イメージ: いろいろな幅広いプロジェクトや組織の方針を知り、その決定や実行に関わる・影響を与えることができる - 実際: できるが、マネージャーレベルでできないことも多い - できないこととしては、自分の組織外の方針は特に知ることはそんなに多くはなく、より上位のマネジメントしか呼ばれないことも多そう。通常のメンバーと同じように周知されたものを聞くしかない - イメージ: 個人としてではなくて、チームとして成果を出すことを求められる - 実際: 概ね想定通り - ただマネージしているチームだけでやるのは難しい場合も多く、チームとして、というよりは組織としてやるべきだ、と思うことを進める力を持つ必要があると感じる。調整力が必要。 - イメージ: メンバーの評価やフィードバックをして、育てたり成長機会を与えたりする必要がある - 実際: 字面ではその通りだが、想像以上に他のマネージャーはメンバー・チームの成長について時間を割いて考えていた。(昔の自分からはそう見えていなかった、ということは、今の自分もメンバーからはそう見えていないということだと思う) - 評価については、一貫性がとても大事なんだ、と思っていたが、パラメータが多すぎて難しく、できていないもあるし、一貫性よりも評価される人本人との対話のほうが大事なのかも?という疑問も湧いてきた。人によって評価してほしいことも違う。ただ、相対評価という意味では、確実にこういう理由でこの人とこの人は違う評価なのです、という説明ができる程度に自分のなかで言語化している(し、それは本人にフィードバックするためにも大事) - イメージ: コミュニケーションに気を使う必要がある。そのため、自分のような人と話すことが本質的に苦手な人間には難しいのでは? - 実際: 本当にそう。でも、みんな優しいのでなんとかなっている。一方で、人によってコミュニケーションの好みが違うというところは意識するようになってきてはいる。実践できてはいない - イメージ: 複雑なコミュニケーションをやる必要がある - 実際: 少し意思決定を含むものや別部署とのコミュニケーションで頼られることが増えた。意思決定についてはサイズ感など次第で任せていくべきものだと思う。 マネージャーになって変えたこと/変わったこと 1on1の時間を曜日や枠を決めて、確保するようになった 1on1の数が多いので、曜日や時間帯をまとめて確保するようにした 今は火/水/木の午後がメイン 1on1の中で本人が思っている最近の成果を書いてもらうようになった もともとは評価の時期にお互いに忘れている成果がないようにしたいので、1on1のドキュメントに書いてもらうことにしたもの。兼務が増えてから詳細に業務を把握することが難しくなったので、各メンバーの関連するプロジェクトの進捗についての解像度も上がるし、その進め方についてのフィードバックをする際にも便利になった たまにJIRAと見比べて、書いていないものを聞いてみたりして、(忘れていただけ、という回答が多いが)詳細を聞く機会にしている 実際、中間フィードバックの時期に眺めて、この成果は今こういうことにつながっている、という理解に役立っている 意識的に自分自身の振り返りをしている 2週間~1ヶ月間隔で、自分のうまくできたこと、改善できること、今後やるべきことを振り返っている エンジニアリングはあまり自覚的に振り返りの機会を持たなかったし、機会を持たなくても何となく試行錯誤の結果がわかるタイミングもあると思っている(できるなら、振り返りはした方が良いとは思う)。ただ、マネジメントは正解・不正解がすぐにわからないし、正解もあるのかわからないことも多いので、意識的なイテレーションで改善をしないとすぐに崩壊してしまう気がした。メンバーに対する責任もあるので、崩壊しちゃった😅、では済まないところもある。 自分にマネジメントが向いていないと思っているというのもあるのかも 他に変えたことも多いが、マネージャーとPOを兼務して、両方の帽子でプロジェクトに関わっていたり、チームメンバーとコミュニケーションしている面があるので、もう少しどちらの立場としてやっているのか整理してからまとめたい。 メンタル的な面では、 - あまり業務の詳細な進め方に立ち入らなくなった - 困っていない限りは好きにしてくれ、という気持ちはもともとあるが、Aの進め方とBの進め方があったときに、以前は自分と違う進め方ならPros/Cons聞くこともあったと思う - 今は重大な問題にならなそうであれば、まずやってみてもらって、そのうち感想を聞こうという気持ちになった。 - ただ、お手並み拝見になりすぎないように気をつけている。マネージャーがごちゃごちゃいうと強制力があり自主性を失うと思うが、だからと言って正解の選択肢があることを隠したり、というのは違うので。コーチングとのバランスが大事だと思っている マネージャーをやってどうだったか? まず全体としてやって良かった。 マネージャーになるか、ICになるか、おそらく選択できる立場にあったが、マネージャーを一度は経験したかったということに加えて、当時のSRE組織にもっとマネージャーの頭数が必要で、自分が適切な人が見回しても限られていたので、マネージャーになった。 そのため、少し広い範囲を領域を見る機会をもらえたし、サポートもしてもらえた。 またSRE組織は長らくEmbedded SREだけ(というか、昔はスクラムに参加しながら横断的な改善も各自空いた時間でやっていこう、という文化だった)だったが、SRE CoE的なチームを作ることができたのが、一番良かった。ただ、これは自分が作ったというよりは上司の力が大きい。ずっと作りたい、作るべきだと言って、いくつかプランやアイテムを書いたりはしたが、上位マネジメントなどと話したり、そのための将来像の説明などは上司が行っている。こういう調整含め、やっていく必要があるし、パスを持つ必要がある。 とはいえ、チームとして成果を出し、capabilityを拡大していくことは目指していきたい。 またできていないこととしては、中期方針の策定に入り込むことや、技術マネジメントのフレームを作ることだったり、まだまだ足りないことがある。 あとは社外との交流もしていかなければな、という気持ちがある。どうしてもマネジメントは個別的な話と、普遍的な話が入り混じりがちで、他社のケースについて聞くことで、新しい普遍性に気づけるような気がしている。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2024/12/14/0200.html","loc":"https://www.hitsumabushi.org/blog/2024/12/14/0200.html"},{"title":"CloudFormation Stackを安全に削除する","text":"以前、 CloudFormation の不満点 というのを書いたが、諦めて大半のCfnをterraformに移行した。 その際、cfn リソースをきれいにするために、cfn stackを安全に削除する必要があり、その方法をメモしておく。 概要 アイデアは簡単で以下の通り。 cfn の操作のみが許可されているIAM roleを作成する 上記roleを指定して cfn stackをdeleteする モチベーション そもそも以前の記事でも、予期せぬ手作業の変更があった場合に、ドリフト検出やChange setで検出・対応が難しいことを問題としていた。 cfn stackを削除しようと思った場合、ドリフトしている状態だと、単純には削除できない。一旦cfn updateすれば簡単かもしれないが、何か手作業で変更されているかもしれない状況だと、updateしたくない。 cfn updateをせずに、安全にcfn stackをdeleteしたい。 方法 cfnの操作しかできないIAM roleを作成する 以下の通り、terraformで作成した。 resource \"aws_iam_role\" \"delete_cfn_stack\" { name = \"delete-cfn-stack\" path = \"/\" assume_role_policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { Action = \"sts:AssumeRole\" Effect = \"Allow\" Sid = \"\" Principal = { Service = \"cloudformation.amazonaws.com\" } }, ] }) } resource \"aws_iam_role_policy\" \"delete_cfn_stack_policy\" { name = \"cfn_policy\" role = aws_iam_role.delete_cfn_stack.id policy = jsonencode ({ Version = \"2012-10-17\" Statement = [ { Effect = \"Allow\" Action = [ \"cloudformation:*\" , ] Resource = \"*\" }, ] }) } 上記のroleでスタック削除する export STACK_NAME = stack_name # 失敗するdeleteを実行 aws cloudformation delete-stack --role-arn arn:aws:iam::_aws_account_id_:role/delete-cfn-stack --stack-name ${ STACK_NAME } # resource idのリストを取得 aws cloudformation describe-stack-resources --stack-name ${ STACK_NAME } | jq '.StackResources[].LogicalResourceId' | xargs # 削除しないリソースとして上の結果を指定 aws cloudformation delete-stack --role-arn arn:aws:iam::_aws_account_id_:role/delete-cfn-stack --stack-name ${ STACK_NAME } --retain-resources ( ↑の結果をペースト ) 終わり 地道にterraform importしてから、上記をコツコツ(スクリプト化して)実行して、cfn stackを削除した。 terraform importは単純に差分がなくなるまでtf fileを修正するだけなので、そんなに困ることはない。 これで安心・安全にリソース管理できるようになった。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2021/10/12/0200.html","loc":"https://www.hitsumabushi.org/blog/2021/10/12/0200.html"},{"title":"1Password CLIを使ってTOTPを取得する","text":"2要素認証を必須にしたAWSのswitch roleで、temporary credentialsをCLIで取得したかった。 TOTPの数字を毎回調べるのが面倒で、簡単にできないか調べたところ、1Password CLIというのがあった。1Passwordユーザなので、これを利用する。 ただし、当然のことだが、2要素認証のデバイスとして、1Passwordが利用されている状況とする。 1Password CLIの初期設定 1Password CLI を利用できるようにする。 1Password CLIのGetting Started を見て、初期設定する。 $ op signin _1password_url_ _signin_address_ ... # Secret Key, Password, (設定していればTOTPの6-digit)を聞かれる 短縮形が _1password_url_ から自動的に決まるが、どうしても指定したい場合には、 --shorthand オプションで指定する。 TOTPの取得 通常の1password appのように、30minアクセスがなければ、ロックされるため、パスワードの入力が必要。 1passwordのURLが example.1password.com であれば、 $ eval $( op signin example ) として、再認証する。 環境変数 OP_SESSION_example に認証したときのsession情報が載る。( --session オプションで各コマンドごとに渡しても良い) TOTPを取得するには、以下のコマンドを実行する。 $ op get totp \"UUID or Name\" UUIDで指定したい場合には、 op list items --categories Login の結果をjqなどで調べて、取得する。 終わり 以上で概ねやりたかった、CLIでTOTPを取得することができるようになった。 最近1PasswordはLinux Desktop向けのアプリを出していたりしているので、もう少し真面目に使っていきたい所存。 ちなみに、複数のプロファイルの切り替えなどで便利なように、 99designs/aws-vault というのもあるらしいが、特に使わずに自前シェルスクリプトで使っている。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2021/10/12/0100.html","loc":"https://www.hitsumabushi.org/blog/2021/10/12/0100.html"},{"title":"Python でコメント付きYAMLを扱う時には ruamel.yaml が便利だった","text":"資料 Document: ruamel.yaml Repository: ruamel.yaml 背景 とある yamlで書かれたconfigファイル群(数百ファイル)を一括で変更したいことがあった。 sedで変更するには少し難しかったので、パースしてから条件判定して、書き換えたい。 コメントは消したくない ブロックスタイルのままにしたい diff を最小限にしたい (細かい中身を知らないので、チェックするのが面倒) ruamel.yaml python で yaml を扱うときは、PyYAML が有名だと思う。 PyYAMLを使う場合、yamlをloadしてdumpすると、フロースタイルなのは変更できるが、 コメントは消えてしまうのに対応するのが簡単ではない(と思っている)。 ruamel.yaml はPyYAMLをフォークしたもので、YAML 1.2 をサポートしているし、コメントやスタイル、キーの順番を保つloaderが実装されている。 https://yaml.readthedocs.io/en/latest/overview.html 使い方 使い方としては、 load, dumpの代わりに、 round_trip_load , round_trip_dump を使えば良い。 オプションは自分が使っているconfigに合わせて使えば良い。 import ruamel.yaml import os def process ( filepath ): with open ( filepath , 'r+' ) as f : data = ruamel . yaml . round_trip_load ( f , preserve_quotes = True ) # 必要な処理をする # data[\"foo\"] = \"bar\" if rewrite : f . seek ( 0 ) ruamel . yaml . round_trip_dump ( data , f , explicit_start = True ) f . truncate () for pathname , dirnames , filenames in os . walk ( '.' ): for filename in filenames : print ( f \" { pathname } / { filename } \" ) process ( f \" { pathname } / { filename } \" )","tags":"blog","url":"https://www.hitsumabushi.org/blog/2019/02/12/1630.html","loc":"https://www.hitsumabushi.org/blog/2019/02/12/1630.html"},{"title":"CloudFormation の不満点","text":"以下では「手作業で」というのは、「CloudFormation管理外で」という意味で使う。 要点 CloudFormation は、リソースが何かの理由で手作業で変更されていた場合に安全に操作できない Drift 検出は誤検出が多すぎて使いづらい Change Set は動いている状態との差分を見ていないので、信用できない 経緯 CloudFormation で管理されているリソースすべてについて、新しいタグをつけたくなった。 ただ、各サービスの担当者ごとにある程度自由にオペレーションできるため、手動で変更されていないか、一応調べておこうと思い、 2018-11 にリリースされたドリフト検出 を使って、 手作業で実施された変更点もCloudFormationに取り込みつつ、対応しようとした。 期待したこと CloudFormation のスタックがたくさんあるため、1つ1つ細かく差分を見て修正することが難しい ドリフト検出されたところだけ1つずつ修正していって、Change Setを作りながら現状に合わせたい 実際 ドリフト検出 サポートされるリソースが少ない Resources that Support Drift Detection そもそも誤検出が多い Detecting Unmanaged Configuration Changes to Stacks and Resources LBのプロパティなど、配列で定義されるところをデフォルト値で埋められただけで、ドリフトが検出される エッジケースの(とAWSが主張している)誤検出に該当するのか、目で見て考える以外の方法がない Change Set 実際に動いているシステムとの差分を見ていない 前回の設定とのスタックの差分を見ている 手作業で変更されているとChange Setの実行に失敗する(実際には差分がないので) その場合 Update Stack するしかない 例えば、手作業でタグを増やしたとして、翌営業日にCloudFormationに反映したい、と思った場合、 ドリフト検出で誤検知を目でフィルタしつつ(誤検知だと確信が持てるかはわからない)、 CloudFormation のパラメータを書き換えて、Update Stackしないといけない。 (一応、Update Stack 前に Change Set で差分を見て、execute して失敗することを確認したほうが安心感はある。) まとめ 手作業の変更をできないようにしておく以外にない。 あるいは、本格的にCloudFormationを利用し始めるまでの間に Terraform に移行する。 (なんでChange SetをRunnning Stateとの比較にしなかったんだろう...)","tags":"blog","url":"https://www.hitsumabushi.org/blog/2019/01/25/1454.html","loc":"https://www.hitsumabushi.org/blog/2019/01/25/1454.html"},{"title":"DebianでLuaJITTeXを使いたい","text":"そろそろLuaTex使ってみたいなと思ったところ、LuaJITTeXの方が早い場合があるということで、試してみようと思った。 参考 luajittex のセットアップ そもそもこのサイトの情報が有用 TeX Live を使おう──主に Linux ユーザのために── 未解決の問題 /var/lib/texmf/fmtutil.cnf-TEXLIVEDIST が自動生成されている、とコメントされているが、元ファイルがわからない ### This file was automatically generated by update-fmtutil. # # Any local change will be overwritten. Please see the documentation # of updmap on how to override things from here. # ### 準備 パッケージインストール sudo apt install texlive-lang-japanese texlive-latex-extra texlive-luatex # 参考文献とか書くとき sudo apt install texlive-bibtex-extra biber /var/lib/texmf/fmtutil.cnf-TEXLIVEDIST を編集したい。(未解決の項目に書いた通り、元ファイルがわからないので直接編集している) @@ -6,7 +6,7 @@ ### dviluatex luatex language.def,language.dat.lua dviluatex.ini etex pdftex language.def -translate-file=cp227.tcx *etex.ini -#! luajittex luajittex language.def,language.dat.lua luatex.ini +luajittex luajittex language.def,language.dat.lua luatex.ini luatex luatex language.def,language.dat.lua luatex.ini mf mf-nowin - -translate-file=cp227.tcx mf.ini pdfetex pdftex language.def -translate-file=cp227.tcx *pdfetex.ini @@ -14,7 +14,7 @@ tex tex - tex.ini dvilualatex luatex language.dat,language.dat.lua dvilualatex.ini latex pdftex language.dat -translate-file=cp227.tcx *latex.ini -#! luajitlatex luajittex language.dat,language.dat.lua lualatex.ini +luajitlatex luajittex language.dat,language.dat.lua lualatex.ini lualatex luatex language.dat,language.dat.lua lualatex.ini mptopdf pdftex - -translate-file=cp227.tcx mptopdf.tex pdflatex pdftex language.dat -translate-file=cp227.tcx *pdflatex.ini 使い方 luajittex --fmt = luajitlatex.fmt { tex file } 問題なくコンパイルできれば、pdfが生成されるはず。 VSCodeでTeXする LaTeX Workshop をインストールして、以下の設定をする。 参考文献が必要なときは、 luajittex_with_bib のレシピでビルドするように書いている。 { \"latex-workshop.chktex.enabled\" : true , \"latex-workshop.latex.recipes\" : [ { \"name\" : \"luajittex\" , \"tools\" : [ \"luajitlatex\" ] }, { \"name\" : \"luajittex_with_bib\" , \"tools\" : [ \"luajitlatex\" , \"biber\" , \"luajitlatex\" , \"luajitlatex\" ] }, ], \"latex-workshop.latex.tools\" : [ { \"name\" : \"luajitlatex\" , \"command\" : \"luajittex\" , \"args\" : [ \"--cmdx\" , \"--synctex=1\" , \"--fmt=luajitlatex.fmt\" , \"%DOCFILE%\" ] }, { \"name\" : \"biber\" , \"command\" : \"biber\" , \"args\" : [ \"%DOCFILE%\" ] } ], \"latex-workshop.intellisense.surroundCommand.enabled\" : true , \"latex-workshop.synctex.afterBuild.enabled\" : true , } その他実際のTeXを書く時に参考になりそうなもののメモ 参考文献まわり BibTeXの書誌情報の書き方知りたいとき: Wikipedia 参考文献を探したい時: Google Scholar","tags":"blog","url":"https://www.hitsumabushi.org/blog/2018/10/31/1901.html","loc":"https://www.hitsumabushi.org/blog/2018/10/31/1901.html"},{"title":"Debian でログインシェルをzshにしている人が snappy を使う場合の注意","text":"Snappy について Snappy snapcraft Canonical が主導しているパッケージシステムで、Universal Linux Package と銘打つように、ポータブルなパッケージングができそうな感じ。 ポータビリティを上げるために、dockerみたいな感じで、依存ライブラリも全部パッケージに含めてしまうスタイルなので、多少debパッケージよりは大きくなる。 その分、sidを使っているとよく起きる、共通ライブラリの依存バージョンの不整合がおきる、という問題は起きない。 snapcraftのページではパッケージングの方法も紹介されているので、配布したい人自身がsnapパッケージを作りやすいはず。 少なくとも、ディストーションごとのパッケージを各アプリケーション作成者がやるよりは、遥かにやりやすい。 自動アップデートとかもあるので、サービス提供者が利用者に常に最新版を使ってもらいたい場合などにメリットもある。 実際、 Ubuntu以外にも、Debian, Arch Linux, Gentoo, Fedora, open SUSE などでも利用できる。 Snappy は、snapd というAPIデーモンが動くことになっている。 snappy のインストール $ sudo apt install snapd snap パッケージのインストール $ sudo snap install hello-world 困ったこと Snappy でインストールしたアプリケーションへのパスが通らない Snappy でインストールしたデスクトップアプリケーションが、menuに登録されない 調べたこと バージョン Kernel はpinningしているので、少し古い。 $ snap version snap 2 .32.3 snapd 2 .32.3 series 16 debian kernel 4 .9.0-3-amd64 snappy でインストールした場合のパス 実態はバージョンごとに、 /snap/_app_name_/_version_/ 以下に置かれる。現在利用しているものへのリンクは /snap/_app_name_/current/ コマンドは、 /snap/bin/ 以下にコピーされる。 デスクトップエントリのファイルは /var/lib/snapd/desktop/applications/*.desktop というファイルが作成される パスの設定方法について /etc/profile.d/apps-bin-path.sh にパスを設定するためのスクリプトが置かれている。 $ cat /etc/profile.d/apps-bin-path.sh #!/bin/sh --this-shebang-is-just-here-to-inform-shellcheck-- # Expand $PATH to include the directory where snappy applications go. if [ \" ${ PATH #*/snap/bin } \" = \" ${ PATH } \" ] ; then export PATH = $PATH :/snap/bin fi # desktop files (used by desktop environments within both X11 and Wayland) are # looked for in XDG_DATA_DIRS; make sure it includes the relevant directory for # snappy applications' desktop files. if [ \" ${ XDG_DATA_DIRS #*/snapd/desktop } \" = \" ${ XDG_DATA_DIRS } \" ] ; then export XDG_DATA_DIRS = \" ${ XDG_DATA_DIRS :- /usr/local/share:/usr/share } :/var/lib/snapd/desktop\" fi zsh では /etc/profile などを読まないので、パスが設定できていない。 /etc/zsh/zprofile に、上記の内容を書いて置けば良い。 あるいは、 /etc/zsh/zprofile で以下のようにする。 for i in /etc/profile.d/*.sh ; do [ -r $i ] && source $i done","tags":"blog","url":"https://www.hitsumabushi.org/blog/2018/04/11/1058.html","loc":"https://www.hitsumabushi.org/blog/2018/04/11/1058.html"},{"title":"PyPIへパッケージをアップロードする","text":"資料 パッケージ構成については github.com/pypa/sampleproject https://pypi.python.org/pypi/twine https://packaging.python.org/tutorials/distributing-packages/ 手順 PyPIへユーザー登録する PyPI には普段使われている本番環境とは別に、テスト環境がある。 アカウントがそれぞれ独立しているので、両方で作成する必要がある。 PyPI PyPI Test .pypirc の作成 以下のように ~/.pypirc を作成して、test 環境を利用できるようにしておく。 平分でパスワードを書くことになるので、最低限パーミッションを変えておくことにする。 $ cat ~/ . pypirc [ distutils ] index - servers = pypi testpypi [ pypi ] repository = https : // upload . pypi . org / legacy / username = _username_ password = _password_ [ testpypi ] repository = https : // test . pypi . org / legacy / username = _username_ password = _password_ $ chmod 0600 ~/ . pypirc パッケージのアップロード コマンドのインストール $ pip install twine パッケージング前のチェック $ pip install check-manifest $ check-manifest パッケージング $ pip install wheel $ python setup.py sdist bdist_wheel Test 環境へアップロード $ twine upload - r testpypi dist /* pip でインストールするには、以下のようにする。 $ pip install --index-url https://test.pypi.org/simple/ _package_ 本番へアップロード $ twine upload dist /*","tags":"blog","url":"https://www.hitsumabushi.org/blog/2018/04/05/1420.html","loc":"https://www.hitsumabushi.org/blog/2018/04/05/1420.html"},{"title":"Google Cloud Pub/Sub をGolangから使おうとしてハマったことまとめ","text":"概要 Google Pub/Sub を GoのSDK から使おうとしていました。 やっているといくつか詰まったので、メモしておきます。 サービスアカウントを利用するためにCredentials JSONを指定する サブスクリプションの Pub/Sub サブスクライバー権限 を与えても Permission Denied になる サービスアカウントを利用するためにCredentials JSONを指定する 権限の都合上、サービスアカウントのCredentials JSONを利用して認証したい、という要件がありました。 ドキュメントを見ていると、 ADC(Application Default Credentials) を利用して認証している場合が多いです。 これを使う場合、 GOOGLE_APPLICATION_CREDENTIALS という環境変数が設定されていれば、そのファイルを読んでくれるのですが、今回の要件では複数の Credentials を利用したかったので、Go プログラム中で指定する必要がありました。 結論としては、以下のような形でpubsub client を作るときに認証情報を渡すことができます。 jsonKey , err := ioutil . ReadFile ( credentialJSONPath ) conf , err := google . JWTConfigFromJSON ( jsonKey , pubsub . ScopePubSub , pubsub . ScopeCloudPlatform ) if err != nil { log . Fatal ( err ) } ctx := context . Background () ts := conf . TokenSource ( ctx ) c , err := pubsub . NewClient ( ctx , projectID , option . WithTokenSource ( ts )) Publisherのサンプルは以下の通りです。 サブスクリプションの Pub/Sub サブスクライバー 権限を与えても Permission Denied になる 結論としては、 Pub/Sub サブスクライバー 権限に加えて、 Pub/Sub 閲覧者 の権限が必要でした。 ドキュメント上は、 Pub/Sub サブスクライバー 権限だけで良さそうに見えますが、権限不足だったようでした。 Subscriberのサンプルは以下の通りです。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2018/02/05/2009.html","loc":"https://www.hitsumabushi.org/blog/2018/02/05/2009.html"},{"title":"さくらのクラウドでN百台を管理するためにterraformとansibleを使っている話","text":"これは、 さくらインターネット Advent Calendar 2017 として書いた 記事 です。 さくらインターネットでは、今年4月からIoTプラットフォームの sakura.io をサービス提供しています。 sakura.io は、さくらのクラウド上で本番・検証環境を構築しており、数百台のサーバーを利用しています。 私はリリース直前にチームに参加し、開発の傍ら運用改善活動をしていました。 その結果としてTerraform を導入し、Terraform (+ Terraform for さくらのクラウド) + Ansible で運用することになりました。 導入までの課題と、どのように導入・利用しているのか、について書きたいと思います。 運用の課題 検証環境と本番環境で構成に差分があり、それに気づきづらい mesos+marathon を利用したコンテナ実行環境を使ったマイクロサービスアーキテクチャになっていて、それ以外にも、redis, memcached, メッセージキューなどいろいろなコンポーネントがあります。 そのため、個別の開発者がクリーンな環境を気軽に用意しづらいです。 インフラの構成を変更したいと思ったとき、個別の開発者がクリーンな環境を個別に作るのは手間がかかってしまうため、直接検証環境でテストしがちです。その際に、行われた変更がそのまま残ってしまい、本番環境との差が残ってしまっているケースがありました。 Ansible Playbook の内容と現実に差分がある もともとOS内の設定はansibleで管理していたのですが、対象ホストの増加、ansible playbookの肥大化に伴い、実行時間が増えてしまいました。 それに伴い、 --limit , --tags をつけて部分的に実行するようになり、playbook と差分が散見されるようになりました。 方針 ansible は全部実行しろ、などとルールを作ることは簡単なのですが、上記に書いたとおり、心理的なハードルによって発生している問題だと思いました。そこでルール化することはエンジニアリングの敗北という感じがしたので、差分がわかるように構成管理することと、差分を定期的にチェックして通知することに集中することにしました。 構成管理として、OS内の部分はansibleを利用していたためそのまま利用し、インフラの構成管理としては、ちょうど Terraform for さくらのクラウド もあったため、terraformを利用することにしました。 差分の検知としては、terraform plan, ansible check mode を定期的に実行することにしました。 terraform 導入のために terraform のフォルダ構成 他のクラウドでのベストプラクティス を参考に、フォルダ構成は以下のようにしています。 terraform plan, apply などする場合には、 pod-xxx ディレクトリ配下で実行します。 . ├── module │ └── sakuracloud │ ├── compute │ │ ├── compute.tf // compute のエントリーポイント。各roleのモジュールへ変数を渡す │ │ ├── role-hoge // role ごとに作成 │ │ │ └── role-hoge.tf // util の base-wrapperへ変数を渡す。role固有の変数変換はここでやる │ │ └── util // 他のroleから読まれる │ │ ├── base-wrapper // instance と service-dns を呼ぶためのやつ。他のroleからはこいつを見る │ │ ├── instance // 個別のサーバーの、サーバー/ディスク/DNSレコードの作成 │ │ └── service-dns // roleごとに、DNSレコードがある場合(VIPに紐づくレコードなど) │ ├── dns // さくらのクラウドのドメイン/DNSレコードを管理するために利用 │ │ ├── dns.tf │ │ └── example │ │ ├── example.tf │ │ └── output.tf │ ├── gslb // さくらのクラウドのGSLBを管理するために利用 │ │ ├── api │ │ │ └── api.tf │ │ └── gslb.tf │ ├── network // さくらのクラウドのスイッチ/ルーターを管理するために利用 │ │ ├── internal │ │ │ ├── internal.tf │ │ │ └── output.tf │ │ ├── network.tf │ │ └── output.tf │ └── simple-monitor // さくらのクラウドのシンプル監視を管理するために利用 │ └── simple_monitor.tf └── providers └── sakuracloud └── pod-xxx // システム単位ごとに作る ├── pod-xxx.tf // compute.tf, dns.tf, ... などのモジュールごとのエントリーポイントへ変数を渡す ├── terraform.tfstate.d // tfstate。env (今で言うworkspace) で本番と検証環境をわけてる。 │ ├── dev │ └── ... └── terraform.tfvars // 変数をひたすら書く。具体的な値は全てここに書かれているはず。 大体はさくらのクラウドのコンポーネントごとに、moduleを作っているのですが、compute のところは大きく変更しています。 サーバー作成と同時に、ディスクの作成や、DNSレコードの作成を行うためです。 例えば、role-hoge というロールが2台あるとき、tfvar中では以下のような変数を作っています。 sakuracloud_dns_foo = { dev . zone = \"dev.example.com\" staging . zone = \"staging.example.com\" } sakuracloud_role_hoge = { dev . ipaddresses = [ \"192.168.0.10\" , \"192.168.0.11\" ] dev . server_tags = [ \"hoge\" , \"develop\" , \"__with_sacloud_inventory\" , \"starred\" ] dev . disk_tags = [ \"hoge\" ] staging . ipaddresses = [ \"192.168.1.10\" , \"192.168.1.11\" ] staging . server_tags = [ \"hoge\" , \"staging\" , \"__with_sacloud_inventory\" , \"starred\" ] staging . disk_tags = [ \"hoge\" ] これを使って、dev環境でサーバー作成したとき サーバー名をFQDNに一致させる hoge-01.dev.example.com , hoge-02.dev.example.com DNSレコードも合わせて作成する Aレコードとして hoge-01.dev.example.com は192.168.0.10, hoge-02.dev.example.com は192.168.0.11 に対応させる サーバータグとして、ansibleのroleをつける hoge , develop , staging がansible側で利用しているロール名です サーバータグとして、 __with_sacloud_inventory をつける 一応、terraform 管理外のリソースを許すためにつけています サーバータグとして、 @group=a などのグループタグをつける 1つ目のサーバーはa、2つ目はb、...4つ目はd、5つ目はaなどとしています 既存リソースをterraformにimport 既存のリソースをインポートします。形式としては、以下のような形式です。 terraform import 'module.hoge.module.base-instance.sakuracloud_server.base[0]' _リソースID_ 注意点としては、tfファイルを書いたあと、terraform planすれば、何をインポートしないといけないかわかるのですが、 その表示では、 module.hoge.base-instance.sakuracloud_server.base[0] というように、途中にmoduleがない形式で表示されます。 import するには都度moduleを書く必要があります。 また、さくらのクラウドにはリソースIDがないけど、terraform で管理できるものがあります。例えばDNSレコードがそうです。 この場合、リソースIDに当たるものを自分で作成する必要があります。 どのように生成するかは、 terraform for さくらのクラウドのソースを見ればよいのですが、参考までにgolangで生成する例を載せておきます。 https://play.golang.org/p/OvQw6BdxVf ansible の dynamic inventory のスクリプト作成 上記のような sacloud_inventory.py スクリプトをansibleのリポジトリに実行権限をつけて用意しています。 それにより、 ansible-playbook -i sacloud_inventory.py ... とすることで、インベントリファイルを書くことなくansibleを実行できます。 このスクリプトで、さくらのクラウドのAPIを叩くところはすべて、 usacloud におまかせしています。 先程の terraform の例であったように、 __with_sacloud_inventory が入っていないものは全部無視し、タグは全てグループ名にすることにしています。 (13-22行目は、 Ansible TowerのOSS版であるAWX を試しているために入れています。) もしかしたら、さくらのクラウドの特殊タグである @group=a などは除いた方が使いやすいかもしれませんが、監視側からも見るために現状はすべて入れています。 ansible 側の準備 ansible-playbook -i sacloud_inventory.py site.yml --check をしたいのですが、しばしば check modeで実行できていないplaybookや、変更がないのにchangedにしているplaybookがあります。これを直しましょう。 check modeで実行できていない、典型的なものとしては以下のようなものがあります。 別のtaskの実行結果を参照している register を使っているタスクAの結果を参照しているタスクBがある場合、check mode 中でAが実行されないため、タスクBのcheck実行時に変数が参照できないエラーが出ます。 これを防ぐには、register を使っているタスクAに check_mode: no をつけることです。 check modeでも本当に実行されるようになるため、例えば、ファイルの存在だけを確認している場合など、副作用がない場合のみ、 check_mode: no をつけましょう。 check modeに対応していない(特にshellモジュール) 実行する判断を行うようなタスクを作って、その結果次第で、タスクを実行するように書き換えます。 新しく作成したタスクでは、上述の通り副作用がないようにし、 check_mode: no をつけましょう。 補足: DNSゾーンファイルの更新 今回、check modeで実行可能にするためにplaybookをみなしていると、ansible で更新されていないものとして、template指定されたDNSゾーンファイルがありました。 ゾーンファイルは、シリアルを増やしていく必要があるため、面倒だったのだと思います。 確かに、言われるとめんどくさいような気もしますが、やってみると以下のようにすれば良さそうです。 現在のzone fileを見て、現在の serial を取り出し、registerで変数に入れる template ファイルに、 1.で取ったserialを入れて、差分があるかどうかを確かめる もし、2.で差分があったら serialをインクリメントした上で、template を実行する 実際に書いてみると以下のようになると思います。(NSDを利用した場合の例) server : port : 10053 zonesdir : \"/etc/nsd/zones\" {% for zone in dns . zones %} zone : name : \"{{ zone }}\" zonefile : \"{{ zone }}\" {% endfor %} - block : - name : parse serial from zone file shell : grep \";Serial\" / etc / nsd / zones / {{ item }} | awk '{print $1}' with_items : \"{{ dns.zones }}\" register : old_serials check_mode : no changed_when : no tags : nsd - name : check whether zone files updated or not template : src = zones / {{ item . 1 }} . j2 dest =/ etc / nsd / zones / {{ item . 1 }} owner = root group = root mode = 0644 vars : serial : \"{{ old_serials.results[item.0].stdout }}\" with_indexed_items : \"{{ dns.zones }}\" diff : no register : zonefiles_changed tags : nsd - name : update zonefile when changed template : src = zones / {{ item . 1 }} . j2 dest =/ etc / nsd / zones / {{ item . 1 }} owner = root group = root mode = 0644 vars : serial : \"{{ old_serials.results[item.0].stdout|int + 1 }}\" when : zonefiles_changed . results [ item . 0 ] . changed with_indexed_items : \"{{ dns.zones }}\" notify : reload nsd tags : nsd 定期的な差分チェック ここまでくれば、あとはcronなりJenkinsで、 terraform plan と ansible-playbook -i sacloud_inventory.py site.yml --check を定期的に走らせて、差分があったらslackなりに通知すればオッケーです。 Jenkinsからapplyするようにしておけば、より良いと思います。 感想と今後に向けて ここではインフラのデプロイをどのように改善してきたのかを書きました。 usacloud や Terraform for さくらのクラウドがあるおかげで、規模が大きくなっても楽に管理できている実感があります。 実際、雑に自前スクリプトを書いていたものも捨てていっているところです。 この記事で書いたようにインフラの整合性を確かめることができるようになったので、今後としては、 (ansibleの実行を AWX で行う(だいたいできた) ) インフラ/アプリケーションのCDを行うための監視改善。特に node/service discovery改善 アプリケーションのCD terraform, ansible の自動適用 などを目標に改善活動を行っていく予定です。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/12/05/1748.html","loc":"https://www.hitsumabushi.org/blog/2017/12/05/1748.html"},{"title":"Mesos の sandbox のログローテーションをする","text":"結論 http://mesos.apache.org/documentation/latest/logging/#logrotatecontainerlogger LogrotateContainerLogger を使って、 module parameter を設定する 概要 Mesos + Marathon 環境でdockerを動かしている。 基本的にコンテナのログは fluentd で飛ばしているのだけど、日に日に mesos slave のディスク容量が圧迫されていた。 調べてみると、 /var/lib/mesos-slave/slaves/ 以下にあるフォルダのうち、sandbox のログが肥大化していた。 sandbox には stdout, stderr があって、それぞれコンテナのstdout, stderrを記録しているファイルで、mesosからファイルとして取得することができる。 長期的なものは fluentd で飛ばしているので問題ないため、障害時や直近の確認のために sandbox を使うことにして、 短期間でのログローテーションを行うことにした。 設定方法 LogrotateContainerLogger の利用が可能か調べる 共有オブジェクト /usr/lib/liblogrotate_container_logger.so にあることを確認する。 パスが違っても良いが、その場合は以下で出てくるパスも変更する。 モジュールの設定ファイルを書く /etc/mesos_slave_modules.json として以下を書く。 { \"libraries\": [{ \"file\": \"/usr/lib/liblogrotate_container_logger.so\", \"modules\": [{ \"name\": \"org_apache_mesos_LogrotateContainerLogger\" # パラメータを指定するときはここに書く }] }] } モジュールの有効化 /etc/mesos-slave/modules として、以下が書かれたファイルを置く。 file:///etc/mesos-slave-modules.json /etc/mesos-slave/container_logger として、以下が書かれたファイルを置く。 org_apache_mesos_LogrotateContainerLogger mesos-slave の起動 mesos-slave を起動する。 動作確認 sandbox を見ると、 stdout.logrotate.conf, stderr.logrotate.conf が出来ている。 ファイルの中身は / path / to / stdout / { ## logrotate オプション: パラメータで指定してない場合は size 10481664 くらいしかない } となっている。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/12/01/2001.html","loc":"https://www.hitsumabushi.org/blog/2017/12/01/2001.html"},{"title":"Ansible でバージョンチェックする","text":"やりたいこと サーバー管理には ansible を使っていますが、apt で入れているパッケージに対して、以下をやりたい状況がありました。 - インストールされていなかったらインストール - あるバージョン未満だったらアップデート pipやgemといった言語のパッケージマネージャと違って、apt ではバージョンの制約を書くことが面倒です。 いい感じに playbook の中でバージョン比較をして、インストール/アップデートすべきかを判定したいと考えました。 結論 バージョン比較には、 version_compare filter を使う Version Comparison バージョン比較方法としては、 LooseVersion と StrictVersion がある 実装としては、 python の distutils.version を利用しているので、どう判定されるか迷ったら、比較してみれば良い playbook 例えば、docker の場合だと、以下のような感じで書けば良い。 ## docker version を取得 ## docer version してみて、 ## - rc が 1 だったら、docker がインストールされていないと判断 ## - stdout にバージョンが入る - name : check docker version command : docker version -f \\{\\{.Server.Version\\}\\} changed_when : false ignore_errors : yes register : docker_installed - name : install curl apt : name=curl - name : install docker shell : curl -sSL https://get.docker.com/ | sudo sh when : docker_installed.rc == 1 ## docker_min_version という変数が定義されているとする。 ## 例えば、 '1.11.0' という文字列が入っている - name : update docker if older than docker_min_version apt : name=docker-engine state=latest when : docker_installed.stdout | version_compare(docker_min_version, '<') distutils.version で試してみる >>> from distutils.version import LooseVersion >>> LooseVersion ( '1.10.3' ) < LooseVersion ( '1.11.0' ) True >>> LooseVersion ( '17.05.0-ce' ) < LooseVersion ( '1.11.0' ) False","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/11/10/1402.html","loc":"https://www.hitsumabushi.org/blog/2017/11/10/1402.html"},{"title":"Travis CIで git submodule update --init --recursive を止める","text":"結論 https://docs.travis-ci.com/user/common-build-problems/#Git-Submodules-are-not-updated-correctly に書かれている通り、 .travis.yml に以下の行を付け加えれば良い。 git : submodule : false 個別に git submodule update --init hoge していくとき、ビルドのトップディレクトリに戻りたくなることがある。この場合には、 $TRAVIS_BUILD_DIR を使えば良い。 経緯 久しぶりにこのブログを書いたらビルドに失敗していた。 原因としては、このブログで利用している getpelican/pelican-plugins のsubmoduleの設定によって、 取得できないsubmoduleがあり、 git submodule update --init --recursive に失敗しているという感じだった。 ハマった点 もともと、自分自身で .travis.yml の install で git submodule update --init --recursive していて、そこが悪いと思っていた。 そのため、必要なsubmoduleについて、個別に git submodule update --init hoge していって終わりだと思った。 実際にやってみると、 https://travis-ci.org/hitsumabushi/blog/builds/288689875 のようになるが、install よりも早い段階でエラーになっていることに気づくのに時間がかかった。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/10/17/0336.html","loc":"https://www.hitsumabushi.org/blog/2017/10/17/0336.html"},{"title":"Intel NUC 上に vSphere 6.5 のVSAN環境を作る","text":"あまりVMwareを触らなくなってきて、何も見ずにvCenterの設定とかMaximum configurationsとか言えなくなってきたので、VMware周りのことをメモに残しておくことにする。 ひとまず、自宅のVSAN環境の構築メモ。 環境 以下の構成のIntel NUC 3台にESXiをインストールして、その上にvCenterを立て、VSAN環境を作る。 Machine: NUC6i3SYH https://www.amazon.com/gp/product/B018NSAPIM Memory: 16GBx2 https://www.amazon.com/gp/product/B015YPB8ME Cache SSD: Intel 128GB https://www.amazon.co.jp/gp/product/B01JSJA1Z2 SSD: Crucial MX300 275GB https://www.amazon.com/gp/product/B01IAGSD5O USB storage: SanDisk USB Flash Drive 8GB https://www.amazon.co.jp/gp/product/B005FYNSUA ESXi インストール用 あんまり良くはないけど、省スペースのために妥協 USB NIC: StarTech.com USB32000SPT https://www.amazon.com/dp/B00D8XTOD0 一時期、管理ネットワークとVSANネットワークを兼用していたが、すぐにぶっ壊れるので、NICを追加するために購入した 4K までだけどJumbo frame がサポートされているので、まあ良さそう。もちろん本当は9K欲しい。 NFS サーバー vCenter をデプロイしたいだけなので、データストアがVSANと別に作成できれば何でも良い。 外付けHDDとかでもたぶんできるし、vCenterを作成するESXi上ではない場所にデプロイできるならそれでも良い。 あと、vCenterデプロイ時にSingle host VSANを構成することができるようになっているみたいだけど、今回はライセンスの適用順とかの問題を考えたくなかったので、使わなかった ESXi Customizer を利用する都合上、作業マシンはWindowsである必要がある(と思う)。 手順 ここでは、最初に StartechのUSB Ethernet adapterをインストール時点で使うために、ESXiのイメージカスタマイズから行う。 通常のESXiインストール後にvibでドライバを入れる場合には、 このブログ を参照すれば良い。 ESXi用のバンドルファイルのダウンロード このブログ のStep 0 にある、 ESXi 6.5 USB Ethernet Adapter Driver Offline Bundle をダウンロードする。 これを新しくディレクトリを作って(ここでは、 C:\\path\\to\\pkg とする)保存しておく。 ESXi イメージの準備 普通のESXi イメージを使っても良い。 ESXi Customizer を使う。 事前に、 VMware PowerCLI をインストールしておく。 (VMware PowerCLI の最新版の場所はわかりづらい。たぶん https://www.vmware.com/support/developer/PowerCLI/ あたりに飛んで、新しそうなバージョンのリンクを踏めば良い。) ESXi Customizer をインストールした場所で、以下のコマンドを実行する。 ( C:\\path\\to\\pkg は、先の手順で作成したディレクトリ) .\\ESXi-Customizer-PS-v2.5.1.ps1 -pkgDir C:\\path\\to\\pkg -nsc すると、 ESXi-6.5.0-20171004001-standard-customized.iso のような名前のISOファイルができる。 オプション: USBでブートしてインストールできるようにする 今回は、CDに焼くのが面倒だったのでUSBからブートしてインストールする。 何かしらのUSBドライブを用意する。 UNetbootin を利用して、用意したUSBドライブにイメージを書く。 見たままだけど、ディスクイメージの項目からISOファイルを選んで、USBドライブを選べび、OKを押せば良い。間違って変なドライブを選択しない限りは特に問題はない。 以下のインストールでは、このUSBドライブをIntel NUCに挿して行う。 ESXi インストール 普通にインストールする。 boot時はF10連打してbootしたいドライブを選ぶ。 install先はインストール用のUSB(私の構成ではSanDiskのUSBドライブ)を選ぶ。 インストール後の追加の初期設定 通常通り、管理ネットワークの設定やホスト名の設定を行う他に、NICを有効にする必要がある。 手順としては以下の通り。 DCUI(もしくはSSH)に入れるようにして、DCUIに入る esxcli software vib list して、 vghetto-ax88179-esxi65 というvibが入っていることを確認する 以下を実行する esxcli system module set -m=vmkusb -e=FALSE reboot 以下をDCUIで実行して、NICが見えていることを確認する esxcli network nic list 自分の環境では以下のように見える。 [ root@esxi-01:~ ] esxcli network nic list Name PCI Device Driver Admin Status Link Status Speed Duplex MAC Address MTU Description ------ ------------ ------------ ------------ ----------- ----- ------ ----------------- ---- -------------------------------------------- vmnic0 0000 : 00 : 1 f .6 ne1000 Up Up 1000 Full b8 : ae : ed : eb : 0 c : 11 1500 Intel Corporation Ethernet Connection I219 - V vusb0 Pseudo ax88179_178a Up Up 1000 Full 00 : 0 a : cd : 2 f : 23 : 6 a 1500 Unknown Unknown vusb1 Pseudo ax88179_178a Up Down 0 Half 00 : 0 a : cd : 2 f : 23 : 6 b 1500 Unknown Unknown これを3台とも行う。 NFS のマウント vCenter をデプロイする先のデータストアが必要なので、どれか1台でNFSをマウントする。 vCenter のデプロイ && VSANの設定 vCenter は通常どおりデプロイする。CLIからデプロイした。 VSANの設定も通常どおり行えば良い。 vDSを作って、VSAN用のportgroupを作る。今回の vusb0/1 はMTU4000までいけるので、必要に応じて、MTU4000にしておく その後各ホストで、vmkアダプタを作って、VSAN用ポートグループにアサインして、VSANネットワークとして設定する HAを有効化するのはVSAN有効化後に行う 参考 ESXi Customizer: https://www.v-front.de/p/esxi-customizer-ps.html UNetbootin: http://unetbootin.github.io/ virtuallyGhetto: http://www.virtuallyghetto.com/2016/11/usb-3-0-ethernet-adapter-nic-driver-for-esxi-6-5.html","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/10/13/0106.html","loc":"https://www.hitsumabushi.org/blog/2017/10/13/0106.html"},{"title":"さくらのクラウドを便利に使うためのツールメモ","text":"1. CLIで操作したい usacloud が今一番良い。 非公式と書かれているが、サポートされている機能、更新頻度、使い勝手、導入のしやすさ、などどれをとっても usacloud を使うべき。 https://sacloud.github.io/usacloud/ 便利なコマンド 〇〇の一覧が欲しい 〇〇 list すれば良い。例えば、以下の通り。 ## サーバー usacloud server list ## スイッチ usacloud switch list サーバーのメンテナンス情報を知りたい usacloud server maintenance-info サーバーにSSHしたい usacloud server ssh -l username example_node peco と usacloud でもっと便利にSSHする peco ( http://peco.github.io/ ) をインストールする。 以下のようなスクリプトを .zshrc に書く。 .zshrc を読み込み直して、 Ctrl + g を押すと、サーバー名やIPで絞り込みを行って SSH できる。 上記スクリプトではタグ情報を入れていないけど、入れたい場合には、 usacloud server list --output-type json などにして、出力を jq でフィルタして表示するのが良いと思う。 2. terraform 経由で使いたい usacloud と同じ作者が作っている terraform provider がある。 terraform 公式には入っていないが、インストールも簡単。 https://sacloud.github.io/terraform-provider-sakuracloud/ ただ、既存リソースをインポートして使うにはなかなか大変。 色々 tfstate ファイルを生で書き換える必要がある。 特に、リソースには基本的にIDが必要だけど、さくらのクラウドにIDがないものがある。 例えば、DNSのゾーンにはIDがあるけど、レコードにはIDがない。 これを管理するために、terraform provider 側でIDを生成している。 以下のような形で生成したものをtfstateに書けば良い。 https://play.golang.org/p/rYk3VNLeGN 3. ansible で便利に使いたい TODO","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/07/10/1600.html","loc":"https://www.hitsumabushi.org/blog/2017/07/10/1600.html"},{"title":"自宅 vSphere 上の CoreOS (Container Linux) のデプロイメモ","text":"構成 Mesos などを立てるため、Master 3台, Slave 2台以上の予定で作る。 OSイメージ VMware用のイメージを利用。 /usr/share/oem/cloud-config.yml に vmware tools周りの設定が入っているので便利。 password の変更 grub で linux...の行の末尾に coreos.autologin と入れてパスワードなしログインをしてから、パスワード変更する。 初期設定: cloud-config /var/lib/coreos-install/user_data を編集する。 (作法を無視すれば、 /usr/share/oem/cloud-config.yml でも良いはず。) #cloud - config coreos : units : - name : docker - tcp . socket command : start enable : true content : | [ Unit ] Description = Docker Socket for the API [ Socket ] ListenStream = 2375 BindIPv6Only = both Service = docker . service [ Install ] WantedBy = sockets . target - name : ens192 - static . network runtime : false content : | [ Match ] Name = ens192 [ Network ] Address = 192.168.101.11 / 24 Gateway = 192.168.101.1 DNS = 8.8.8.8 DNS = 8.8.4.4 - name : down - ens192 . service command : start content : | [ Service ] Type = oneshot ExecStart =/ usr / bin / ip link set ens192 down ExecStart =/ usr / bin / ip addr flush dev ens192 - name : ens224 - static . network runtime : false content : | [ Match ] Name = ens224 [ Network ] Address = 192.168.105.11 / 24 - name : down - ens224 . service command : start content : | [ Service ] Type = oneshot ExecStart =/ usr / bin / ip link set ens224 down ExecStart =/ usr / bin / ip addr flush dev ens224 - name : systemd - networkd . service command : restart ssh_authorized_keys : - \"ssh-rsa ...\" hostname : \"coreos-00x\" users : - name : \"hitsumabushi\" passwd : \"hashed password: https://coreos.com/os/docs/latest/cloud-config.html#users\" groups : - \"sudo\" - \"docker\" ssh - authorized - keys : - \"ssh-key ...\" write_files : - path : \"/etc/resolv.conf\" permissions : \"0644\" owner : \"root\" content : | nameserver 8.8.8.8 nameserver 8.8.4.4 もうすこし設定する NTP デフォルトで、 systemd-timesyncd が上がっていて、 /etc/ntp.conf をみると [0-3].coreos.pool.ntp.org へ接続している。 etcd2 coreos : etcd2 : # generate a new token for each unique cluster from https :// discovery . etcd . io / new ? size = 3 discovery : \"https://discovery.etcd.io/<token>\" # multi - region and multi - cloud deployments need to use $public_ipv4 advertise - client - urls : \"http://192.168.101.11:2379\" initial - advertise - peer - urls : \"http://192.168.105.11:2380\" # listen on both the official ports and the legacy ports # legacy ports can be omitted if your application doesn ' t depend on them listen - client - urls : \"http://0.0.0.0:2379,http://0.0.0.0:4001\" listen - peer - urls : \"http://192.168.105.11:2380,http://192.168.105.11:7001\" ... units : ... - name : etcd2 . service command : start ... locksmith coreos : update : reboot - strategy : \"best-effort\" ... units : ... - name : locksmithd . service command : start","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/02/08/2231.html","loc":"https://www.hitsumabushi.org/blog/2017/02/08/2231.html"},{"title":"Rust を始めるための設定","text":"rust のインストール 公式からインストールのためのスクリプトが提供されている。 これを使うと、rustup というrustのマネージャが使えるようになって、 rustc のバージョンアップや切替なんかができるらしい。 ## rustup のインストール curl https://sh.rustup.rs -sSf | sh ## これを .zshrc などに加える source ${ HOME } /.cargo/env rust のツールのインストール cargo というので、いろんなライブラリとかツールをダウンロードできるらしい。 go get 的なノリだと思う。 ひとまず、補完とフォーマットのために以下の2つをインストールする。 ## 割と時間がかかるので注意 cargo install rustfmt cargo install racer vim の設定 rust.vim , vim-racer の2つをインストールする。 設定は以下の通り。 \" Rust let g:rustfmt_autosave = 1 set hidden let g:racer_cmd = \" ${ HOME } /bin\" hello world 例えば、以下のようなわざとフォーマットが崩れた状態のファイルを保存すると、保存時にフォーマットされる。 fn main () { println! ( \"Hello, world!\" ); } 最低限これで十分使えるけど、定義ジャンプとかもできたりするらしいので、その辺のショートカットの設定と、syntax エラーがわかるように設定をしたい。 参考 気付いたらRustの環境構築がかなり楽になってた","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/02/07/2355.html","loc":"https://www.hitsumabushi.org/blog/2017/02/07/2355.html"},{"title":"ニフティ株式会社を退職しました","text":"本日最終出社日でした。 ニフティでやったこと ニフティでは、ニフティクラウドのIaaSを中心に仕事をしていました。 基本的にずっと @ysaotome さんの下で働いており、 チームメンバーも優秀だったので、とても面白い環境でした。 本来は仮想ネットワークチームという名前だったのですが、割と何でもやるチームで、 自身がやっていた仕事も、仮想ファイアウォールやIPアドレスの払い出しに関する諸々、 DRサービスやデスクトップサービスなど、色々なことをやらせてもらったと思います。 技術/ツール的には、 vSphere関係のことについてはかなり詳しくなりましたし、 ansible, python, golang, ...といったものを使いつつ、運用改善やサービス化のためのツールを書くことができました。 特に自分として印象に残っている仕事としては、DRサービスというサービスの課金情報を作るための仕組みを、golangで実装したことでしょうか。 β版をリリースした段階で離れてしまったのですが、、 ここにも記載されている通り 、VMwareのプロダクトを利用していて、 当時は『インフラ部分も大体作り方理解したし、さーて実装するかー』と製品仕様調査をしていたところ、 展示会に間に合わせたいと言われ、2，3週間で一気に実装しきった記憶があります。 課金に必要な情報を色々なイベントから取得すればできることがわかった後も、 VMwareがどのように実装しているかわからないため、 どの操作でどんなイベントが出て、どんなイベントが出たらどんなことが起こっているのか、ということをひたすら調べて実装していました。 golangで(運用ツールではなくて、)プロダクション環境で動くコードを書いた最初の機会だったので、今も印象に残っています。 また退職日当日、 会社再編を行うことが発表され 、 会社の事業が売却されることについて、労働契約承継法という法律の存在を教えてもらうなどし、 なかなか貴重な体験ができました。 明日から 明日から次の会社で働く予定です。 通勤経路がほとんど変わらないので、寝ぼけて前の会社に出社してしまいそうで怖いです。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2017/01/31/1936.html","loc":"https://www.hitsumabushi.org/blog/2017/01/31/1936.html"},{"title":"チーム活動で失敗したことの振り返り","text":"これは、 NIFTY Advent Calendar 2016 の15日目です。 @plan0213 さんの 『ネットワーク機器へのコマンド入力自動化』 でした。 ネットワークインフラエンジニアが大好きな話でしたね。 はじめに 今日の記事ですが、私が社内でやっていたチーム活動について振り返りを書きます。 特に、失敗した点に注目して共有することで役に立てばと思っています。 大前提としては、私は一般社員であり、チームのマネジメント等を行う立場ではありません。 あくまでボトムアップからの活動です。 パダワン制度という取り組みについて 1年~1年半前あたりに始めた取り組みです。基本的な制度設計は、期間を区切った徒弟制度になっています。 徒弟制度といえば、 ペパボのブログで書かれていたもの もあったりしますが、 パダワン制度は、トランザクティブ・メモリーの文脈で \"組織内で『誰が何を知っているか』を把握すること\" が大切だ言われているのを意識していました。 組織構造について 部全体では30~40人程度の時期 各プロジェクトごとに、関係するチームの人がアサインされる 1チームからは1人か2人 タスクの進捗については、行われている 実際の活動内容は、同じプロジェクトにアサインされいてるか、同じプロジェクトに入っていないとわかりづらい 当時の課題 新しいツールや仕組みを導入する人が偏っている 世の中の流れにキャッチアップできている人が限られている キャッチアップする人には常に最高速を出せる状態にあって欲しいが、質問が集中して負荷が高い 導入した後の手離れが悪い いつまで経っても、質問がその人に集中する ヨコ展開がない/少ない 導入済み環境で利用している人も、自分で導入しようとしていない そもそも新しい取り組みについて知らない 個人のスキルから組織のスキルにできていない 試み 当時、以下のような資料を作成して、パダワン制度を始めました。 マスターがパダワンを教育する期間は、適当に区切っていました。 教育期間の終わりに勉強会を部内向けに行い、その時点でパダワンは独り立ちする、としています。 また、マスターがパダワンへ教育した後は、パダワンが他の人から質問を受けることにしていました。 パダワンの選定については、マスターと異なるチームから選ぶことにしていました。 結果 無事に特定メンバーへの質問を減らすことができた とはいえ、あまり口うるさく言わない パダワンを『○○に詳しい人』と部内で認知させることができた 新しく使い始める人が、パダワンへ相談することができるようになった 反省 まず、この活動ですが、現在は継続していません。 それに関しては、以下のような理由があります。 マスター/パダワンの負荷がとても大きい 自由活動としてやっていたため、既存業務と並行して行っていた きちんと業務として考慮されるように調整すべきだった マスター/パダワン選択の難しさ 業務に余裕のある人でないとできない 一通り聞いた後は、メンバーが聞きたいことが先立って、それほど詳しくない状態のトピックが挙がるようになった これ自体は良いことだけど、マスターが学習・習得・教育を一気に行うことになるので、落ち着きたい それほど深い話も聞けないので、あえて弟子を取る必要が無い また、成果物の扱いが曖昧になってしまった、という反省があります。 これは会社の文化だと思います。 成果物の扱いが曖昧で、成果物が社内でしか閲覧できない 内容的には、社内に限定する意味はないので、社外公開したい 最終的には社外へ公開しましょう、というのを明示しておかないといけなかった 一方で、個人的な思いとしては、一通り問題の大きい部分は解消できたという思いも半分あって、新しいマスターが生まれるような活動を進めたいなぁ、というのが最近の気持ちです。 ただ、そのためには、組織構造や仕事の仕方を変えることも含めて考える必要があると思っていて、個人的に得意ではない『ご説明』をしないといけないのかなぁ、と考えている今日この頃です。 終わり 良いチームで良い活動を行うことが、エンジニアにとってもサービスにとっても大事だと思っているので、 今よりうまくチームとして活動することを常に目指していきたいです。 貪欲に改善活動をしていきたい所存なので、上手くいった話ばかりではなくて、 もっと失敗談とか、どういう準備をすべきだった、という振り返りが共有されると良いなぁ、と思います。 明日は、 @qrac さんの『リメイクUIコーディング』です。 qracさんは、 Yaku Han JPというWebフォント を作ったりするほど、デザインやUIガチ勢です。 そんなqracさんのUIコーディングの話が読める、明日のAdvent Calendarはとても面白そうなので、ぜひ見てください。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/12/15/0000.html","loc":"https://www.hitsumabushi.org/blog/2016/12/15/0000.html"},{"title":"git bisect でバグ/仕様変更のコミットを探す","text":"まとめ git bisect が便利 ansible 2.1.0 -> 2.1.1 で group名に / を入れるとうまく動かないケースが存在する ansible リポジトリでbisect すると対象のコミットは 7287effb5ce241ce645d61e55e981edc73fa382a group名には / を入れないように、 group_vars 以下はフラットな構成にしよう 遭遇した問題 ansible で構成/コンフィグ管理やプロビジョニングをしているのだけど、 複数のリージョンやゾーンにまたがるシステムのため、うまく設定を見やすくするために、 以下のように group_vars 配下にディレクトリ構造を作っていた。 再現環境 ディレクトリ構成 用途やrole, ノードの場所などで、グルーピングして、 group_vars 以下のディレクトリを作成する。 . ├── group_vars │ ├── env │ │ ├── production │ │ └── staging │ │ ├── app . yml │ │ ( └── vault - pass . yml ※今回は関係ないが暗号化したい情報。 ansible - vault で作成。 ) │ ├── region │ │ └── region - 1. yml │ ├── role │ │ └── web . yml │ └── zone │ └── zone - 1. yml ├── inventories │ └── sample . ini ├── library ├── roles └── site . yml group_vars $ cat env/staging/app.yml --- app_name: \"sample_app_name\" inventory 各ノードの状況に応じて、 env/staging や region/region-1 などにノードを所属させていた。 $ cat inventories / sample . ini [ env/staging ] app_server - 1 db_server - 1 [ region/region-1:children ] zone / zone - 1 [ zone/zone-1 ] app_server - 1 db_server - 1 [ role/web ] app_server - 1 [ app ] app_server - 1 ansible_host = localhost [ db ] db_server - 1 ansible_host = example . com playbook $ cat site.yml --- - hosts: app gather_facts: no tasks: - debug: msg = \"{{ app_name }}\" 問題の再現 ansible 2.1.0.0 だと app_name を参照できている。 $ ansible -- version ansible 2.1.0.0 ... $ ansible - playbook - i inventories / sample . ini site . yml PLAY [ app ] ********************************************************************* TASK [ debug ] ******************************************************************* ok : [ app_server-1 ] => { \"msg\" : \"sample_app_name\" } ... ansible 2.1.1.0 だと app_name を参照できず、未定義になっている。 $ ansible -- version ansible 2.1.1.0 ... $ ansible - playbook - i inventories / sample . ini site . yml PLAY [ app ] ********************************************************************* TASK [ debug ] ******************************************************************* fatal : [ app_server-1 ] : FAILED ! => { \"failed\" : true , \"msg\" : \"the field 'args' has an invalid value, which appears to include a variable that is undefined. The error was: 'app_name' is undefined\\n\\nThe error appears to have been in '/home/hitsu/tmp/blog/site.yml': line 6, column 7, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n tasks:\\n - debug: msg=\\\" {{ app_name }}\\ \"\\n &#94; here\\nWe could be wrong, but this one looks like it might be an issue with\\nmissing quotes. Always quote template expression brackets when they\\nstart a value. For instance:\\n\\n with_items:\\n - {{ foo }}\\n\\nShould be written as:\\n\\n with_items:\\n - \\\" {{ foo }}\\ \"\\n\" } ... コミット履歴の調査 コードを読むと この辺 が怪しいというのはわかるが、コードを順番に読んでいくと時間がかかるので、調査する範囲を絞りたい。 そこで、どのコミットまで正しく動作していて、どのコミットから挙動が変わっているのかを知りたい。 最初、手作業で二分探索していたが、Twitterで mapk0yさんに git bisect を教えてもらった。 @_hitsumabushi_ なんのことか全然わかってないの勘違いしてるかもしれませんが、ちゃんと問題の検出ができるならば git bisect を使えば二分探索で怪しいコミットを見つけることができるはずです。 — mapk0y (@mapk0y) November 9, 2016 git bisect での調査 リポジトリのクローン git clone https://github.com/ansible/ansible.git テストスクリプトの準備 $ cd ansible $ cat check.sh #!/bin/sh PATH_TO_PLAYBOOK = \" ${ HOME } /tmp/sample\" PATH_TO_ANSIBLE = \" ${ HOME } /tmp/ansible\" cd \" ${ PATH_TO_ANSIBLE } \" git submodule update --init --recursive pip uninstall -y ansible ; pip install \" ${ PATH_TO_ANSIBLE } \" cd \" ${ PATH_TO_PLAYBOOK } \" ansible-playbook -i inventories/sample.ini site.yml git bisect の実行(自動) 現在、すでに 2.1.0.0 と 2.1.1.0 の間で動作が変わっていることがわかっている。 タグとしては、 v2.1.0.0-1 と v2.1.1.0-1 の間になる。 まずは、 2分探索をどの範囲で実施するか設定して、開始する。 git bisect start \"壊れているコミット\" \"正しく動作しているコミット\" の順番で指定する。 $ git bisect start v2.1.1.0-1 v2.1.0.0-1 次に、各コミットに対して、 good/bad の判断を行うため、テストスクリプトを指定する。 これで、自動的にテストスクリプトを実行して、2分探索を行ってくれる。 あとは、結果が出るまで待てば良い。 $ git bisect run ./check.sh ... 7287effb5ce241ce645d61e55e981edc73fa382a is the first bad commit commit 7287effb5ce241ce645d61e55e981edc73fa382a ... これで、 commitが特定できた。あとは、内容の変更を精査すれば良い。 片付け 調査が終わったら、 git bisect reset して、終わってしまえば良い。 git bisect 便利!! 参考資料 問題のあるコミットを特定する ( git bisect ) github.com/ansible/ansible add_host module in v2.1.1.0 does not follow paths with forward slash #16881","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/11/10/0352.html","loc":"https://www.hitsumabushi.org/blog/2016/11/10/0352.html"},{"title":"DockerHub で docker build のオプションを設定したい","text":"参考リポジトリ https://github.com/hitsumabushi/docker-phpipam 目的 Dockerfile の中で、 ARGを使いたい。 LABEL としてビルドした日付や、 VCSのリビジョンを入れたい ソフトウェアのバージョンをARGで指定したい やること Dockerfile と同じ場所に、 hook ディレクトリを作成する。 hook ディレクトリ以下に、 build というファイルを作成する。 build には、 build時に実行したいシェルスクリプトを書くと、build 時に実行される。 自分で docker build ... というコマンドを書く TIPS Dockerfileを別の場所に置く場合 https://github.com/hitsumabushi/docker-phpipam では、 Dockerfile を、 /dockerimages/phpipam/ 以下に置いている。 その場合であっても、 Dockerfileと同じディレクトリに、 hook ディレクトリを作成すれば良い。 その上で、 DockerHub の Build Settings の Dockerfile Location にて、 /dockerimages/phpipam/Dockerfile とすれば良い。 注意点としては、実際にビルドされるとき、 /dockerimages/phpipam/ 以下でビルドされることになるのだけど、 上位(例えば、リポジトリルート) のファイルなどはビルド中アクセスできない。 もし必要なファイルがあるのであれば、Dockerfileと同じか、そのサブディレクトリに置く必要がある。 build 中の環境変数の参考 printenvなどすればわかる。 色々便利な環境変数がセットされていて、例えば、以下のようなものが便利そう。 GIT_SHA1 : revision の sha-1 ハッシュ GIT_MSG : commit メッセージ SOURCE_BRANCH : git branch DOCKER_REPO : DockerHubで公開されるときのURL DOCKER_TAG : ビルドに成功したときに利用されるタグ (Build Settingsで設定できる。) IMAGE_NAME : イメージ名 ( index.docker.io/hitsumabushi/phpipam:latest のようになる。) Dockerfile のラベルについて Dockerfile には LABEL を追加することでメタ情報をつけておくことができる。 フォーマットに関して指定はないが、以下のようなラベルをつけるのも良さそう。 http://label-schema.org/rc1/ その他のhook もし、build の前に何かやりたいなどあれば、 pre_build などのhookもある。 その他の hook については、 thibaultdelor/testAutobuildHooks が参考になる。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/09/27/1445.html","loc":"https://www.hitsumabushi.org/blog/2016/09/27/1445.html"},{"title":"GnuPGのメモ @ Debian Sid","text":"Debain での GnuPG GnuPG は OpenPGPの実装の一つで、GPGと呼ばれることもあります。 Debian では、2016/07/12現在、gpg コマンドは 1.4系で、 gpg2 パッケージで 2.1系を提供しています。 普段使いでgpg 1.4系をわざわざ使う必要はないとは思うので、以下ではgpg2 を利用することにしています。 以下では、gpg と gpg2 の違いに触れることはありませんが、例えば、RSAではなく楕円暗号を使おうと思ったら、gpg2 を使うしかありません。 PGP での鍵の使われ方 ざっくり言うと、暗号化と署名の場合に鍵を利用することがあると思います。 説明の都合で、ファイルをAさんからBさんへ送るというシチュエーションで考えてみます。 (AさんとBさんが同一人物でも同じです) ファイルを暗号化して送信 Aさんは暗号化をするとき、Bさんの公開鍵を利用します Bさんは復号するとき、 Bさんの暗号鍵を利用します ファイルに署名して送信 Aさんは署名するとき、Aさんの秘密鍵を利用します Bさんは署名を検証するとき、 Aさんの公開鍵を利用します 鍵の管理体系について 上述の通り、暗号化と署名時に鍵を利用することができますが、事前に公開鍵を配布しておく必要があります。 このために、公開鍵サーバーを利用することができます。 公開鍵をここにアップロードしておき、公開鍵を利用したいユーザーは、鍵ID (16進8桁) を指定するかユーザー情報(\"名前 <メールアドレス>\")の一部を指定して、鍵を受け取ることができます。 ただ、ユーザー情報から、鍵を検索・受け取ることはできるのですが、第3者が同じ情報を使って別の鍵を登録することもできてしまいます。 もし、間違って、別の鍵でファイルを暗号化してしまうと、相手は復号できないだけでなく、別の人が復号できていまいます。 そういった事態を極力防ぐために、フィンガープリントを確認することが必要になります。 (鍵ID, ユーザー情報, フィンガープリントの3つを重複させることはめったにできない、という前提です。) 秘密鍵を生成した人は、鍵ID, ユーザー情報のほか、フィンガープリントを提供しておき、公開鍵を利用する人は、公開鍵を受け取ったときにフィンガープリントを確認してから、利用するという手順です。 鍵の有効性を確認するための別の考え方として、 web of trust という考え方があり、 What is not WoT? (In Japanese: WoT とはなんでないか) に詳しく載っています。 gpg2の使い方 鍵の生成 gpg2 --full-gen-key 鍵の失効 # まず失効用の証明書を作成しておく # 厳重に保管しておく必要がある gpg2 -o revoke.asc --gen-revoke <keyid> # ... 失効したくなったら gpg2 --import revoke.asc 公開鍵サーバーの利用 アップロード gpg2 --send-key <keyid> ダウンロード # 検索 gpg2 --search-keys ( <keyid> | <user> ) # インポート gpg2 --recv-keys <keyid> 鍵を信用する gpg2 --edit-key <keyid> gpg> trust ... gpg> (適切に 1-5 のどれかを選択) gpg> q gpg2 --update-trustdb 公開鍵のフィンガープリント gpg2 --fingerprint ファイルの暗号化 gpg2 -e -r <keyid> plain.txt # 同時署名をする場合 gpg2 -se -r <keyid> plain.txt ファイルの復号 gpg2 plain.txt.gpg ファイルの署名 gpg2 -sa plain.txt ファイルの署名の検証 gpg2 -d plain.txt.asc 資料 GNU Privacy Guard講座 トップ GnuPG で遊ぶ - 暗号化してみる 盗難/紛失に備えるGPG鍵の作り方 What is not WoT? (In Japanese: WoT とはなんでないか)","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/12/0742.html","loc":"https://www.hitsumabushi.org/blog/2016/07/12/0742.html"},{"title":"もともとgitで管理されているアプリケーションをdebパッケージにしたいというメモ","text":"はじめに debパッケージを作る経験が少ないので、あまり良い方法ではないかもしれない。 サンプル https://github.com/hitsumabushi/hub 手順 # install sudo apt install fakeroot # tag, release をきれいにする git tag -l > tag_list for x in $( cat tag_list ) ; do git push origin : $x ; done # 色々リポジトリを整理した後、空っぽの masterを作る git checkout --orphan master # 自前で control, copyright ファイルを書く # アップストリームのものを持ってくる # 参考: https://github.com/bcandrea/consul-deb/tree/debian/debian fakeroot dpkg-deb --build pkg tmp # 生成された deb ファイルの中身を確認し、install してみる ar x <deb file> # ... check files sudo dpkg -i <deb file> # ... check installation # dpkg -s <package> # dpkg -L <package> # 問題がなければ git commit -a git push origin master","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/11/0428.html","loc":"https://www.hitsumabushi.org/blog/2016/07/11/0428.html"},{"title":"s3で自前 Debian Package リポジトリを作る","text":"この記事でやること aptly の初歩的な使い方 aptly を使ってs3へ自前リポジトリを公開する。 自前パッケージの作成については、書かないです。 自前 Debian Package リポジトリ なぜ自前のリポジトリが欲しいかというと、個人的には以下の3つくらいかと思います。 カジュアルにパッチを当てて、サーバに適用したい 必要なパッケージをフリーズしたいが全てのサーバーで Pin するのは面倒、などの理由で自分でバージョンをコントロールしたい 公式パッケージがない場合に、ビルド済みのものをインストールする形式にしたい ただし、リポジトリの構成を調べたり、一度手で作ってみるとわかるが、更新がとっても大変です。 そのため、 aptly や reprepro といったリポジトリを管理するためのツールがあります。 aptly リポジトリ管理を行うツールで、かなりリポジトリ管理のフローを考慮されたツールになっています。 全く新しくリポジトリを作る場合や、リポジトリのミラーをしたりできます。 特に個人的に便利に使えそうだと思っているのは、スナップショットの機能で、複数のリポジトリからスナップショットを作成できます。 スナップショットを作成して、s3(やs3互換ストレージ)やswiftへアップロードすることができます。 注意点として、リポジトリのミラーをする場合、現状、自分のGPG keyになってしまうようで、ミラーのためにこのツールを使うのは難しいです。ミラーをしたいだけであれば、 rsync した方が良いです。 リポジトリのミラーを resign なしで作成できるようにする機能は作成途中のようです。 また、今回はs3にアップロードする前提ですが、aptly自体は、自前サーバーで公開するためにも利用可能になっています。 その場合にもほとんど同じフローでできます。 インストール sudo apt-get install aptly すれば良いです。 自前のパッケージを含むリポジトリをs3へアップロードする手順 事前作業 GPG keyの作成が必要です。 gpg --gen-key アップロードするリポジトリの作成 まずは、空のリポジトリを作成します aptly repo create -distribution=wheezy -component=main <repo_name> 次に、パッケージをリポジトリに追加します aptly repo add < repo_name > < deb_packages > 現状、どんなパッケージが追加されているかは、 aptly repo show -with-packages <repo_name> で確認できます。 スナップショットの作成 さて、必要なパッケージ追加でき、リポジトリをアップロードする準備ができたら、スナップショットを取得しておきます。 最低限、publishする単位ごとにスナップショットを取っておけば良いと思います。 アップロードするのも、このスナップショットをもとに行います。 aptly snapshot create <snapshot_name> from repo <repo_name> スナップショットの一覧は、 aptly snapshot list です。 aptly snapshot show -with-packages <snapshot_name> とすれば各スナップショットの詳細を確認できます。 スナップショット名としては、サービスのバージョンと連携してパッケージをpublishするのであれば、そのバージョンをsuffixにつけるのが良いと思います。通常の運用時には、タイムスタンプをsuffixにつけましょう。 aptlyのs3設定 事前にs3のアクセスキーなどを取得している前提です。 以下のように環境変数に設定します。(後述する、 aptly.confに書くこともできます) export AWS_SECRET_ACCESS_KEY = \"XXXXXXXXXXXXXXXXXXX\" export AWS_ACCESS_KEY_ID = \"YYYYYYYYYYYYYYYYYYYY\" 次に、 ~/.aptly.conf にバケットの情報を書きます。 { \"rootDir\" : \"/home/hitsu/.aptly\" , \"downloadConcurrency\" : 4 , \"downloadSpeedLimit\" : 0 , \"architectures\" : [], \"dependencyFollowSuggests\" : false , \"dependencyFollowRecommends\" : false , \"dependencyFollowAllVariants\" : false , \"dependencyFollowSource\" : false , \"gpgDisableSign\" : false , \"gpgDisableVerify\" : false , \"downloadSourcePackages\" : false , \"ppaDistributorID\" : \"ubuntu\" , \"ppaCodename\" : \"\" , \"skipContentsPublishing\" : false , \"S3PublishEndpoints\" : {}, \"SwiftPublishEndpoints\" : {}, \"S3PublishEndpoints\" :{ \"<endpoint_name>\" : { \"region\" : \"ap-northeast-1\" , \"bucket\" : \"<バケット名>\" , \"prefix\" : \"debian\" , \"acl\" : \"public-read\" } } } スナップショットのアップロード スナップショットをpublishします。 このコマンドを打つと、s3へ自動でアップロードしてくれます。 s3のWebサイトのホスティングを設定していれば、これでパッケージを公開できたことになります。 aptly publish snapshot <snapshot_name> s3:<endpoint_name>: リポジトリの更新について 基本的には、 aptly repo add → aptly snapshot create で良いのですが、アップロードについては、注意が必要です。 aptly publish switch <distribution_name> s3:<endpoint_name>: <new_snapshot_name>","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/10/0344.html","loc":"https://www.hitsumabushi.org/blog/2016/07/10/0344.html"},{"title":"VCP6-NV 取得した","text":"VCP6-NV 受験シリーズ VMware VCP-NV (VCP6-NV) の試験を今週受けるので、試験について調べる VCP6-NV 試験勉強メモ VCP6-NV 試験勉強メモ 2日目 VCP6-NV 取得した 結果 462 / 500 (合格点: 300) 試験の内容について 割と事前に学習した内容が出ていたと感じる。 VCP-DCV (といっても、VCP5-DCVしか受けたことはない。) と違って、トレーニングの内容と試験内容がきちんと一致しているので、トレーニングのテキストを読み返すだけでも、かなり点は取れる気がする。 もっと学習しておくべきだったこと VCNSからのアップグレード条件 3問くらい出てた気がする アクティビティモニタリング 一般的な使い方を調べておけば良さそう トラブルシューティングの方法 特にフローキャプチャについて QoS の知識 QoSの仕組み 物理機器との連携について 動的ルーティングプロトコル プロトコル自体の一般的な事柄 一応、vRealize についての問題も出ていたが、使うつもりがないので、学習しない。 感想 3日間くらいでVCP-NVを学習したけど、普段使っていない機能について振り返りつつ、「こんな感じで導入すれば上手くいきそう、と思ったけど上限にぶち当たるなー」とか考えながらできるので良い。 やっぱり日常的に使っているプロダクトのベンダー試験は、そこそこの勉強で合格できるようになっているみたいだし、本番である程度運用に慣れてきたら、一度こういう試験を受けるのはアリだと思う。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/09/1232.html","loc":"https://www.hitsumabushi.org/blog/2016/07/09/1232.html"},{"title":"VCP6-NV 試験勉強メモ 2日目","text":"VCP6-NV 受験シリーズ VMware VCP-NV (VCP6-NV) の試験を今週受けるので、試験について調べる VCP6-NV 試験勉強メモ VCP6-NV 試験勉強メモ 2日目 VCP6-NV 取得した 資料 ネットワーク仮想化をネットワークの基本から理解する 〜 第1回：理解するために必要なことを整理する NSX 6.2 新機能のご紹介 Part 1 〜 Cross vCenter NSX 〜 NSX におけるVXLAN 関係する要素 論理スイッチ 分散論理ルータ 論理ルータコントロール VM Edge 分散論理ルータ カーネルモジュール内で動作するルーター。 各ESXiに分散して存在していて、ヘアピントラフィックを防ぐことができる。 分散論理ルータのインターフェースは、LIF(論理インターフェース)と呼ばれる。 LIF IPアドレスが割り当てられる ARPテーブルはLIFごとに持つ vMAC LIFが分散スイッチにささっている時のMAC 物理スイッチに保存されることはない VXLAN LIF の時のMACアドレス VLAN LIFと違い、代表インスタンスなどは不要 1つの論理スイッチに接続できるVXLAN LIFは1つ トランスポートゾーン全体のDVSに広げることができる pMAC LIFがポートグループにささっている時のMAC 物理スイッチに保存される VLAN LIF の時のMACアドレス VLAN LIF は1台のDVSにのみ所属できる VLAN LIF は代表インスタンスを1つ持ち、ARP要求はその代表インスタンスが処理する 疑問: ARP要求は代表インスタンスが答えるが、実際には全てのホストで同じpMACを持っていて、その後のトラフィックは各ホストで処理するということで良いのか? 代表インスタンスの選定は、NSX Controllerが行う 制御プレーンについて 分散論理ルータの制御プレーンは、以下の2つある。 インスタンスごとに作られる、論理ルータコントロール仮想マシン Active/Standby 構成にもできる ダイナミックルーティング(OSPF, BGP)を処理する つまり、隣接ノードから見ると、VLAN LIFとは異なるIPとセッションをはることになる 処理したルーティング情報をNSX Controllerへ送る NSX Controller ルーティング情報を分散ルータへ(つまり対象のすべてのESXiへ)プッシュする L2 ブリッジ VXLAN と VLAN の L2ブリッジのみがサポートされる。 アップリンクをDVSに割り当てておけば、物理機器とVXLANをつなぐような用途で利用できる。 L2 ブリッジには分散ルータが必要で、論理ルータコントロール仮想マシンが載っているホストが、ブリッジインスタンスとして、実際のブリッジを行う。つまり、1ホストのラインレートが上限になる。 スループットを向上させるには、1つのブリッジに対しては1つの分散ルータを作成するようにすることが良い。 分散ルーティングと論理スイッチ上のブリッジは同時に有効にできない。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/07/0742.html","loc":"https://www.hitsumabushi.org/blog/2016/07/07/0742.html"},{"title":"VCP6-NV 試験勉強メモ","text":"VCP6-NV 受験シリーズ VMware VCP-NV (VCP6-NV) の試験を今週受けるので、試験について調べる VCP6-NV 試験勉強メモ VCP6-NV 試験勉強メモ 2日目 VCP6-NV 取得した 資料 最も公式っぽい資料 VMware Certified Professional 6 – Network Virtualization Exam の \"How to Prepare\" に書いているもの。 Reference Design: VMware® NSX for vSphere (NSX) Network Virtualization Design Guide VCP6-NV (2V0-641) Practice Exam ブログ : シリーズものは一通り目を通した方が良い ネットワーク仮想化をネットワークの基本から理解する 〜 第1回：理解するために必要なことを整理する NSX 6.2 新機能のご紹介 Part 1 〜 Cross vCenter NSX 〜 NSXの configuration maximums vSphere 製品のように 公開されているわけではない。 でも、以下のサイトのように、情報を公開している人はいる。 NSX-V 6.1 Configuration Maximums VMware NSX-v Configuration Maximums 会社でNSX使っている人は、VMwareの人に言えば上限値資料もらえると思うけど、会社外で確認したいときに、便利。 運用・開発を考える場合でも、上限値系は何も見ずに言えることが望ましいと思う。 おおざっぱな各上限値のオーダーは頭に入れておきたいところ...なんだけど、NSX 6.1と6.2で数値が違うところがあるんだよなぁ。 VCP6-NV (2V0-641) Practice Exam の問題抜粋 上限値系 Q. \"What is the maximum number of VNIs that can be used in a vSphere environment?\" 10,000 これは、NSXの制限というよりは、 vCenterの制限かもしれない※要確認 VNIを1つ使うごとにポートグループを作ってしまう。現状のvSphere 6.0では、 10,000 portgroup/vC なので、これ以上作成できることに意味がない、ということと覚えている Q. \"How many DHCP pools can be created on the NSX Edge?\" 20,000 覚えるしかない? \"NSX DHCP service can provide configuration of IP pools, gateways, DNS servers, and search domains.\" Q. \"What is the maximum number of audit logs retained by the NSX Manager?\" 1,000,000 覚えるしかない デプロイメント Q. \"What is the minimum vSphere configuration needed to deploy NSX?\" A cluster of ESXi hosts managed by vCenter Server Q. \"What is the minimum vSphere 6.0 license edition required to deploy NSX?\" 今は、any vSphere edition で動作する。Enterprise plus が必要だったのは昔の話。 「いやいや、vDS必要でしょ」と思うかもしれないですが、そのあたりは、 VMware NSXには、vSphere Enterprise Plusが必要？それは過去の話です。 に書かれています。 Q. \"Which two components are valid minimum prerequisites for installing NSX in a vSphere environment? (Choose two.)\" VMware vCenter Server 5.5 or later VMware Tools 8.6 or later ESXi 5.0 or later Q. \"What is the packet size of the VXLAN standard test packet when using the Ping test on the logical switches?\" 1,550 VXLAN 分のオーバーヘッドは 50 byteなので。 良く意味がわかっていない Q. \"What is the minimum number of vSphere Standard Switches (vSS) that must be configured before deploying VMware NSX for vSphere?\" 選択肢は、0, 1, 2, 3 のどれかで、正解は0。 解説は、 \"NSX includes logical switches. The NSX logical switch creates logical broadcast domains or segments to which an application or tenant virtual machine can be logically wired.\" となっていた。 いまいち問題の趣旨がわかっていない。答えるとしたら 0だけど、解説を読む限りは、「NSX自体をデプロイするのにportgroupが必要になるけど、それはvDSで作れば良い」とかそういう話ではないっぽい。 NSX の概要おさらい おさらいなので、一度でもNSXについて理解したことのある人向け。 NSX のアーキテクチャ Management plane NSX Manager Control plane NSX Controller 論理ルータコントロール VM Data plane ESXi カーネルモジュール 論理スイッチ 分散ルータ 分散FW Edge NSX のコントローラ コントローラは必ず3つデプロイする。 コントロールプレーンとして、4つのテーブルを持つ ARP MAC VTEP ルーティング コントローラとESXiホストのカーネルモジュールは、分散FWを除いて、netcpaというUWAを通じて通信する。(VXLAN, 論理ルータなど。) UWAはカーネルモジュールと通信してよしなにする。 分散FWは、直接カーネルモジュールがvsfwdを通じて、NSX Managerと通信する。 コントローラは、Paxos ベースのアルゴリズムで分散処理している マスターを選出 VXLAN, 論理ルータのそれぞれに対して、スライスを作成し、各コントローラに割り当てて処理させる コントローラの追加・削除時には再度スライスの分散が行われる NSX の VXLAN プロトコル概要 VXLAN は 24bitのIDを持っていて、VNI という。(つまり、 1677万以上ある) VXLAN のオーバーヘッドは、 50byte分ある MTUは少なくとも 1550byteである必要がある IPv6 のことも考えると、推奨は、1600byte以上らしい ジャンボフレームを設定すれば間違いない カプセル化後のパケットは、 4789/UDP VNI のうち、1NSXでは、10,000までしか利用できない VNI のうち、5,000からしか利用できない 論理スイッチを作成するとき、レプリケーションモード(マルチキャストモード、ハイブリッドモード、ユニキャストモード)を選択する。それにより、BUMトラフィックのパケットフローが大きく変更される。 BUMトラフィックとは、 ブロードキャスト、不明なユニキャスト、マルチキャストのトラフィックのこと トランスポートゾーン トランスポートゾーンは、VNI用の境界になる Layer 3/VXLAN ネットワークの境界になるという意味 VXLANオーバーレイのメンバーまたは、VTEPを定義している。 異なるvSphereクラスタのESXiホストを含めることができる 1個のクラスタを複数のトランスポートゾーンに含めることもできる VTEP,MTEP,UTEP VTEP VXLAN のカプセル化、またはカプセル化の解除を行うエンドポイント vSphereでは、vDS上の VMkernelポートでカプセル化される NSXと連携可能な H/W VTEP も世の中には存在する予定 VTEPプロキシ : リモートセグメントにある別のVTEPから受け取ったVXLANトラフィックをローカルセグメントに転送するVTEP。MTEP,UTEPがある。 レプリケーションモード それぞれの場合のBUMトラフィックの図が以下のサイトにある VXLAN simplified - what, why and how ? マルチキャストモード Layer 2でIGMPが有効化されていて、かつ、Layer 3でマルチキャストルーティングが必要 ローカルもリモートのどちらのトラフィックもマルチキャストのパケットを投げる ハイブリッドモード Layer 2でIGMPが有効化されている必要 ローカルではマルチキャストで投げ、リモートにはMTEPへユニキャストパケットを投げる MTEPは受け取ったパケットをローカルセグメントへマルチキャストとして投げる ユニキャストモード 特に制限はない ローカルもユニキャストで投げ、リモートにはUTEPにユニキャストパケットを投げる UTEPは受け取ったパケットをローカルセグメントへユニキャストとして投げる","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/05/2121.html","loc":"https://www.hitsumabushi.org/blog/2016/07/05/2121.html"},{"title":"VMware VCP-NV (VCP6-NV) の試験を今週受けるので、試験について調べる","text":"VCP6-NV 受験シリーズ VMware VCP-NV (VCP6-NV) の試験を今週受けるので、試験について調べる VCP6-NV 試験勉強メモ VCP6-NV 試験勉強メモ 2日目 VCP6-NV 取得した この記事について 書くこと VCP6-NV の試験要項 VCP6-NV の受験方法 書かないこと 試験の勉強方法 試験内容 VCP6-NV について VCP-NVは、VMwareのNSXを用いた仮想ネットワーク管理についての試験です。 他のVCPと同様に、VCA-NV → VCP-NV → VCAP-NV {Design, Deploy} → VCIX-NV → VCDX-NV というような資格体系です。 上記の図を含め、資格と試験の情報は以下にまとまっています。 VMware Certified Professional 6 – Network Virtualization (VCP6-NV) 試験について まず、今日(2016/07/05) 時点での、試験は、 2V0-641: VMware Certified Professional 6 - Network Virtualization Exam です。 以前の試験から、2015/08/30あたりで切り替わっているようです(詳細は調べていません)。 そのため、昔の情報を見る場合には、きちんと現行のVCP-NVになっているのか確かめる必要があります。 受講資格について 現在、有効なVCPを持っているかどうかによって異なります。 昔のVCPを持っている人に取っては、あまり馴染みはないですが、現行のVCPの試験には Foundation試験を受けた後、DCVならDCVの試験、NVならNVの試験という風な2段階の試験になっています。 そのため、必要なトレーニングを受講後、試験2つをパスする必要があります。 ただし、現在有効なVCP資格を持っている場合、トレーニングもFoundation試験も免除されるため、NVの試験( 2V0-641: VMware Certified Professional 6 - Network Virtualization Exam ) を受けるだけで問題ありません。 まとめると以下です。 有効なVCPを持っている場合 2V0-641: VMware Certified Professional 6 - Network Virtualization Exam を受験し、合格する VCPを保有していない、もしくは失効している場合 対象のトレーニングの受講 NSX: Install, Configure, Manage [V6.0] , [V6.1], [V6.2] NSX for Internetworking Experts Fast Track [V6.0],[V6.1] NSX Troubleshooting & Operations [V6.1] (available as Onsite Training only) NSX: Design & Deploy [V6.2] (available as Onsite Training only) Security Operations for the Software Defined Data Center vSphere 6 の Foundation試験を受験し、合格する 2V0-641: VMware Certified Professional 6 - Network Virtualization Exam を受験し、合格する 試験の概要について このページにまとまっています。 VMware Certified Professional 6 – Network Virtualization Exam 抜粋すると、以下の通りです。 試験時間 : 100分 問題数 : 85問 合格点 : 300点 対象のプロダクト : VMware NSX for vSphere v6 試験費用 : $225 USD まとめ とりあえず、これから受けるのでまとめてみましたが、以前のVCPの情報よりよくまとまっていて、資格試験としての体裁が整っている印象です。 VMwareの人と話していても、資格試験としてまともなっていると聞くので、受ける価値がありそうです。 (以前のVCPはちょっと・・・という感じの試験内容だった。) とりあえず、受けたらまた試験について書きます。 追記 VCP6-NV の試験勉強メモを書いた。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/07/05/2040.html","loc":"https://www.hitsumabushi.org/blog/2016/07/05/2040.html"},{"title":"会社にslack入ったので、色々やってた","text":"勤務先の会社に今日(2016/06/16)から真面目にSlackが導入されたので、やった設定をメモしておく。 現在のチャンネル構成 定常系 分報用のチャンネル。個々人が作る 部署用 チーム用 プロジェクト用 雑多な情報共有用 ニュース用 トラブル用 その他 臨時に必要になるとか テストとか やったことメモ RSS を Slack に移行 live dwango reader をずっと使っていたのだけど、更新頻度がそこそこで、重要度の高いRSSを Slack で追いかけることにした。 やり方 追加自体は簡単で、通知をしてほしいチャンネルで、以下のようなコマンドを打つ。 # 通知したいチャンネルで入力 /feed <feed url> こうしておくと、更新された時に、チャンネルに通知が来る。 コメント 通知先のチャンネルについては、内容に応じて分けておいた方が良いと思う。 今まで自分はRSSを読んで、チームで共有するものは個別に共有していたけど、そういう情報源はチームのチャンネルに入れてしまった。 もしかしたら、別途チームのニュースチャンネルを作った方が良いかもしれない。 以下のような場合分けにした。 更新頻度が高すぎる RSS のまま そのうち、botでサマライズするように変える予定 後述する通り、「RSSを読もう」という remind をしている 更新頻度がそこそこ 共有したい チーム・プロジェクト用のチャンネルへ通知 自分だけで良い 分報用チャンネルへ通知 分報用チャンネルはオープンなので、近くのチームの人は入っているので、興味があれば見る人もいるので、RSSよりは良い 予定を remind する 例えば、新人の日報を見る、とか、毎日RSSを見るとかの予定を Slack で通知させることにした。 他に、そのうちやろうと思っている短い個人作業(5分以内とかのレベル)のタスクも、面倒なので、言われたその場で、 remindに入れてしまっている。 やり方 # 自分用 : 平日は毎日19:00 に \"<リマインド>\"をする /remind #<分報用チャンネル> to \"<リマインド>\" at 17:00 every weekday # チーム用 : 月曜日は毎日19:00 に \"<リマインド>\"をする /remind #<チームチャンネル> to \"<リマインド>\" at 10:00 every Monday コメント /remind me ... としても、リマインドできるけど、その場合、プライベートチャットでリマインドされる。 会社のタスクだったり、予定だったりするので、特にプライベートでやる意味がないので、チャンネルに通知することにしている。 他の使い方については、helpを見よう。 よく使いそうなのは、 in とかだと思う。 /remind help 実行に時間がかかるコマンドの終了通知 検証などで、 ansibleやらを実行するときに、時間がかかるステップがある。(例えば、windwos update。) 実行中は他の作業をしたいので、終了を通知されるようにした。 やり方 Slack 側 : incoming webhook を設定。 アイコンは、スクリプト側で指定するので、無視して良い。 URLをメモするのと、通知先のチャンネル、ユーザー名を決めておく程度。 zsh shell側 : 時間がかかるコマンドの実行結果をSlackに通知する を参考にした。 SLACK_WEBHOOK_URL をメモしたURL SLACK_USER_NAME をメンションしたいユーザー名 コメント これは分報用チャンネルに入れた。 GitLab, Jenkins との連携 Merge Requestが来たときとか。 やり方は、割愛。 コメント 今のところは、複雑なことはやっていない。 また細かい設定をしたら、書く。 Zabbix などの監視 やった。 ひとまず、alert の発生と収束だけを通知している。 slackから操作できるようにしたいが、まだやっていない。 Redmine/JIRA 連携 これからやるけど、全社共通のものなので、Redmine 側には pluginは入れてくれないかもしれない。 botでやろうかと思っている。 Office 365 できていない。 メール・カレンダーは連携したい。 社内ADと連携しているはずなので、社内ADの認証を通してから、APIを叩くbotを作る必要があると思っている。 でも、APIのたたき方がわからない。まだ調べる気が起きていない。 暫定対処 直接は連携できていないが、ひとまず、メールをSlackに転送するようにした。 bot まだ書いてない。 これから書く。ひとまず、以下のあたりをやるつもり。 打刻 定時頃になったら、今月の残業時間を通知 Redmine連携","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/06/16/1445.html","loc":"https://www.hitsumabushi.org/blog/2016/06/16/1445.html"},{"title":"Anisble for Windows","text":"久しぶりに ansible で windows を操作するので、メモ。 実行側は Debian Sid でやっているので、apt-get しているところについては、適宜置き換えて欲しい。 ansibleは2.1.0を使っている。 やること 以下を anisble で実行する。 Windows Update Windows Server 2012 R2 に AD をインストール ADに適当なダミーエントリを突っ込む 初期設定 ansible 実行サーバーの準備 winrm経由で実行することになるので、 pywinrm が必要。 # pip install ansible pywinrm Inventory [windows_server] ad-seed-001 ansible_host = 164.70.5.244 [windows_server:vars] ansible_user = adminuser ansible_port = 5986 ansible_password = <password> ansible_connection = winrm ansible_winrm_server_cert_validation = ignore 接続確認 # ホスト側のwinrmの初期設定を行っていないと、おそらく失敗する ansible -i inventory_file all -m win_ping ホスト側の初期設定 以下のスクリプトを実行すれば、ひとまずは、上記のpingが通る。 FWがパブリック/プライベートの両方空いたりするので、必要に応じてスクリプトを変更する。 https://github.com/ansible/ansible/blob/devel/examples/scripts/ConfigureRemotingForAnsible.ps1 再度、接続確認を行えば、おそらく成功する。 ansible -i inventory_file all -m setup などとして、factを確認しておくと便利。 ansible playbook ある程度できたら書きます。 資料 http://docs.ansible.com/ansible/intro_windows.html http://qiita.com/yunano/items/8bddf084007671a38f57 : スクリプトなどはこちらを参考にした","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/06/16/0602.html","loc":"https://www.hitsumabushi.org/blog/2016/06/16/0602.html"},{"title":"YAMAHAのネットボランチDNSを使って、ニフティクラウドとVPN接続する","text":"やりたいこと いろいろあって、自宅のグローバルIPv4アドレスが変更される機会があった。 ニフティクラウド上のルーターとVPN接続しているため、グローバルIPが変更されると、 いちいち変更されたタイミングでVPN設定を変更する必要があり、非常に面倒くさい。 そういうわけで、DDNSを使って設定することで、グローバルIPが変更された場合でも設定変更の必要がないようにしようと思う。 ネットボランチDNSとは ネットボランチDNSは、YAMAHAが提供しているダイナミックDNSサービスのこと 。 YAMAHAのルーターで利用可能。 別にネットボランチDNSじゃなくとも、世の中のDDNSサービスを使えば良いとは思うけど、 YAMAHAのルーターを使っている場合には、こちらの方が設定が簡単なので、今回はネットボランチDNSを利用する。 接続までの手順 以下は、YAMAHA RTX1200 (Rev.10.01.65) で実施している。 回線はフレッツ光ネクストで、IPv4側の接続はPPPoEになっている。 (今回は関係ないけど、IPv6側はIPoE。) ネットボランチDNSの登録 登録状況の確認 今回はまだ何も登録されていないことを確認している。 # netvolante-dns get hostname list all (Netvolante DNS server 1) PPPoEで利用しているIPの登録 ホスト名を example-yamaha とすると以下のようにして登録を行う。 # pp select 1 pp1 # netvolante - dns hostname host pp example - yamaha pp1 # netvolante - dns go pp 1 ( Netvolante DNS server 1 ) [ example-rtx.aa0.netvolante.jp ] を登録しました 新しい設定を保存しますか ? ( Y / N ) Yセーブ中 ... usb1 : / config . txt 終了 登録の確認 登録状況の確認としては、ネットボランチ用のコマンドで確認することに加え、名前解決しておく。 pp1 # netvolante - dns get hostname list all ( Netvolante DNS server 1 ) PP01 example - rtx . aa0 . netvolante . jp # pp1 # netvolante - dns go pp 1 ( Netvolante DNS server 1 ) pp1 # show status netvolante - dns pp ( Netvolante DNS server 1 ) ネットボランチ DNS サービス: AUTO インタフェース: PP [ 01 ] ホストアドレス: example - rtx . aa0 . netvolante . jp IP アドレス: < グローバル IP アドレス > 最終更新日時: 2016 / 05 / 03 07 : 23 : 13 タイムアウト: 90 秒 ( Netvolante DNS server 2 ) ネットボランチ DNS サービス: AUTO インタフェース: PP [ 01 ] ホストアドレス: IP アドレス: 最終更新日時: タイムアウト: 90 秒 pp1# nslookup example-rtx.aa0.netvolante.jp <グローバルIPアドレス> ニフティクラウドでVPNゲートウェイを作成する 細かな作成手順などは、 YAMAHAさんのサイト にキャプチャつきで記載されているのえ、要点だけかいつまんで記載する。 カスタマーゲートウェイについては、以下のキャプチャの通りに作成した。 また、VPNコネクションについては、以下のキャプチャを参考にしてほしい。 すこしわかりにくいけど、VPNゲートウェイとカスタマーゲートウェイを結ぶ線をクリックすると、 以下のような設定テンプレートをコピーできる このテンプレートを元に、YAMAHA RTX1200を設定すれば良い。 基本的には、テンプレートの内容をそのまま書くか、PPPoE用に書けば、だいたい大丈夫。 テンプレートに含まれていないこととして、フィルタを空けるというのがある。以下のような感じでやれば良い。 ip pp secure filter in ...(既存のフィルタ) 200080 200081 200082 ip filter 200080 pass * 192.168.100.1 udp * 500 ip filter 200081 pass * 192.168.100.1 esp * * ip filter 200082 pass * 192.168.100.1 udp * 4500 疎通確認 YAMAHA RTX側で以下を実行すると良い # show ipsec sa Total : isakmp : 2 send : 1 recv : 1 sa sgw isakmp connection dir life [ s ] remote - id ----------------------------------------------------------------------------- 1 1 - isakmp - 27132 < VPNゲートウェイのIP > 2 1 - isakmp - 27144 < VPNゲートウェイのIP > 3 1 2 tun [ 001 ] esp send 1944 < VPNゲートウェイのIP > 4 1 2 tun [ 001 ] esp recv 1944 < VPNゲートウェイのIP > さらに、クラウド側のローカルIPに向かって、pingを打って疎通確認をしておけば完璧。 まとめ ネットボランチDNSを使って、ニフティクラウドとIPSecで接続できるようにした。 IPを変更したり...というテストはそれほどちゃんとやっていないが、まあ通常のVPNの接続断と大差ないので、大丈夫だろうと楽観視している。 とりあえず、これで自宅環境で検証できることが増えるぞ。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/05/03/0624.html","loc":"https://www.hitsumabushi.org/blog/2016/05/03/0624.html"},{"title":"久しぶりにGitHub Pagesのjekyll触ったので、メモ","text":"このブログは、pelicanを使っているのだけど、久しぶりにGitHub Pagesのjekyllを触ることがあって、 昔とだいぶ状況も変わっていそうだったので、メモをしておく。 gem とりあえず、Gemfileを用意して、以下の2行を書く source 'https://rubygems.org' gem 'github-pages' 全体的なconfig GFMを使う ローカルでのテストと、GitHub Pages上のテストを同じにするためにも、 _config.yml に以下を追加しておいた方が良さそう。 markdown : kramdown kramdown : input : GFM related postを表示する lsi : true","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/03/25/0306.html","loc":"https://www.hitsumabushi.org/blog/2016/03/25/0306.html"},{"title":"Travis コマンドでのエラー","text":"Travis CIでCIする素振りをしていたら、最新版のtravisコマンドでエラーが出るようになった。 発生した問題 $ travis setup releases Invalid scheme format: git@github.com for a full error report, run travis report 問題の解析 こういう時には、 $ travis report をして、スタックトレースを見るものらしい。 $ travis report System Ruby: Ruby 2 .3.0-p0 Operating System: Mac OS X 10 .11.2 RubyGems: RubyGems 2 .5.1 CLI Version: 1 .8.0 Plugins: none Auto-Completion: yes Last Version Check: 2016 -01-02 14 :54:05 +0900 Session API Endpoint: https://api.travis-ci.org/ Logged In: as \"<username>\" Verify SSL: yes Enterprise: no Endpoints org: https://api.travis-ci.org/ ( access token, current ) Last Exception An error occurred running ` travis setup ` : Addressable::URI::InvalidURIError: Invalid scheme format: git@github.com from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/addressable-2.4.0/lib/addressable/uri.rb:867:in ` scheme = ' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/addressable-2.4.0/lib/addressable/uri.rb:795:in `block in initialize' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/addressable-2.4.0/lib/addressable/uri.rb:2302:in ` defer_validation ' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/addressable-2.4.0/lib/addressable/uri.rb:792:in `initialize' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/addressable-2.4.0/lib/addressable/uri.rb:135:in ` new ' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/addressable-2.4.0/lib/addressable/uri.rb:135:in `parse' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/travis-1.8.0/lib/travis/cli/repo_command.rb:71:in ` detect_slug ' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/travis-1.8.0/lib/travis/cli/repo_command.rb:60:in `find_slug' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/travis-1.8.0/lib/travis/cli/repo_command.rb:21:in ` setup ' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/travis-1.8.0/lib/travis/cli/command.rb:197:in `execute' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/travis-1.8.0/lib/travis/cli.rb:64:in ` run ' from /Users/<username>/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/travis-1.8.0/bin/travis:18:in `<top (required)>' from /Users/<username>/.rbenv/versions/2.3.0/bin/travis:23:in ` load ' from /Users/<username>/.rbenv/versions/2.3.0/bin/travis:23:in `<main>' For issues with the command line tool, please visit https://github.com/travis-ci/travis.rb/issues. For Travis CI in general, go to https://github.com/travis-ci/travis-ci/issues or email support@travis-ci.com. 見た感じ、addressable というモジュールでパースに失敗しているらしい。 travis.gemspec を見ると、確かに、 addressable というモジュールがあるけど、 s . add_dependency \"addressable\" , \"~> 2.3\" という指定になっている。一方で、上記のスタックトレースでは、 addressable-2.4.0 を使っているので、当たりをつけて以下を実行する。 $ gem uninstall addressable $ gem install -v2.3.8 addressable これで、travis コマンドが正常に実行できるようになる。 ISSUE上での話 というところまで来て、改めて、issueを見てみたところ、 https://github.com/travis-ci/travis.rb/issues/342 で議論されている。 結論としては、以下の通りで、現状はまだ未修正だった。 \" addressable モジュールでは、URIをparseするためのライブラリだけど、SCP styleのURIというのは、RFCにもなってないので、 addressable モジュールでサポートするものではないよ。なので、 travisコマンド側でハンドルしよう。\"","tags":"blog","url":"https://www.hitsumabushi.org/blog/2016/01/02/1513.html","loc":"https://www.hitsumabushi.org/blog/2016/01/02/1513.html"},{"title":"vSphere 6.0 でのHA機能のエンハンスの要点","text":"まとめ vSphere 6からは、ストレージパスが死んだ場合でもHAを設定できるようになった。 vCenterは watchdogs によりプロセス落下時には再起動される vCenterの可用性をさらに高めるにはWindows版を利用し、MSCSクラスタを設定する必要がある 資料 VMware vSphere 6 のドキュメント http://www.vmware.com/files/pdf/vsphere/VMW-WP-vSPHR-Whats-New-6-0-PLTFRM.pdf vSphere 5.x および 6.x での永続的なデバイスの損失 (PDL) と全パス ダウン (APD) (2081089) vSphere 5.5 までの障害あるある(Part 1) 物理ホストのHBAが死んだり経路上の問題で、特定のホスト群のみストレージへのパスが切れた。 vSphere 5.5 までは、ストレージへのパスが死んだ場合、HAによる保護ができない障害パターンで、IOが止まっているが、管理者が別のホストに移してから再起動するまでアプリケーションは死んでいた。 vSphere 6になると... VMCP(Virtual Machine Component Protection) の機能として、ストレージへのパスが切れた時に検知し、挙動を制御できる。 VMware のドキュメントとしては、 APL と PDL という2つの用語に分かれていて、設定上も異なる設定が可能。 APL All path down 全パスが切れた場合 PDL Permanent device loss LUNがぶっ壊れて認識できなくなった場合など どういう状態かは KB 208108 に例がある vSphere 5.5 までの障害あるある(Part 2) vCenterのプロセスが落ちる。 仕方ないので、再起動させるスクリプトを書く。 vSphere 6になると... Watchdog がvCenterに組み込まれている。 PID Watchdog と API Watchdog があって、それぞれプロセス自身を監視するのか、API経由で監視するのかの違いがある。 API Watchdog はデフォルトで起動する。 Watchdog は、vCenterプロセスの再起動してくれる。2回プロセス再起動してもプロセスが上がらない場合には、リブートする。 vSphere 6以降の設計ポイント (今までもやっていたと思うけど、 )VMCPのおかげで、1クラスタの中でストレージの障害範囲を分けておけば、 HAしてくれるようになった。 そのため、クラスタ内でストレージ機器への接続で、同一のスイッチを使わないようにするなど、検討する必要がある。 ところで、今回はvSphere 6で設計上の大きなポイントになる(と思っている)PSCについて一切触れていない。 PSCの構成については、 List of recommended topologies for VMware vSphere 6.0.x (2108548) に詳しく記載されている。 とはいえ、日本語版はアップデートされていないようだし、気が向いたら書こうとは思う。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/12/08/0551.html","loc":"https://www.hitsumabushi.org/blog/2015/12/08/0551.html"},{"title":"最近の仕事の振り返り","text":"最近、仕事のやり方について考えている(悩んでいるに近いかも)ことが多いので、 頭の中を整理するために、文章にしてみようと思う。 なので、技術的な内容は一切ない。 目次 スケジュール調整と突発案件 エンジニアの成長パス 愚痴とポエム スケジュール調整と突発案件 前置き どんな仕事にも突発的な仕事は大なり小なりあると思う。 自分としては精神的に負担が大きいし、元々やっている仕事との切り替えの負荷が高く集中力が削られるので、減らしたいと思っているのだけど、それ自体は無くせないし、仕方ないと思う。 一方で、(これはどこの会社も同じかわからないけど、)基本的にサービスの目標リリース日は、開発とか検証をする前に決まっていて、マイルストーンが後になればなるほど、リリース遅延が発生した場合の調整の難度が上がる。 特に、営業が社外でプレセールスして良いという段階をすぎると、顧客からの信頼にも関わることなので、営業の人は絶対に避けたいだろうし、僕も避けたい。 そうなると、できる限りリスクを把握して、リリース遅延が発生しうるなら、すぐに周知してマイルストーンを変更する必要があるのだと思う。 (全部完成してから、というのがもっともリスクが少ない方法だけど、実施されていない理由として、スピード感が失われるし、投資回収が遅くなって良くないのだと思う) 実際に困った状況 実際に最近困ったシチュエーションは以下のようなものだった。 体制的には、だいたいマーケティングする人と、開発リーダーと僕を含む開発エンジニアでサービスを作りつつ、プレセールス中は営業の人がフィードバックしてくれる、という体制だと思って読んでほしい。 複数のプロジェクトに参加。 優先度最高とされた進行中のプロジェクトは2つ プロジェクトAは2人で分業, Bは1人で開発 同じ開発リーダー プロジェクトAで開発以前の遅延があり、大きくずれ込む Aのリリース日を延期 問題があり、プロジェクトAのリリース日をさらに延期 Aのリリースがずれ込んだ結果、AとBのリリース日がほとんど近接 おそらくこの辺りで、プロジェクトBのリリース時期が営業に伝えられていた 突発でプロジェクトCが舞い込んでくる マネージャー権限でプロジェクトA, Bよりも優先して実施することが明言される プロジェクトCの全体スケジュールはまだない状況 プロジェクトBのリリース日が変わる様子がなかったので、「やだやだ、開発間に合わないし、絶対無理ですぅ」とリーダーに泣きついて、延期調整をしてもらう プレセールスを実施している営業の方から、当然どうにかならないか相談を受ける...悲しい プロジェクトAはリリース日を変えられないと言われて、「やだやだ、たぶん開発間に合わないし、絶対無理ですぅ」ともう一度泣きついて、別の人をアサインしてもらう \"たぶん\"がついているのはプロジェクトCのスケジュールが来ていなかったから 悩んでいること どうすれば良かったのかが謎。 自分としてできることは、次はもっと早い時期に、「リリース日遅らせてくれ」って言って行くしかない。 でも、それでも限界があるし、余裕を持ちすぎたスケジュールを作ってしまって、スピード感がなくなっても困る。 開発し始めて、「リリースは来年な」とか言えないし、どうやって複数プロジェクトを並行して開発しつつ、各プロジェクトの遅延とか、突発的なプロジェクトによる差を吸収するのか、本当に難しい。 自分が他部調整を直接やって、プレセールスする時期を遅らせると、もう少しマシなのかもしれないけど、開発に割ける時間が減って、開発期間長くなるしで、悪循環しかない。 追記: 2015/11/01 プロジェクト管理の手法としての、リスク管理を学べばヒントがわかるかも、と気づいたので、以下を読んでみようと思う。 http://dtcn-wisdom.jp/J-personal%20use/SP-6105%20NASA%20SE%20HBK-J.pdf エンジニアの成長パス 前置き http://r-kurain.hatenablog.com/entry/2015/10/28/090554 という記事が話題になっていたのもあるけど、最近自分のスキルセットについて考えることが多かった。 この記事では、プライベートを大切にしつつ、仕事中に成長できるようにしよう、と言っていると思う。 自分もそういう派というか、「自分がプライベート中に勉強するのは良いけど、他の人には強要できないよね。だから仕事中にできるようになろうね」派なので、共感できる。 悩んでいること そういう立場だと、仕事中に触れるものは、どういうものなんだろう、と考えることになる。 自分の会社は、割とインフラをサービスにしていて、かつ、特化型に近いのだけどその中で広くやっている。 そのため、人によるのだけど、自分の場合は1つのことをやっている期間が短くて、広く浅くやる感じになる。 その中でエンジニアとしてどういう成長ができるんだろうか。 そして、特化してやっている人もいるのだけど、そういう人に対する成長はどうやったら仕事中に実現できるんだろうか。 特化していると言っても、今やっている仕事に最適化されているだけ、という状況が生まれやすいので、それをどうやって回避して良いエンジニアの成長に繋げられるんだろうか。 (ジョブローテは僕では実施できる権限がないので...。) 最近やっていること 最近、部内勉強会をやっていて、いろんな理由にかこつけて、普段と違うことをやってもらえる準備をしている。 今のところは、徒弟制度でやっていて、1ヶ月以上は準備期間が用意できるようにしている。 ざっくり言うと、詳しい人を選定してマスターになってもらい、その人と違うチームの人を選んでパダワンになってもらうスターウォーズ制度にしている。 勉強会も仕事中にやってもらってるし、準備もできる限り仕事中にしてもらえるようにお願いしている。 準備には知識の伝達と習得の時間がかかるので、1ヶ月以上前からお願いするように心がけている。 現状は、まだ試験実施の段階だけど、上司の理解があるおかげで継続はできているし、もう少し形になってきたら、詳しく書こうと思う。 愚痴とポエム 配属されて結構経って、少しずつ仕事の領域が広がってきたからか、仕事をしていると仕事のやり方について、一緒に仕事をしている人の「悪いやり方」みたいなのが目につくようになってきた。 属人化して自分の仕事を守っているエンジニアとか、それを今まで放置してしまっているマネージャとか、社外とやり取りしている人からの要求に対して文句を言い続けるエンジニアとか、それを一緒になって言ってるエンジニアとか、問題を隠そうとするエンジニアとか。 (もちろん、そんな例は少数だから最近まで気づかなかったんだけど。) たぶんどんな会社にでもいると思うんだけど、自分は「もっとサービスを良くして、世界で勝てるサービスにしたい」と思っちゃってる人なので、邪魔だな、と思うし、仕事中だけは最低限をこなそうぜ、と思う。それで、それができない人がいるんだったら給料を下げるなり解雇してよ、ってマネージャーに対して思う。人が足りないから穏便に、って思うならメンタリングするなりしよう。 悪い雰囲気が一回広がっちゃうと立て直すのは難しいと思うし、どうにかしたいなー。 最近は仕事で悩みが多いので、地元に帰ってゆっくりと仕事したい。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/10/31/1350.html","loc":"https://www.hitsumabushi.org/blog/2015/10/31/1350.html"},{"title":"Dockerの細々としたメモ","text":"Debian で利用する際のメモを書いておく。 grub でのカーネルパラメータ systemd を利用する設定 cgroups で、メモリに制限をかけるための設定 # quiet はあってもなくても良い GRUB_CMDLINE_LINUX_DEFAULT = \"quiet init=/bin/systemd\" GRUB_CMDLINE_LINUX = \"cgroup_enable=memory swapaccount=1\" cgroups まわり cgroups のディレクトリ配下に設定がある /sys/fs/cgroup/cpu,cpuacct/docker/ network 参考: Dockerのネットワーク管理とnetnsの関係 docker コンテナを1つ立ち上げるごとに、vethデバイスができる 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00 :0c:29:e1:2c:42 brd ff:ff:ff:ff:ff:ff 3 : eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 00 :0c:29:e1:2c:4c brd ff:ff:ff:ff:ff:ff 4 : docker0@NONE: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether ba:fe:c4:10:27:19 brd ff:ff:ff:ff:ff:ff 8 : veth279cc5b@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master docker0 state UP mode DEFAULT group default qlen 1000 link/ether ba:fe:c4:10:27:19 brd ff:ff:ff:ff:ff:ff link-netnsid 0 netns : network namespace $ ls -l /proc/ {{ .State.Pid }} /ns/ 合計 0 lrwxrwxrwx 1 root root 0 9月 15 04 :38 ipc -> ipc: [ 4026532695 ] lrwxrwxrwx 1 root root 0 9月 15 04 :38 mnt -> mnt: [ 4026532693 ] lrwxrwxrwx 1 root root 0 9月 15 04 :38 net -> net: [ 4026532599 ] lrwxrwxrwx 1 root root 0 9月 15 04 :38 pid -> pid: [ 4026532696 ] lrwxrwxrwx 1 root root 0 9月 15 04 :38 user -> user: [ 4026531837 ] lrwxrwxrwx 1 root root 0 9月 15 04 :38 uts -> uts: [ 4026532694 ] iptables $ sudo iptables-save | grep -v \"&#94;#\" *nat :PREROUTING ACCEPT [ 28 :2281 ] :INPUT ACCEPT [ 27 :2197 ] :OUTPUT ACCEPT [ 68 :6733 ] :POSTROUTING ACCEPT [ 68 :6733 ] :DOCKER - [ 0 :0 ] -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127 .0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER -A POSTROUTING -s 172 .17.0.0/16 ! -o docker0 -j MASQUERADE COMMIT *filter :INPUT ACCEPT [ 16482 :42893307 ] :FORWARD ACCEPT [ 0 :0 ] :OUTPUT ACCEPT [ 12048 :507911 ] :DOCKER - [ 0 :0 ] -A FORWARD -o docker0 -j DOCKER -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT COMMIT 設定の意味 storage RHEL7におけるDockerのディスクイメージ管理方式 Resizing Docker containers with the Device Mapper plugin","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/09/15/0421.html","loc":"https://www.hitsumabushi.org/blog/2015/09/15/0421.html"},{"title":"VM Resource Allocation","text":"資料 stillwell_ipdps12.pdf Generalized Resource Allocation for the Cloud J0411055155.pdf Ghostscript wrapper for C:\\Documents and Settings\\Paolo\\Desktop\\icas2.pdf - icas_camera_ready.pdf resilience fetch.php","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/07/26/2030.html","loc":"https://www.hitsumabushi.org/blog/2015/07/26/2030.html"},{"title":"Twitterからメールアドレス拾えるんじゃね?的な攻撃について","text":"任意のTwitterアカウントの登録メールアドレス（伏せ字含）を表示させる攻撃が急増中（対策設定有り） というエントリが盛り上がっていたらしい。(とてもシェアされている) ただ、自分には有用な攻撃シナリオには見えなかったので、考えを整理するために書いておきます。 自分はセキュリティ素人なので、間違った考察かもしれません。 リンク先ブログの主張 Twitterの機能から、(アカウント名, メールアドレスの一部) を取得できる メールアドレスは、最初の2文字と、ドメイン名部分はgmailのみ表示されるが、他は最初の1文字のみ。 (アカウント名, メールアドレスアドレスの一部)のリストから、(アカウント名, アカウント名+ランダム列@gmail.com)のリストを生成する メールアドレスアドレスの最初の2文字と、アカウント名が一致しているものだけを利用する メールアドレスについては、ドメイン部分がgmailしかわからないので、gmail以外には有用でないため 得られた(アカウント名, アカウント名+ランダム列@gmail.com)の情報を利用して、フィッシングメールを送信 フィッシングメールから偽Twitterページに飛ばして、ログイン情報を盗む 攻撃としての問題点 いずれにせよ、(アカウント名, アカウント名@gmail.com) のリストしか取得できないので、 Twitterからアカウント名のリストを取得 (アカウント名, アカウント名+ランダム列@gmail.com)を機械的に作成 得られた(アカウント名, アカウント名+ランダム列@gmail.com)の情報を利用して、フィッシングメールを送信 フィッシングメールから偽Twitterページに飛ばして、ログイン情報を盗む としたほうが手間も少ないし、攻撃できる対象は変わらない。 また、ランダムな文字列を付加せずに、アカウント名@gmail.com固定で送ったほうが効率が良いと思う。 もっと多くのメールを送ることができるなら、@gmail以外にyahooなどにも送っても良いかもしれない。 評価 リンク先ブログの手順をA, このブログで説明した手順をBと書くことにします。 攻撃の容易さ リスト作成 A では、Twitterのサイトから有用なメールアドレスを得る必要があるが、条件として以下の2つがある上、Twitterにスパムと思われる可能性がある メールアドレスが、アカウント名から始まっていること gmailであること B では、Twitterのアカウントだけあれば、リストが作成できる メール送信 A のほうがBよりは不要なメールは少ない A, Bともある程度の期間で攻撃を達成しようと思えば、大量のメールを送る必要がある A, Bともスパム扱いされない工夫が必要 攻撃可能なユーザー A, Bとも同じ 攻撃成功時に得られる情報 A, Bとも同じ 自分的な結論 自分がこの攻撃を行うなら、Bを採用したいと考えます。 理由としては、 Aの攻撃を達成するには、Twitterのスパム判定回避と、gmailのスパム判定回避の2つを実行する必要がありますが、Bであればgmailのスパム判定回避だけで良いこと。 Aの攻撃とBの攻撃で、攻撃可能対象は変わらない Aの攻撃とBの攻撃で、攻撃成功時に得られる情報は変わらない という理由からです。 セキュリティ詳しい人だと異なる結論なのでしょうか...? まとめ 盛り上がっている内容の割に、特にセキュリティ的なリスクは変わらない Twitterの設定を変えても意味がない。予測可能なメールアドレスを登録している限り、同じ攻撃ができる フィッシングメールには、気をつけよう。パスワード入力する場合は、httpsであることや証明書を確認しよう","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/06/26/1250.html","loc":"https://www.hitsumabushi.org/blog/2015/06/26/1250.html"},{"title":"runCをDebianで実行する","text":"opencontainers/runc を実行してみようとしたところ、コンパイルまではすぐできるのに、実行するとエラーが出て困ったので、対処方法を書いておく。 runCのインストール golang的に普通の場所に置いてビルドすると良い。git clone の代わりに go getでも良い。 git clone https://github.com/opencontainers/runc $GOPATH/src/github.com/opencontainers/runc cd $GOPATH/src/github.com/opencontainers/runc/ make sudo make install 状況 ディレクトリ構成 . ├── container . json ( とりあえず、 runc spec で生成されるもので OK ) └── rootfs ( docker exportして 、 tarを展開したものを置いたディレクトリ ) 利用中のカーネル % uname -a Linux debian 4 .0.0-2-amd64 #1 SMP Debian 4.0.5-1 (2015-06-16) x86_64 GNU/Linux 問題 ここまで準備できるとruncが実行できるはずなのだけど、やってみるとエラーが出る。 % sudo runc --debug WARN [ 0000 ] signal: killed Timestamp: 2015 -06-23 20 :50:18.148962383 +0900 JST Code: System error Message: open /sys/fs/cgroup/cpu/user.slice/tmp/cpu.cfs_quota_us: permission denied Frames: --- 0 : start Package: github.com/opencontainers/runc/libcontainer. ( *initProcess ) File: process_linux.go@197 --- 1 : Start Package: github.com/opencontainers/runc/libcontainer. ( *linuxContainer ) File: container_linux.go@107 --- 2 : execContainer Package: main File: run.go@41 --- 3 : func·006 Package: main File: main.go@88 --- 4 : Run Package: github.com/codegangsta/cli. ( *App ) File: app.go@159 --- 5 : main Package: main File: main.go@96 --- 6 : main Package: runtime File: proc.go@63 --- 7 : goexit Package: runtime File: asm_amd64.s@2232 対処 カーネルの再コンパイル cfs_quota_us というのは、CFS Bandwidth Control というカーネルの機能らしく、cgroupなどと一緒に入ったものっぽい。 Red Hat Customer Portal Linux 3.2 の CFS bandwidth control (2) - TenForwardの日記 とりあえず、カーネルコンパイルする必要がありそうなので、以下を参考に実施する。 Debian Linux Kernel Handbook - Common kernel-related tasks カーネルコンパイル -Debian Linux- - GreenLeaf Debian流Linuxカーネル構築法 sudo apt-get install build-essential kernel-package libncurses5-dev bc sudo apt-get install linux-source sudo -s cd /usr/src tar xvf linux-source-4.0.tar.xz cd linux-source-4.0 cp /boot/config-4.0.0-2-amd64 .config make oldconfig make menuconfig ここで、 '/' を押し、\"CFS\" などを検索すると CFS Bandwidthの項目が見つかるので、チェックを入れる。 make-kpkg clean make-kpkg --revision 1 .0 --initrd kernel-image sudo dpkg -i ( できた debパッケージ ) ブート時のカーネルパラメータ 上記だけでは動かなかったので、さらに、GRUBでカーネルパラメータを追加した。 /etc/default/grub を開いて、 # GRUB_CMDLINE_LINUX=\"\" <-コメントアウト GRUB_CMDLINE_LINUX = \"cgroup_enable=memory swapaccount=1\" 編集した後、 sudo update-grub をして、reboot。 これで、 sudo runc で実行できるようになっている。 open container 自体は、まだspecも公開されていないし、特に遊べることはなさそうだ。 その他の参考資料 他にもカーネルコンフィグをいじった方が良いかもしれない。 Check Kernel Config Script for CRIU","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/06/23/2047.html","loc":"https://www.hitsumabushi.org/blog/2015/06/23/2047.html"},{"title":"Windowsの勉強を始めた","text":"人生で初めて、Windowsの勉強を始めた。 進捗は以下のgithub pagesから確認できる。 とりあえず、章立てを書いて、内容を徐々に埋めているところ。 Windows Server スタートガイド 内容の誤りやコメントなどあれば、githubのissueで欲しい。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/06/15/0004.html","loc":"https://www.hitsumabushi.org/blog/2015/06/15/0004.html"},{"title":"VDI, DaaS市場を調べてみた","text":"最近、vCloud AirがDaaSを出すというのを聞いて、にわかに自分の中でDaaSに興味が出てきた。 DaaSとかVDIは、必ずActive Directoryの話になるので、気後れしていたのだけど、会社ではWindows使っていることもあって、少しは抵抗もなくなってきたので、良いタイミングだったので、調べてみた。 とは言っても、まずは世間にどんなものがあるかをしらべてみただけ。 参考資料 http://www.netone.co.jp/report/column/20120302.html VSANでの検証 アプリケーション仮想化の概説 利用シーン 世の中の利用シーン オフィス内からの利用 Cisco WAASなどのTCP圧縮・冗長排除・キャッシュ 出先での利用 オフィス? 映像系の人は? VDIまわりとは一体何か? オフラインVDI: ユーザーデータのみロードパターンもある VDI: VDI, DaaS アプリケーション仮想化: アプリケーションごとの配信。thinappとか VDIの特徴 ユーザーごとにデスクトップを割り当て クライアント集中管理 可用性向上 検討ポイント 動画/Flash利用ユーザーへの配慮 回線の悪い状況(帯域が狭い場合 / レイテンシが悪い)のユーザー GPUの利用は? マルチデバイスで利用可能か? Zero Client みたいなものもある Chrome book なぜ使うのか、何が課題か よく書かれているのは、以下の通り。 運用コスト パッチ当て セキュリティ 端末紛失 ユーザーのデータを端末に残さない 災害対策・BCP 安全なDCを使う ワークスタイル変革 オンプレでやるとすると キャパシティプランニング ブートに必要な領域。さらにメールデータも注意。 個別のプロファイルを持たせている場合は、そのデータストア、フォルダリダイレクトを行う場合、ファイルサーバーに負担がかかる。 ストレージ IOPS 1ユーザーあたりの性能データ取得 ユーザー分布を調査 同時接続数の測定 バースト時IOPS 最大ユーザー数 フラッシュストレージの検討 容量 ネットワーク CIFSのパケットのやりとりが非常に多い ファイルサーバー CIFSで死ぬ 性能検証 ストレージ ストレージシステムコントローラ性能 SPECやSPCのベンチマーク結果 縮退時のこともあるので、結局N-1~N-2程度での性能 ディスクIO性能 現状のサービス Amazon Amazon Workspace vCPU, mem, storage, office, ...を選択可 $35/month~ 1ユーザー1台 Windows Server 2008R2のみ ユーザー領域はS3 12hourごとのバックアップ VDIの動的割り当てみたいなものはない いつも通りβ版っぽい 毎週日曜日 0:00-4:00までは利用不可 Amazon Workspace Application Manager(Amazon WAM) アプリケーション仮想化 普通のやつ ライセンス管理とかできる?? 使用頻度が取れる 割とお高い。$5/user Amazon WorkDocs Sync ドキュメントストアみたいなやつ VMware Horizon Air http://www.atmarkit.co.jp/ait/articles/1505/21/news054.html 4300円/month~ , 50台以上 Desktop DRという安価サービスもある。ただ、復帰が24hour以内はつらい... とはいえ、事前にセットアップしておける、というのは便利なのかも? 700円/month + (起動してる間)300円/日 起動は最低7日なので、起動するには、2100円/月以上 Azure 富士通 V-DaaS http://fenics.fujitsu.com/outsourcingservice/lcm/workplacelcm/virtualdesktop.html VMware Horizon DaaSベース 価格は顧客ID数/構成によって異なるが、参考価格はある 小規模だと1名あたり5000円/month前後~。大規模だと3000円/month強くらいまでいける。 同時接続数での課金ではなさそう? デフォルトだと1台、420MHz/2GB Mem/40GB disk 3ヶ月/20ID以上~ いろんなオプションはありそうだし、さすがに富士通って感がある SIベースなのかな? 新日鉄ソリューションズ \\(M&#94;3\\) DaaS@absonne http://www.absonne.jp/service/m3daas/menu3.php 画面転送、仮想デスクトップ型の2タイプ XenApp, XenDesktopベース 標準メニューではスペックが低め オプションで増やせそうだけど、価格不明 オプションはたくさん(書いてある): すべて価格不明 スナップショット、定期バックアップ ウイルススキャン 2要素認証とか、インターネットGW MDM ストレージサービス ADサービス 新日鉄という名前が私に、このサービスはSI前提だと囁いている 本当のところ、どんなサービスなんだろうか DaaSの国内シェアNo.1らしいし、利用者の話を聞いてみたい まとめ とりとめもなく、ggったことを書いてみたけど、やはりWindows連携が大事にされているっぽい。 Windowsのことはよくわからないけど、少しオンプレVDIのネットワーク特性については、興味が湧いてきた。 特性として、概ねブートストームと、CIFSの特性の2つで決まりそうだけど、機会があれば実験したい。(機会はたぶんない。)","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/06/07/0331.html","loc":"https://www.hitsumabushi.org/blog/2015/06/07/0331.html"},{"title":"WindowsをAnsibleで設定する","text":"資料 Windows Support — Ansible Documentation マシンの準備 ansibleコマンドを実行するマシン ansibleがすでに実行できる状況であれば、 pip install http://github.com/diyan/pywinrm/archive/master.zip#egg = pywinrm pip install kerberos # AD accountを使う場合 とすればOK。 Windowsマシン 要件 WinRM がインストール済み PowerShell version > 3.0 自動的に、Windows 7SP1, Windows Server2008 SP1以降になる Windows Server 2012ははじめからPowershell 4.0がインストールされている セットアップ方法 Powershell 3.0にアップデートが必要な場合 WinRMのインストール Configure-SMRemoting.exe -get で有効になっていることを確認 WinRM get WinRM/config でWinRMのポートを確認 ansibleの実行方法 Inventory 最小構成は以下の通り。 以降は、hostsという名前で呼ぶ。 [windows] hostname [windows:vars] ansible_ssh_user = administrator ansible_ssh_pass = _password_ ansible_ssh_port = 5986 ansible_connection = winrm 利用可能なモジュール raw, script, slurpなど Windows用のモジュール この辺 にあるやつはできそう windows machineの facts ansible -i hosts _hostname_ -m setup を実行する。 \"ansible_facts\" : { \"ansible_distribution\" : \"Microsoft Windows NT 6.3.9600.0\" , \"ansible_distribution_version\" : \"6.3.9600.0\" , \"ansible_fqdn\" : \"_FQDN_\" , \"ansible_hostname\" : \"_hostname_\" , \"ansible_interfaces\" : [ { \"default_gateway\" : null , \"dns_domain\" : null , \"interface_index\" : 13 , \"interface_name\" : \"vmxnet3 Ethernet Adapter #2\" }, { \"default_gateway\" : \"_GATEWAY_\" , \"dns_domain\" : null , \"interface_index\" : 14 , \"interface_name\" : \"vmxnet3 Ethernet Adapter #3\" } ], \"ansible_ip_addresses\" : [ \"_local_ipv4_\" , \"_local_ipv6_\" , \"_global_ipv4_\" , \"_global_ipv6_\" ], \"ansible_os_family\" : \"Windows\" , \"ansible_powershell_version\" : 4 , \"ansible_system\" : \"Win32NT\" , \"ansible_totalmem\" : 2147483648 }","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/05/28/0607.html","loc":"https://www.hitsumabushi.org/blog/2015/05/28/0607.html"},{"title":"Ravelloが便利","text":"VMware環境のテストをしたい 会社でVMware使っているのもあって、外でAPI叩いたりツールのテストしたいということがよくある。 今までは自宅でESXiとvCenter立ててやっていたんですが、やっぱりリソース的にVM数を増やしたりできないので、とても困っている。 あと、PackerとかでVMware用のイメージ作るときに使えるESXiも欲しかったので、どうにかできないかなー、というのもあって、 探していたらRavelloというサービスが面白そうだったので、紹介がてら書いておく。 Ravello とは Ravello は、テスト環境を作成するためのクラウドサービス。 裏では、AWSとGCEを使っているのですが、どちらにデプロイされるかは意識せずに使える。 というのも、きちんとオーバーレイネットワークだったり、ストレージオーバーレイが行われているため。 それで、AWSなどでESXiをデプロイする際の最大の問題になるCPUの仮想化支援ですが、Ravelloを使うと良きにはからってエミュレーションして、バイナリ変換してくれるらしい。 要は、Ravelloを利用すると、Public Cloud上にNested ESXiを構築できるようになるということ。 Ravelloを利用する Sign Up まだ、ESXiをデプロイする機能はベータ版っぽいので、以下のページからSign Upする。 http://www.ravellosystems.com/solutions/esxi-cloud/esxi-on-aws 制限事項 デプロイできるバージョンとしては、5.x, 6.0となっているみたい。 ただ、以下の通り、6.0ではWindows版のvCenterしか使えない。 たぶんvCSAのデプロイ方法が変わったせいだと思う。 6.0をテストしたいときには、Windows Serverが別途必要になるので、気をつけましょう。 version is supported ESXi 5.x yes ESXi 6.0 yes vCSA 5.x yes vCenter Server 5.x yes vcSA 6.0 yes vCenter Server 6.0 no 実際の手順 手順は、公式の資料を読もう。 1. http://www.ravellosystems.com/blog/create-esxi-5-5-iso-image/ 2. http://www.ravellosystems.com/blog/install-vcenter-server-on-cloud/ 3. http://www.ravellosystems.com/blog/vsphere-lab-environment-cloud/ Price Price List を見る。 element price VM(Cost) $0.14/h ~ VM(Performance) not fixed Storage $0.12/GB/month Library $0.12/GB/month IP $0.01/VM/hour Network転送量 $0.15/GB 以上のような表になっている。 試算 VM NUM spec vCenter 1 2vCPU/8GB mem, 125GB storage ESXi 3 4vCPU/8GB mem, 100GB storage NFS 1 2vCPU/4GB mem, 200GB storage という構成で作ると、 element price VM(Cost) $1.25/h Storage $0.2158/h IP $0.05 となって、$1.5/hくらいになる。（ただし、Library, Network転送量は除く） 安いコーヒー1杯分くらいで、自宅よりは快適な検証環境が作れそうだ。 この価格も、そもそもESXiに100GBのストレージとかいらないし、もう少し安くできそうではある。 まとめ的なの もうちょっとRavelloを使い倒していきたい。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/05/09/1822.html","loc":"https://www.hitsumabushi.org/blog/2015/05/09/1822.html"},{"title":"Xmonadの設定","text":"資料 http://xmonad.org/documentation.html Archlinux Wiki - Xmonad Xmonad/Config archive/John Goerzen's Configuration Xmonadの設定 1/3 「基本」編 - ナレッジエース 環境 Debian Sid & Xfce4 Install and Initial Configure Package install # basic packages sudo apt-get install xmonad libghc-xmonad-dev libghc-xmonad-contrib-dev # for adding launcher sudo apt-get install dmenu # for status bar sudo apt-get install xmobar # tools sudo apt-get install suckless-tools scrot tilda trayer Xsession add /usr/share/xsessions/custom.desktop [Desktop Entry] Name = Xsession Exec = /etc/X11/Xsession ${HOME}/.xsession xmodmap ~/ . Xmodmap exec xmonad Edit config import XMonad main = do xmonad $ defaultConfig Set xmonad as the default window manager $ sudo update-alternatives --config x-window-manager デフォルトのキーボードショートカットは、 man xmonad で見ることができる。 とりあえず、 Alt + Shift + p を覚えておけば、ターミナルは起動できるので、困らないはず。 Xmonad Setting w/ Xmonad 設定のためのTIPS xmonadの設定ファイルを書き換えたので反映させたい mod-q で反映される(restart) xmonad --recompile で構文チェックできる ショートカットを設定するときのクラス名を知りたい xprop というコマンドが役立つ xprop | grep WM_CLASS vmware環境でウインドウサイズの変更に追従してくれない vmware-user-suid-wrapper","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/04/05/0138.html","loc":"https://www.hitsumabushi.org/blog/2015/04/05/0138.html"},{"title":"Hatena Engineer Seminar #4に行ってきました","text":"Intro タイトルどおり、行ってきました。 目当ては、mackerelの話だったんですが、思ったより他の話も面白かったです。 今回は抽選だったし、参加できなかった人のためにも、メモと感想を書いておきます。 序盤の話はTwitterでつぶやいているので、参考までに。(後半はPCの電源が死んだのでないです) 資料は見つけたら追加します。 開会の挨拶 はてなの東京オフィスが増床するらしい。 今回の会場は、その増床先のオフィスで、まだ机など入れる前にやってみよう、という感じでした。 普通のオフィスとして使った時はどんな風になるのかわからないですが、今までのオフィス(今回の会場の1階下にあたる)も使うらしいので、ゆったりできそうです。 Goで書かれたmackerel-agentのOSS化や自動化にまつわるあれこれ(@Songmuさん) 資料はこれ 。 自分もためにmackerel-agentのソースを見たりしているので、この話を一番楽しみにしていた。 タイトルの通り、 mackerel-agentをOSS化 するにあたって、どういうことを考えているか、というお話でした。 mackerel-agent-plugins との対比もあって、微妙な違いがあり面白かったです。 個人的に一番興味を持って聞いていたのは、Travis CIを使ったリリースの自動化でした。 それぞれのリポジトリの.travis.ymlファイルを見ているとわかりますが、タグを打たれたときに、自動でテストしてデプロイしているみたいです。 それ自体はわりと珍しくもないと思いますが、OSSだからこそ大事だという話をされていて、なるほど、となりました。品質のためももちろんあると思うのですが、テストとかそういうものを整備して客観的な基準を作ることで、レビュー時のコミュニケーションロスを減らす目的のようです。 あと、なかなか真似できないですが、ユーザーがhackする余地を残す、というのもとても共感できました。 ちょっと頑張ればhackできそう、という雰囲気を出すのが、難しいと思うんですが、mackerelはとっても良く出来ていてすごいです。 あと、WindowsようにAppVeyorを使っている、と言っていたんですが、このサービスを初めて知りました。 はてなのサービスの開発環境(@astjさん) まず、2013年11月の段階のこのスライドを最初に読んだほうが良いです。 Vagrant と Chef でつくるはてなブックマークの開発環境 半分は、この話からのアップデート+詳細という形式の話でした。 上記のスライドは、どちらかというとツールのインストールまわりで困ります、という話だったんですが、今回は一歩進んで実際のミドルウェア設定まで考えた時に、本番との差異をどう吸収するのか、という話が聞けました。 ただ、個人的に悩みどころの、開発用DBのメンテの話とかはなかったので、あんまりちゃんと覚えられてないので、割愛します。(間違ったことを書きそうなので。) 個人的に気になったのは、chefでプロビジョニングすると古いものの依存関係を解決するのが大変、みたいな話があったんですが、packerとかでイメージ作っても良いのかな、と思いました。 ただ、hatenaでは、本番に使う用の古いライブラリrpmがあるみたいなので、どっちでもコストは大して変わらんのだろうな、という印象。 はてなブックマークの新機能における自然言語処理の活用(@skozawaさん) この話は最近 はてなブログに追加されたトピックという機能 の実装の話です。 この話は自分自身が咀嚼できていないことが多いのですが、ざっくり流れをまとめてみます。 大きくわけて、トピック生成の話と、トピックタイトル生成の話でした。 トピック生成の裏側は、Elastic Searchで、 significant_terms を2層にして使っているようです。 significant_termsを使うと、重要語みたいなの(これをトピックとする)がわかるので、それを2層にすることで、特徴語を見つけることができることになります。 トピックに属するエントリの判定は、この特徴語を用いて決めることができて、特徴語のスコアが80を超えたら、などの基準で、分類を行います。 ここまででトピック生成と、トピック内のエントリを分類できました。 次にトピックタイトルの生成です。 実際にページを見ればわかりますが、トピックは単語ではなく、文章です。 記事タイトルを持ってくるのが楽そうなのですが、例えば新聞名が入っていたりとかするので、うまく処理しないといけません。 処理の流れは、以下です。 重要語抽出: ESの機能のほか、TopicSumという考え方も利用 重要文抽出: タイトルのスコアを計測、スコアが高いものを利用 文圧縮: 係受け解析をする(Cabocha & ipadic) : 媒体名を除くなどのため。 実際に文圧縮するには、係り受け解析をしたりして、重要な部分だけを切り取っているそうです。 このあたりは試行錯誤の塊っぽいので、細かいことはフォロー出来ませんでした。 このお話は、試行錯誤の結果が見え隠れしていて、エキサイティングでした。 こういう仕事ができるエンジニアになりたいですね。 はてなのiOSアプリとSwift(@yashiganiさん) 資料というか本人のエントリ 正直、Swiftとかアプリとかあんまり興味ないな、とか思っていたんですが、発表者の話が面白すぎて、今はちょっとSwift書こうかと思っています。 Swiftの良いところ、悪いところ、実際に使うと困るところまで、コードベースで話していて、これは今からObjective-Cは書かないな、むしろSwiftって良い言語じゃね、くらいまで思わせられました。 詳しくは本人のエントリを読むのが良いです。 TypeScriptで実現するMVPアーキテクチャパターン(@nanto_viさん) この話は、 たしかこれ の実装話だった気がする。 (あまりちゃんとサイトまで見てなかったので、間違っていたらすみません...。) MVPモデルで実装することにした理由、またその中でTypeScriptを選択した理由、というお話を聞いていました。 自分はあまりフロントの話はわからないのですが、MVP自体は知識として知っているものの、どういう背景で出てきたモデルなのかよく知らなかったです。(というか自分がフロントの勉強し始めた時には、すでにMVVMがあるような時代だった。) 今回の話では、フレームワークは最小限でやっていたので、そのあたりの細かい話から聞けました。(ロジックとビューの分離をしてテスト可能にしたい。表示位置などのプレゼンテーションロジックもテストしたい、など。) TypeScriptの話もaltJSとしては、とても使いやすそうに聞こえていて、今後自分がJSを書くとすれば生JSではなくて、TypeScriptにしようかな、と思います。(少なくとも型のある言語にしたいです。) まとめ 今回はHatena Engineer Seminar #4は、はっきり言って全部の話が面白かったです。 Hatenaはやっぱり良い会社だと思うし、そういう会社で働いている人と切磋琢磨できるように、自分も面白いことをやっていかねば、と思わずにはいられない感じでした。 コーディング欲が増したので、懇親会には参加しませんでしたが、きっと懇親会も楽しい感じだったんだと思います。 またあったら参加したいですね。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/02/09/2245.html","loc":"https://www.hitsumabushi.org/blog/2015/02/09/2245.html"},{"title":"fioを使ったベンチマーク","text":"Intro ディスクベンチマークのツールとして、dbenchであるとか、もっと簡単にはhdparmなどがある。 今回はfioという、単純なread/writeの計測には必要十分なツールを使おうと思う。 ただ、ベンチマークツールはたいていオプションが多く、またアウトプットも複雑なので、一旦まとめてみる。 オプション オプションの与え方 以下の2つが使える。 コマンドラインオプションで与える オプションを記述したファイルを引数にする [global] rw = randread size = 256m directory = /tmp/fio-test ioengine = libaio iodepth = 4 invalidate = 1 direct = 1 [random-read] rw = randread size = 128m directory = /tmp/fio-test [random-write] rw = randwrite size = 128m directory = /tmp/fio-test 主なオプション option value 意味 rw read, write, randread, randwrite, randrw ベンチマークの内容を決める。randrwはMixさせるもの。rwmixread=40でread 40%。 bs <int>[, <int>] (default: 4k) ブロックサイズ。2つ与えられていると、read, writeになる。 size <int>, 64{k,M, G, T, P}, 10% など Job全体のIOサイズ。単位はb。 numjobs <int> (default: 1) 同じワークロードを実行するスレッドをいくつ生成するか。 directory <str> fioで使うファイルのプレフィックス。 name <str> ジョブ名を上書きする ioengine sync, psync, vsync, libaio, ... Job IOをどのように行うかを決める。 iodepth <int> (default: 1) ファイルに対するIO書き込みのユニット数。IO waitを作り出すのに使ったり、複数のヘッドがあるときに使われる? direct 0 or 1 (default: 0) 1、つまりtrueの時、 non-buffered IOを使う。(たいていは O_DIRECT) invalidate 0 or 1 (default: 1) IO計測の前に、キャッシュを使わないようにしておく runtime <int> 実行最大時間 thinktime <int> IO発行の間で、Jobを止める。単位は microsecond(μs) fsync <int> (default: 0) IOが<int>与えられるたびに、fsyncを呼ぶ。 0の時は呼ばない。 write_iolog <str> IOパターンの書き出し。各Jobで異なるファイルを指定する必要がある。 read_iolog <str> IOパターンの読み出し アウトプットの見方 実行例 # fio -filename=/mnt/test2g -direct=1 -rw=randwrite -bs=4k -size=2G -numjobs=64 -runtime=10 -group\\_reporting -name=file1 file1: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=1 ... file1: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=1 fio-2.1.3 Starting 64 processes Jobs: 64 (f=29): [wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww] [100.0% done] [0KB/315KB/0KB /s] [0/78/0 iops] [eta 00m:00s] file1: (groupid=0, jobs=64): err= 0: pid=3845: Mon Feb 9 20:10:11 2015 write: io=3256.0KB, bw=307748B/s, iops=75, runt= 10834msec clat (msec): min=7, max=10192, avg=205.55, stdev=1093.89 lat (msec): min=7, max=10192, avg=205.55, stdev=1093.89 clat percentiles (msec): | 1.00th=[ 8], 5.00th=[ 9], 10.00th=[ 10], 20.00th=[ 11], | 30.00th=[ 12], 40.00th=[ 13], 50.00th=[ 13], 60.00th=[ 14], | 70.00th=[ 15], 80.00th=[ 16], 90.00th=[ 20], 95.00th=[ 586], | 99.00th=[ 6849], 99.50th=[ 9110], 99.90th=[10159], 99.95th=[10159], | 99.99th=[10159] bw (KB /s): min= 0, max= 332, per=18.29%, avg=54.86, stdev=114.66 lat (msec) : 10=11.30%, 20=78.99%, 50=3.07%, 100=0.12%, 250=0.12% lat (msec) : 750=2.95%, 1000=0.37%, 2000=0.25%, >=2000=2.83% cpu : usr=0.00%, sys=0.00%, ctx=1726, majf=0, minf=1909 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued : total=r=0/w=814/d=0, short=r=0/w=0/d=0 Run status group 0 (all jobs): WRITE: io=3256KB, aggrb=300KB/s, minb=300KB/s, maxb=300KB/s, mint=10834msec, maxt=10834msec Disk stats (read/write): sdc: ios=0/801, merge=0/2, ticks=0/10620, in\\_queue=10624, util=98.48% 実行結果の意味 key value bw バンド幅 clat completeion latency。リクエスト送信から終了までの遅延時間 IO depths 実行時のリクエスト待機状態。submit 以下の行は、IOリクエストまでに要したレイテンシ情報。この例では、0ms~4msですべて処理されている。 WRITE ここのスレッドにおける、帯域幅の平均など 資料 man fio ファイルシステムのベンチマーク集 fioを用いたディスクIOのパフォーマンス測定 Provisioned IOPSの検討 - JPOUG Advent Calendar 2012","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/02/09/2227.html","loc":"https://www.hitsumabushi.org/blog/2015/02/09/2227.html"},{"title":"Easter Egg Collections","text":"イースターエッグは、見つかるとよく話題に上がっているけど、 実際にどんなものがあるのか、よく知らないなー、と思ったので集めようと思う。 ただ、調べてみると、昔あったものがなくなってたりして、このリストも最新状態に保つ必要があるので、なくなってたら教えてほしいです。 とりあえずは、いろいろ見つけるたびに徐々に更新していきたいと思います。 イースターエッグたち Vim 最初にこれを書くきっかけになったものを挙げておこう。 :h quotes.txt : プログラマーがじゃぶじゃぶコード書きたく鳴るような射幸心を煽りまくるvimの説明文 が見られる。 :h 42 もある。 Emacs こいつは何がイースターエッグなのかわからない。 ゲームができるのは、普通だし、何を挙げれば良いんだろうか。 M-x doctor M-x butterfly : これは探してて面白かった。元ネタは これに説明されてるっぽい apt, aptitude Debian/Ubuntuなどでよく見る、 This aptitude does not have Super Cow Powers. , スーパー牛さんパワーなどはありません。 という文章、この周辺のイースターエッグがある。 最近のAPTには、よく使われるコマンドがapt-get, aptitude, apt と3種類あるので、それぞれ調べよう。 apt-get moo , aptitude moo , apt moo aptitude upgrate apt-get | tail -1 aptitude -v moo aptitude -vv moo aptitude -vvv moo aptitude -vvvvv moo aptitude -vvvvvv moo (apt コマンドだけ少ないなー、と思って、ちょっとずるをしてしまって、aptコマンドのmoo周りのソースを読んでしまった。 実際に自分が実行してその文が表示されるまで、ここには書かないで楽しみとして取っておきたいと思います。) Google 検索結果系 do a barrel roll :画面がまわる tilt , askew : 画面が傾く Google Gravity : 崩れる。しかも普通に使える recursion : もしかして、が再帰 999999..999999 と検索するとエラーになる ゲーム zerg rush atari breakout Pacman google map この場所で、街灯の下の白いバッテンをクリックする 電卓系: 検索すると電卓の結果がでる 人生、宇宙、すべての答え =42 the loneliest number =1 a bakers dozen =13 number of horns on a unicorn =1 once in a blue moon =1.16699016 × 10-8 ヘルツ 表示言語の設定におかしいものがいくつかある。 Bork, bork, bork! Hacker クリンゴン語 ラテン語風 イースターエッグを調べていてたまたま見つけた、 昔のグーグルっぽいページ おまけ) 過去のDoodle Python pythonのインタプリタとかで import this とやる Chrome ネット接続していないとき(Developer tooolsでも可)に、ページを検索してエラーページを出す。スペースキーを押せば、ゲームができる。 nmap nmap -oS - example.com : Hacker語での表示 telnet? というか何というか telnet towel.blinkenlights.nl rails forty_two : (1..42).to_a.forty_two # => 42 資料集 http://nanapi.jp/30883 http://allabout.co.jp/gm/gc/296865/ http://toomva.blog60.fc2.com/blog-entry-278.html http://www.techgyd.com/google-easter-eggs-tricks/12438/ http://emacsredux.com/blog/2013/04/03/easter-eggs/ http://d.hatena.ne.jp/mickey24/20080808/1218161057 https://www.digitalocean.com/community/tutorials/top-10-linux-easter-eggs","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/02/07/2144.html","loc":"https://www.hitsumabushi.org/blog/2015/02/07/2144.html"},{"title":"Consistent Algorithm","text":"Consistent Hash Algorithmという負荷分散などの目的で使えるアルゴリズムがある。 たまたまarxivでシンプルで高速, 省メモリな実装についての論文を見つけたので、読んだ。 読んだ内容 pandoc+beamerを試してみるついでに、スライドにまとめた。 疑問点 論文では、キャッシュとしては使いづらい、という趣旨のことが書かれている。ノードのIDをかぶらせてもたせるだけではダメ? ノードを削除するとき、ノード側でリバランスする必要があるが、その場合には別途方法を考える必要があるように思う。(IDの再計算はそんなに難しくはなさそうに思う)","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/02/01/0033.html","loc":"https://www.hitsumabushi.org/blog/2015/02/01/0033.html"},{"title":"CROSS 2015のアンカンファレンスで話しました","text":"CROSS 2015のアンカンファレンスで脆弱性の評価って困る、話をしました。 スタッフやりつつ、当日資料作りもやったので、あまり資料としての完成度はないです。 たぶんプレゼン作成時間30分くらい。 書いてある通りなのだけど、脆弱性とかセキュリティの話は、リスクが無限大に評価されがちに見えるのをどうにかしたい。 自分自身は何か情報系の教育を受けてきたわけでもなく、我流で対応しているのですが、未だに目安として定量的に判断する方法を持っていない。(ただしCVSSスコアを除く)。 会社としては、どんだけバズった脆弱性でも対応しないとなったら、説明責任があるので、そういうときに説明できるような基準を自分で持たないといけないな、と最近は思っているわけです。(話題になってないけど、対応した、みたいなのも、本当は理由をもっておくべきだけど、優先度は低いはず...。) 何か良い方針があれば、教えてください。 あと、最後のページに書いてある本を輪読会したいので、興味がある方がいれば、 @_hitsumabushi_ まで。 やるとしたら、平日は新宿・渋谷とかの辺が良いです。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/31/0320.html","loc":"https://www.hitsumabushi.org/blog/2015/01/31/0320.html"},{"title":"CROSS2015の運営をしたので、やったこととわかったことを書いておく","text":"わかったことざっくり 大規模イベントは、全体を把握している人はいない、という気持ちを持つことが大事。(実際いない) 直前に言われたときほど、利害がはっきりしているので、お互いにメリットがありそうな道を出しやすいので、出す 実行委員長(山口さんという方)すげー。よくわからんけど、強力してくれる人を探してくる。 経緯 2015年の1/29に大さん橋ホールにて、 CROSS2015 というイベントの運営をしてきました。 当日は都心から離れているにも関わらず、たくさんの参加者に来ていただいたのですが、どうにか大きなトラブルはなく、無事に終えることができました。 1日中立ちっぱなしだったり、細かい備品調整の話があって、大変なのは大変でしたが、半年前くらいから準備をしていたこともあり、なんとかやりきった、という気持ちが強い状況です。 やったこと 1000人規模のイベントをやるには、おそらくみなさん1年前~9ヶ月前くらいには準備を始めていると思います。 ただ、今年のCROSSは開始が遅く、半年前からの準備になりました。 時系列順に私がやっていたこと・やるべきだったことをまとめておきます。 全体的な役割 会場の手配 会場に用意する備品・機材の手配 他の係が使用するリソースの調整 当日のネットワークをつくっていただくCONBUとのやりとり 当日は雑用。備品調整, ケータリングの搬入, ゴミ捨て, etc... スタート時期 : 半年前~5ヶ月前 とにかく会場を探す。 CROSSの場合は、イベンターさんにお願いしつつ、自分達でも探してみていた。 この時期には、基本的に会場が埋まっているため、会場スペックの優先度をつけ、評価基準を決める。 評価パラメータとしては、 広さ, 天井の高さ 交通アクセス 料金 (会場のみパターン, 設営業者を含めたパターン) 最大利用時間 搬入のしやすさ(駐車場から会場までの通路の広さなど) 受付(を設営するであろう)場所の広さ ← 参加者が受付に並ぶことになるので。 スピーカー、電源などの位置と数 (工事費に関わる。この段階では工事の見積もりはもらえないことが多いので、参考に知っておく) 回線があるか。自分で引けるかも確認。 ... などがあります。 全部を満点にする会場はないので、重視する項目を考えておく必要があります。 ただ金額が大きくなるため、料金をどれだけ優先するかなど、 概して会場担当に決定権はないので、事前に意思決定者に確認しておきましょう。 会場決め(遅い) : 5ヶ月前~4ヶ月前 会場を絞り、現地に観に行くなどして、会場を決めます。 それと同時に、当日のレイアウトの概要を仮決めします。 おそらく、企画系の係については、まだ具体的なことが決まっていないため、意見を聞いても無駄です。 聞かれた方も困るので、とにかく先に会場レイアウト案を作って、不満点を聞くのが良いようです。 このあたりから、実際の金額の見積もりをして、いくらに収めれば良いのか、もう一度確認をしておきましょう。 会場やイベンターさんから(ざっくりでも良いので)見積もりをもらえるようにしておきましょう。 会場費はハコの値段であって、下がらないのが普通のようです。 その他の部分として、業者を会場指定の業者にしたら下がらないか、とか、細かいことは聞いてみた方が良いです。 (大さん橋ホールは設営・工事について、特に指定はありませんでした。) 折衝の時期 : 3ヶ月前~2ヶ月前 この時期になると、会場のことはほとんど見えてきているはずです。 他の係や業者との折衝が主な役目になってきます。 特に企画系のものが具体的になり始めるため、こういうことをやりたいのでスペース欲しいです、などの話が来ます。 最低限、以下の項目は確認しましょう。 広さ 時間 必要な備品 たくさん人が来そうか セッション会場の近くに置いて良いか 特に騒がしいものは、実際にやってみないと影響は読みづらいので、できるだけ離しましょう。 ただ、相手の方もやるなら良い場所を、という風に思っているようなので、交渉になると思います。 自分の場合、どうしても外に出てやってほしい企画があったのですが、話し合いの末折れてしまいました。 結果論ですが、話し合いの最想定していた以上に騒がしくなってしまい、周囲の参加者に不便をかけてしまったのは、無責任だったと反省する点です。 2ヶ月前~1ヶ月前 自分はこの時期、ほとんど参加できませんでした。 なので、見ていた限りの話になります。 1ヶ月前になってしまうと、備品・工事など、いろいろな調整ができなくなってくるので、 FIXするための駆け込みの確認・変更が増えます。 もしかすると、Q&A票を用意して、3,4日に一回そのまま会場などに聞ける体制を用意した方が良いかもしれません。 今回は、質問を会場係が受け取り咀嚼して、改めて会場などに電話・メールで聞く、というフローになっており、ボトルネックが会場係になりかねない状況でした。 できるなら、初めからQ&A票でやりとりしておき、その票で直接答えてもらうようにしておくと、負担は減ると思います。 直前 当日のフローの確認がメインです。 だいたい何を言われても、調整できることは限られているので、できる範囲でやりましょう。 前日・当日 やるだけ。 自分が意識してやったことは、以下の通り。 だいたいトラブルが起きた時のフローは、自分の中で確認しておく。 連携のために、各担当者のうち少なくとも2人ずつくらいは名前を覚えておく。 明らかに作業担当が1人になっている作業は、手伝って、SPoFを減らす。 つらいこと 昨年ベースのことが多すぎて、わからない 自分は昨年参加していないのだけど、 昨年の資料から読み取れることが少ない。 そのため、一切相談なく、「こうやるもんでしょ」みたいな言い方をされることがある。 だいたい別に確認したら、昨年がそうだった、という話。 そういうこともあるんだな、くらいに受け止めるようにする。 事前に意思決定者に相談していても、なかったことになっている 人数が多い会議あるあるなので、特にいうことはない。 つらいことではあるけど、そういう場合は証跡を残していない自分のせいもある。 感想 書いているうちに取り留めのない感じになってしまいました。 とりあえず、半年間準備して、無事終わることができて、 CROSSに来ていただいた人、一緒に活動したスタッフ、関係してくださった全ての人にお礼が言いたいということで。 ありがとうございました。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/30/2209.html","loc":"https://www.hitsumabushi.org/blog/2015/01/30/2209.html"},{"title":"Dockerのプロキシ設定","text":"自宅ではプロキシを立てていないので問題なかったが、会社でDockerをいじろうとするとプロキシに阻まれてうまくいかず困っていた。 bashの環境変数を設定するのはうまくいかなくて、しばらく手元ではdockerをやらず、作業用マシンをクラウドに立ててどうにかごまかしていたのだけど、 あらためて考えると解決できた。 結論は、dockerのデーモンが起動するときに、プロキシの設定をしておく必要がある、というだけでした。 /etc/default/docker に以下を記載します。 export http_proxy = \"プロキシのIP\"","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/29/0754.html","loc":"https://www.hitsumabushi.org/blog/2015/01/29/0754.html"},{"title":"DNS サーバーの比較資料集め","text":"自宅開発環境を一新するついでに、真面目にサーバー構成を見直すことにした。 DNSサーバーは今までbind+dnsmasqでやっていたが、改めてパフォーマンスの観点から選定したい。 以下に、参考ページを列挙する。 参考になるページ 権威サーバ DNSサーバパフォーマンス評価 NicTool PowerDNS + NSD で DNS を構築する キャッシュサーバ DNSキャッシュサーバ 設計と運用のノウハウ DNSキャッシュサーバ チューニングの勘所 日本Unboundユーザー会 内容 権威サーバ BIND, PowerDNS, NSD, Knot, Yadifaなどの実装がある。 BINDしか知らない人に対する注意点として、上記のDNSサーバの多くは、権威サーバとしてしか動作しない、ということがある。 BINDではしばしば問題にされているのを見るが、権威サーバとキャッシュサーバは分けておくほうがセキュリティ上のリスクは減るので、いっそ異なるサーバを使うようにしたいと思う。 特に、NSD, Knot, Yadifaはパフォーマンスが優れている。 パフォーマンスについては、以下のページを見ると良い。 Benchmark - Knot DNS Benchmark - Yadifa キャッシュサーバ BIND, Unboundなどがあるよう。 Unbound/NSD最新情報（OSC 2013 Tokyo/Spring） を見ると、Unbound(, NSD3, NSD4)の概要がわかる。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/26/0312.html","loc":"https://www.hitsumabushi.org/blog/2015/01/26/0312.html"},{"title":"vShere Beta Program の注意点","text":"VMware のBetaプログラムの利用について、基本的なルールがあるので、忘れないようにまとめた。 ここに書いてあることは、オレオレ要約なので、きちんと自分で確認してください。 ざっくり言うと、「ここで知ったことは他で話すなよ」ってことでした。 資料 vSphere Beta Program 利用方法 上記ページから\"Join Now!\"して、規約に同意すれば良い。 規約は以下の2つあるので、それぞれ同意することになる。 VMware Master Software Beta Test Agreement(MSBTA) vSphere Program Rules vSphere Program Rules の内容 MSBTAの方は、一般的な利用規約に近いもののようなので、省略。 ライセンスは限定的な利用に限るだとか、そういったことが書かれている。 vSphere Program Rulesについては、以下のような内容になっている。 VMware vSphere Beta Program Ground Rules ベータプログラムの情報を受け取ることへの同意 フィードバックや質問は、Beta communityを使うこと。issueなどは、Support Requestを使うが、その解決はSLAなし フィードバックに対して、VMwareから反応があることもあるし、ないこともある バージョン番号とか付いているかもしれないけど、リリース版でどうなるかは謎なので、注意 VMware vSphere Beta Program Confidentiality Rules 許可されて いる こと Beta Program参加者と、このプログラムについて話すこと 保護されているプライベートなディスカッションフォーラムに投稿すること VMwareとのNDAの範囲内でサーバーベンダーとこのプログラムについて話すこと 許可されて いない こと このプログラムについて、公に議論すること 上記(たぶん許可されていることのリスト)のリスト以外の人と、プライベートに議論すること","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/25/2101.html","loc":"https://www.hitsumabushi.org/blog/2015/01/25/2101.html"},{"title":"VCP5-DCV取得した","text":"最近のブログはVMwareの話ばかりだったのですが、やっとVCPを取得してきました。 VCP-DCVを取得している人はたくさんいるので、個人的に受験した所感を書いておきます。 VCPトレーニングと試験内容のギャップ トレーニングを受講しても、おそらく言われると思いますが、 VCP-DCVの試験はトレーニング内容と差が大きいと聞きます。(他のベンダー試験を受けたことがないので、わかりません) 聞いた内容では、ベンダーの資格試験は、トレーニング内容だけで合格ラインか、合格ライン+αくらいまではいけるのが普通だけど、 VMwareの試験はトレーニングの内容だけでは5割くらいしか取れなくて、自分で調べないと厳しい、という話でした。 とはいえ、実際に自分が受けた試験は、トレーニングの内容をきちんと理解していれば6割はギリギリ行けそうかな、という風に感じました。 自分は5.5対応の試験を受けたのですが、聞いたのは5.1までの試験のことだったのかもしれません。 ただ、トレーニング内容と試験にギャップがあること自体は感じたのは、同じでしたので、 個人的にトレーニングを受講した後、実際の試験までにどのあたりの項目について気をつけるべきか、書いておきます。 ただ、もう試験の問題については覚えていないし、具体的に書くのもダメなので、ざっくり所感程度の形で書いておきます。 具体的に感じたギャップ VMware Data Recovery (VDR) 少し古いからか、自分が受けたトレーニングではほとんど重要視されていないように見えた。 でも、試験には出てきた。 VMware Data Protection(VDP), VMware Data Protection Advance 上述のData Recoveryの後継。 Data Recoveryからの新機能、Advanceとのライセンス上の違いまで含めて知っておくと良さそう。 VMware Update Manager(VUM) VUMで何が管理できるか、VUMの設定方法は必須のよう。 vSphere ClientとvSphere Web Clientの差 上述のVDRやVDPはWeb Clientをメインにしているが、VUMはvSphere Client からしかできない操作があるためだと思う。 常用している人でも、両方のクライアントを使っている人はあまりいないと思うので、気をつけておくと良い。 VCP-DCVを取得して 自分自身があまり資格に興味がないこともあり、あまりキャリアについての有用性はわかりません。 ただ、VMware環境を運用するのに重要な知見を学ぶ機会は得られたかな、と感じています。 また、vCenterは、5.1, 5.5でアーキテクチャが大きく変わった部分があり、古い情報からの変更点を知るのにも役立ちました。 あとは、普段読む気にならないドキュメントをいろいろ読む機会にもなりました。 そういう意味では、資格取得は良いきっかけになったと思っています。 VCPのトレーニング受講だけでも高いので、会社などで機会をもらえる人は、自分の運用を見直すきっかけとして利用できると思います。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/16/2348.html","loc":"https://www.hitsumabushi.org/blog/2015/01/16/2348.html"},{"title":"vDSのポートバインドタイプ","text":"資料 VMware KB: ESX/ESXi でのポート バインド タイプの選択 ポートバインドタイプとは vNICをvDSに接続するとき、ポートグループのポートがどのようにVMに割り当てられるかを、ポートバインドタイプとして、設定できます。 バインドのタイプは以下の3つから選択できました。(2つめの動的バインドは、ESXi5.0で廃止。) 静的バインド (Static Binding) 動的バインド (Dynamic Binding) 短期バインド (Ephemeral Binding) 静的バインド vNICが作成された段階で、直ちにポートにアサインされ、削除された時に初めて切断される。 vCenter Server経由の時に利用可能。 動的バインド VMがパワーオンされていて、vNICが接続状態の時のみ、ポートにアサインされる。 パフォーマンスの観点から非推奨。 短期バインド VMがパワーオンされていて、vNICが接続されている時に、ポートが作成され、アサインされる。 VMがパワーオフされるか、vNICが切断された時に、ポートが削除される。 短期バインドの場合のみ、vCenterがダウンしている際、VMのネットワーク接続を管理することができる。 256個までしか作れないとかパフォーマンスの問題から、リカバリ目的にのみ使用することが推奨。 ポート数について vSphere 5.1以降、静的ポートバインドの利用時、自動的にポートグループを拡張されるようになっている。 vSphere 5.0では無効だが、MOBのReconfigureDVPortgroup_Taskを変更して有効にすることもできる。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/10/2349.html","loc":"https://www.hitsumabushi.org/blog/2015/01/10/2349.html"},{"title":"vSphere 5.5環境でMSCSクラスタを組むときの制約","text":"資料 MSCSのサポート状況 VMware KB: Microsoft Cluster Service (MSCS) support on ESXi/ESX VMware KB: MSCS support enhancements in vSphere 5.5 MSCSとは http://www.atmarkit.co.jp/ait/articles/0812/03/news138_5.html MSCS Microsoft Cluster Serviceのこと。 MSFC(Microsoft Failover Cluster)と名称が変わっているけど、未だにMSCSと呼ばれる場合もある。 複数台について、フェイルオーバー型のクラスタを組める。 1台だけをマスターにして、他は待機系として構成する。 VMware環境での利用 クラスタリング一般の注意 クラスタリングのハートビートとして、 ハートビート ネットワーク 共有ディスク(クォラムディスク) が存在するものが多い。 この2つは構成上重要なので、以下ではここに注目する。 サポートされている構成 3種類のクラスタリング構成がサポートされている。 1つのホストでの MSCS 仮想マシンのクラスタリング （CIB） 物理ホスト間での MSCS 仮想マシンのクラスタリング （CAB） MSCS 仮想マシンを使用した物理マシンのクラスタリング （N+1） ※ この構成は、スタンバイ用のホスト1台を置いておき、他のホストのものとクラスタリングするもの。 制約 コンポーネント 条件 仮想SCSIアダプタ Windows 2003 => LSI Logicパラレル, 2008 => LSI Logic SAS ディスクフォーマット シックプロビジョニング、かつ、eagerzeroedthick 共有ストレージ (CIB) 仮想ディスクが推奨。仮想RDMも可 共有ストレージ (CAB) 物理RDMが推奨。仮想RDMも可 共有ストレージ (N+1) 物理RDMが利用可能 ストレージアダプタ ざっくりいうと、iSCSI, FCoEはサポートされている その他の制限事項 NFSディスク上のクラスタリングは不可 NPIV不可 FTとの併用不可 など。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/07/2029.html","loc":"https://www.hitsumabushi.org/blog/2015/01/07/2029.html"},{"title":"VMware VDP, VDPA","text":"VMware Data Protection 資料 vSphere 5.5 の新機能紹介 - VMware Blogs VMware vSphere Data Protection のドキュメント 概要 VMware Data Recoveryの後継みたいなものっぽい。 VDRから発展した点としては、以下があある。 FLR(File Level Restore) をサポート サポート台数の増加 ライセンス体系 VMware Data Protection VMware Data Protection Advanced の2つあって、いくつか違いがあります。特に、容量について、VMware Data Protectionは2TBまで。 VDPとVDPAの違い dedupストレージの上限拡大(2TBから8TBになる) バックアップ データ レプリケーション Microsoft SharePoint 対応エージェント EMC Data Domain システムへのバックアップ (元々は、vmdkにしか保存できない) 自動バックアップ検証機能 (一時的なVMにリストアして、vmware-toolsのハートビートを検証。NICは抜いてある) 以下の比較図は、 vSphere 5.5 の新機能紹介 - VMware Blogs からのもの。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/07/1924.html","loc":"https://www.hitsumabushi.org/blog/2015/01/07/1924.html"},{"title":"ESXiをkickstartでインストールする","text":"ESXiをPXEブートして自動インストールする方法について、日本語での説明があまりなかったので、メモとして残しておく。 利用OS ESXi 5.5 Debian 7.7 (DHCPサーバー, HTTPサーバーを兼務させる) DHCP, TFTP, HTTPサーバーの準備 必要なソフトウェアのインストール apt-get install tftpd-hpa isc-dhcp-server xinetd apache2 DHCPサーバーの設定 BOOTPでインストールするため、あまり自由度がない。 ddns-update-sytle none; option domain-name \"example.org\"; option domain-name-servers 192.168.0.10; default-lease-time 600; max-lease-time 7200; log-facility local7; subnet 192.168.0.0 netmask 255.255.255.0 { option routers 192.168.0.1; option subnet-mask 255.255.255.0; # 日本時間 #option time-offset 32400; range 192.168.0.30 192.168.0.240; next-server 192.168.0.10; filename = \"pxelinux.0\"; } TFTPサーバーの設定 適当なディレクトリ以下を公開する。 ここでは、/srv/tftp とする。 kickstartイメージの準備 まずESXiのイメージの内容をもらってくる cd /srv/tftp/ mkdir esxi mount -o loop <ESXiのISO>.iso rsync -a /mnt/esxi/ 次に、pxe boot用のイメージを準備する。 必要なのは、pxelinux.o, pxelinux.cfg/ wget http://ftp.nl.debian.org/debian/dists/wheezy/main/installer-amd64/current/images/netboot/netboot.tar.gz tar xvf netboot.tar.gz PXEブートの設定ファイル pxelinux.cfg/default default esxi label esxi kernel /esxi/mboot.c32 append -c /esxi/boot.cfg ks=http://192.168.0.10/servers/ks.cfg esxi/boot.cfg * もともとあるファイルの / を除去していく * prefixを代わりに設定 bootstate=0 title=Loading ESXi installer prefix=/esxi50/ kernel=tboot.b00 #kernelopt=runweasel modules=b.b00 --- useropts.gz --- k.b00 --- a.b00 --- ata-pata.v00 --- ata-pata.v01 --- ata-pata.v02 --- ata-pata.v03 --- ata-pata.v04 --- ata-pata.v05 --- ata-pata.v06 --- ata-pata.v07 --- block-cc.v00 --- ehci-ehc.v00 --- s.v00 --- weaselin.i00 --- ima-qla4.v00 --- ipmi-ipm.v00 --- ipmi-ipm.v01 --- ipmi-ipm.v02 --- misc-cni.v00 --- misc-dri.v00 --- net-be2n.v00 --- net-bnx2.v00 --- net-bnx2.v01 --- net-cnic.v00 --- net-e100.v00 --- net-e100.v01 --- net-enic.v00 --- net-forc.v00 --- net-igb.v00 --- net-ixgb.v00 --- net-nx-n.v00 --- net-r816.v00 --- net-r816.v01 --- net-s2io.v00 --- net-sky2.v00 --- net-tg3.v00 --- ohci-usb.v00 --- sata-ahc.v00 --- sata-ata.v00 --- sata-sat.v00 --- sata-sat.v01 --- sata-sat.v02 --- sata-sat.v03 --- scsi-aac.v00 --- scsi-adp.v00 --- scsi-aic.v00 --- scsi-bnx.v00 --- scsi-fni.v00 --- scsi-hps.v00 --- scsi-ips.v00 --- scsi-lpf.v00 --- scsi-meg.v00 --- scsi-meg.v01 --- scsi-meg.v02 --- scsi-mpt.v00 --- scsi-mpt.v01 --- scsi-mpt.v02 --- scsi-qla.v00 --- scsi-qla.v01 --- uhci-usb.v00 --- tools.t00 --- imgdb.tgz --- imgpayld.tgz build= updated=0 kickstart file 以下のファイルを /var/www/servers/ks.cfg として保存。 (ここでは、httpでやっているが、tftpでやっても良い。) # accept VMware EULA vmaccepteula # Setting rootpw # for generating encrypted pasword : Use openssl passwd -1 (rootpw) rootpw --iscrypted .... # install target install --firstdisk --preservevmfs # network setting network --bootproto=static --ip=172.22.159.146 --gateway=172.22.158.1 --netmask=255.255.254.0 --vlanid=19 --hostname=hoge.example.com --addvmportgroup=1 # reboot after install reboot %post --interpreter=python --ignorefailure=true import time stampFile = open('/finished.stamp', mode='w') stampFile.write(time.asctime()) $1$nz6kimqN$gSlFdxZnUncPbAxaFeKaC1 %firstboot --interpreter=busybox vim-cmd hostsvc/enable_ssh vim-cmd hostsvc/start_ssh vim-cmd hostsvc/enable_esx_shell vim-cmd hostsvc/start_esx_shell # enable password login (SSH) sed -i \"s/PasswordAuthentication no/PasswordAuthentication yes/g\" /etc/ssh/sshd_confg 以上で、キックスタートの準備が終わる。 考えること サーバーごとの個別設定の渡し方 事前にMACアドレスがわかっている場合にはできるけど、そうでない場合には、難しい。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/06/2338.html","loc":"https://www.hitsumabushi.org/blog/2015/01/06/2338.html"},{"title":"vCloud Airの価格感を調べる","text":"資料 VMware vCloud Air VMware vCloud Air 価格ガイド vCloud Airとは VMwareが提供している、ハイブリッドクラウドのサービス。 特徴として、オンプレでVMware環境を稼働させている場合に、vCloud Connectorを利用することで、 vSphere Clientのプラグインと統合して操作ができることが挙げられる。 日本では、 ソフトバンクと提携しているようだ 。 vCloud Air の価格感を調べる 大前提として、一般的なクラウドサービスと異なり、従量制ではない、というところは気をつけたい。 プラン ざっくり3つのタイプがあるということらしい。 前者2つがコンピュートリソース、つまり普通にサーバーなどを立てるために使うもの。 3つ目は普段オンプレにあって、オンプレが災害などで落ちたときに使われるもの。 Dedicated : 物理的に分離。いわゆる、物理専有タイプ。 Virtual Private : 論理的に分離。いわゆる、リソース専有タイプ Disaster Recovery : 困った時にだけ起動するタイプ。 以下ではDRタイプのものを調べたい。 DRの価格帯系 VMware vCloud Air 価格ガイド を参考にすると良い。 DRの場合の契約期間は、1, 12, 24, 36ヶ月なので、それほど問題にならないと思う。 サービス 含まれるリソース 価格 （月額） 単価 プロセッサ、メモリ 20GB vRAM, 10GHz CPU ¥26,014 ¥1.78/GB/時 ストレージ＆サポート 1TB Storage ¥30,503 ¥30.5/GB/月 帯域幅 10Mbps ¥26,359 ¥3.61/Mbps/時 グローバルIPアドレス IPアドレス単位 (含む 2 ) ¥5,756 ¥2,878 都度 オプション サービス データプロテクション 1TB ¥49,495 ¥49.5/GB/月 ダイレクトコネクトポート 1Gbps ¥8,633 ¥11.83/Gbps/時 ダイレクトコネクトポート 10Gbps ¥28,776 ¥3.94/Gbps/時 オフラインデータ転送 11TB One time Data Transfer ¥28,776 都度 ¥28,776 都度 さすがに基本プランだと、あまり使えないので、だいたいオプションに申し込むことになるのだと思う。 割と安いけども、当然ながら、制約があるっぽい。 特に、実際にDRとして使う場合には、30日間しか連続起動できない。 所感 最小構成は安いが、すべてをDRしようとすると、いろいろ高くなりそう。 取捨選択をして情報資産の棚卸を一緒に行えば、かなり便利。 特にDRできるか、実際にテストできる機能が便利。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2015/01/05/1735.html","loc":"https://www.hitsumabushi.org/blog/2015/01/05/1735.html"},{"title":"2014年のまとめをしておかねば","text":"今年1年は反省の多い年だったので、きちんと書いて残しておく。 2014年面白かった技術、触りたかったソフトウェア 新たに導入したもの Ansible Ceph Pelican(このブログを生成してるやつ) Flask ちょっとしか触れず... VyOS Consul Fluentd Golang Docker CoreOS Hashicorpのツールその他 Hadoop 触りたかったけど、触らず Strom Rust 2014年のトピックス 思い返すと1月のこととかは何年も前に感じるが、今年は割と色々あった。 自分の中でトピックとして大きい出来事を中心に書いてみる GGJに参加した 配属されたこと 仕事全般 GGJに参加した 研修で滞在していた北海道で、Global Game Jamというイベントに参加した。 作ったゲームはこれ。 http://www.kawaz.org/projects/weaver/ 右も左もわからない状況で参加したが、3日間のなかにゲーム開発の流れがつまっていたと感じる。 私が担当していたのはパーティクルつくるのとレベルデザインの一部だけだけど、レベルデザインは奥が深くて面白かった。 ユーザーに対して、いかに自然にギミックを伝えるか、というデザインが非常に大事で、目から鱗だった。 こういうハッカソンに時々出ると、できないならできないで色々学ぶ事はあるし、できるようになればもっと楽しいと思うので、来年も時間を見つけて出たい。 GGJについては、2014年は周囲の人の力を借りまくっただけなので、2015年はリベンジしたい。 配属とスクラム 4月に配属された。 チームの人数は少ないけど、業務の\"種類\"が多くて(未だに戸惑うこともあるが)配属当初は戸惑っていた。 誰が何をやっているかわからなかったし、何よりわからなかったのが、自分が今やっていることをどうやって共有すれば良いのか、がわからなかった。 良い機会だったので、チームでスクラムをやろうと思い、導入してみたが、最終的には中途半端になってしまった。 よかった点としては、朝会ができたことで、誰が何をやっているのか新鮮な情報がもらえ、割り込みが入った時でも状態がわかりやすくなったこと。 うまくいかなかった原因としては、スクラムマスターをやっていた自分が、業務にかまけてしまったことで、イテレーションをきちんとまわせなかったことが大きい。 他には、そもそも他のチームからの割り込みが多すぎて、バックログ消化できないとか、実質的なPMがいないことなどがあった。 若干スクラムっぽいのは、まわりのチームでも始まっているので、いろんな人と意見を交換しながら、徐々にチーム活動を改善していく予定。 とりあえずは、カンバンのような、割り込みが多くても管理しやすい手法を、もう少し実践する。 仕事全般 自覚はあるが仕事自体が基本的に遅いこともあり、自分の時間を作ることができていなかった。 プログラミングもインフラも初心者なので、もっと学習やOSSへ時間を確保したい。 gitの使い方とかスクラムはチームで広めるために、業務時間中に資料をまとめたので、 そんな感じで、業務と自分のやりたいことをうまくミックスして、業務時間でも時間を確保しようと思う。 仕事納め以降、自分の気持ちが上向きになって、エンジニアリングにも良い影響が出ている。 やはり去年までと比べても、今年触ったツールや技術は浅くなりがちだし、毎日を17時退社の気持ちで(18時退社になるように)仕事に臨もうと思う。 2015年に向けて 目標 fluentd + Consul(などのオーケストレーションツール) で設計したシステムを真面目に運用する Golangで書いたライブラリを公開する ConsulなどのOSSソフトウェアのソースを読んで、パッチを書く。マージしてもらえるように頑張る 分散システムの論文を5本以上読む","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/31/1805.html","loc":"https://www.hitsumabushi.org/blog/2014/12/31/1805.html"},{"title":"VCPの学習中のメモ","text":"項目分けするほどでもないが、知っておくとVCP受講時に役にたつことをメモしていきます。 特に、あまり使われていない機能については、調べてないとわからないことが多いと思うので、参考になればと思います。 VMware Data Recovery 資料 VMware Data Recovery 管理ガイド VMware Data Recovery Datasheet 概要 デプロイ OVF形式で提供されているので、OVF deployします。 動作 VMware vStorage API for Data Protectionを使って、バックアップを取得します。 そのためVSSが利用でき、Windows 2003以降であれば、ファイルシステムの整合性を保つレベルでのスナップショットが取得できます。 また、データの重複排除(デデュープ)を行うため、そこそこのデータ削減が見込めます。 上限 保護VMは、100VM/1アプライアンス 10アプライアンス/1vCenter デデュープ ストアは、最大1TB CIFSのとき、500GB VMDK, RDMのとき、1TB メモリ 資料 以下のブログ記事を見ておけば、万事解決です。 VMware ESXにおけるメモリ管理(10) - メモリ圧縮 (1) - Simple is Beautiful VMware ESXにおけるメモリ管理(12) - いつ、どのタイミングで、どの機能が使用されるのか - Simple is Beautiful 実際のメモリ情報の見方について、参考になります。 vCenterで確認できるメモリ情報の見方について | Japan Cloud Infrastructure Blog - VMware Blogs ESXiメモリ使用方法順 メモリの不足状況に応じて、以下の順番で各機能が利用されます。 後のものほど、VMのパフォーマンスへの影響が大きいです。 TPS バルーニング メモリ圧縮 SSDホストキャッシュ VMkernelスワッピング リソース管理/パフォーマンスに関係する機能 TPS 透過的メモリ共有の機能。セキュリティ上の理由により、デフォルトでオフになります。 ホストマシンにおいて、仮想マシン間のメモリ内容が同じ時には、共通の物理メモリアドレスを参照させ、変更時にはCoWで行います。 これにより、物理メモリを節約することができます。ただ、ラージページが一般的になってきた最近のOSでは、それほど効果はありません。 TPSの設定は、ホスト単位で設定できますが、全VMで共有する以外にも、VMごとにシードを与えることで、同じシードを持つVMだけでTPSを有効にすることが可能です。 バルーニング ESXiがVMに割り当てているメモリを回収するための技術です。 ESXiはVMにメモリを割り当てることはできますが、回収する方法があまりないため、ホストメモリが不足がちになってきたときにバルーニングを使います。 バルーニングのために使われるバルーンドライバ(vmmemctlドライバ)は、VMware Toolsに含まれており、ゲストOS内のプロセスとして常駐しています。 動作としては、ESXiから指示があった時、vmmemctlはゲストOS内でメモリを確保しようとします。 そうすると、ゲストOSはメモリをスワップアウトするなどして、vmmemctlへメモリを割り当てるので、割り当てられたメモリをESXiに渡すことができます。 メモリ圧縮 VMメモリ領域の一部を圧縮し、物理メモリの節約をする技術です。 圧縮前に圧縮率を計算し、50%以上であれば実施します。 圧縮したメモリ領域を使用する場合、解凍処理を行う必要があります。 SSDホストキャッシュ vswapファイルをSSD上に作成して、ESXiのメモリが不足した時にSSD上にスワップアウトする機能です。 後述するスワッピング処理を高速にするための技術です。 通常は、vswapファイルはVMのディレクトリと同じディレクトリに作られ、高速でないこともありますが、 この機能を使うことで、スワップアウトを高速にすることができます。 VMkernelスワッピング ESXiの物理メモリが不足した時に、各VMのメモリ(の一部または全て)を vswapファイルにスワップする機能です。 とにかくパフォーマンスが低下するため、パフォーマンストラブル時にもっとも気にする機能です。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/30/0548.html","loc":"https://www.hitsumabushi.org/blog/2014/12/30/0548.html"},{"title":"VMware環境でのNPIVについて","text":"VCP-DCV取得のために勉強していたところ、NPIVという用語が出てきたので、調べてみたことを書いておきます。 資料 vSphere ドキュメント センター サーバ仮想化のカギはストレージ・ネットワークにあり：基幹系サーバ統合に適したストレージ・ネットワーク要件を探れ！ - ITmedia エンタープライズ ブレードサーバーで始めるSAN - 第4回 ブレードサーバー特有のSAN設計：ITpro まとめ NPIV = N_Port ID Virtualization NPIVとは、1つのHBAに対して複数のID(WWPN)を割り当てることで、 1つのHBAポートを複数のHBAポートに見せる ための技術 VMware的には、RDMのディスクを持つ仮想マシンについて、 アクセス制御, QoSなどのために使われる NPIVのしようとして、1ポートあたり255WWPNしか割り当てられない NPIVとは FC環境の基本 FC環境では、各ノードはWWN( World Wide Name )を用いて識別されています。 IPプロトコルで言うところの、IPアドレスと同様に、アクセス先を表したり、アクセス制限を行ったりする対象が、WWNになります。 このWWNは、 - WWNN ( World Wide Node Name ) - WWPN ( World Wide Port Name ) という2種類で構成されています(が、便宜的なものなので、通常、区別する必要はないです)。 その名の通り、WWNNはノード自身(例えば、HBAごと)に割り当てておき、WWPNはポート(例えば、HBAポートごと)に割り当てておきます。 NPIV 前述の通り、WWNベースでアクセス制限をすることになるのですが、例えば、1つのHBAポートを複数のマシンで共有している状況で、 各マシンごとにアクセスできるLUN(LUNはWWNを持っています。)を変えたいと思うと、どうすれば良いのでしょうか。 NPIVは、1つのHBAポートに対して、最大255個のWWPNを割り当てる技術です。 これを使って、各マシンごとにWWPNを割り当てることで、LUNからはきちんと個別のアクセス制御が行えることになります。 NPIVを使うには、スイッチ側とHBA側の両方で対応している必要があります。 VMware環境で利用する際の制限 スイッチ側でNPIVを有効化 ホストのHBAがNPIVをサポートしている 利用できる仮想マシンは、RDMディスクを持つもののみ 1台の仮想マシンに対して、最大4つの仮想ポートを割り当てられる(つまり、最大4つのWWPNが割り当てられる) 通常の仮想マシンが利用するWWNは、ホストの物理HBAのWWNを利用します。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/30/0257.html","loc":"https://www.hitsumabushi.org/blog/2014/12/30/0257.html"},{"title":"VCP-DCVの概要 : VCP550","text":"資料 VCP-Data Center Virtualization 受験を計画されている方は、Exam Blueprints というPDFを一読されることをお勧めします。 VCP-DCVの概要 バージョン VCP-DCVという資格は、VMwareの製品バージョンに合わせて区別されています。 例えば、現在のvSphereのバージョンは、5.5が最新バージョンであるため、VCP5-DCV呼ばれています。 さらに、試験については注意が必要で、VCP5-DCVという1つの資格に対して、2つのマイナーバージョンがあります。 それが、VCP510, VCP550で、vSphereのマイナーバージョンレベルでの違いです。 (他に前バージョンからのアップデート用試験もあります。Deltaみたいな感じで書かれていると、アップデート用試験です。) 有効期限 VCPの有効期限は、取得日から2年間です。 延長の方法は、3つあります。 1. 同じ試験に合格する 2. 上位試験に合格する 3. 他の試験(例えば、VCP-Cloud)に合格する 必須条件 新しくVCP-DCVを取得しようとする場合、トレーニングを受講する必要があります。 5種類のうち、対応するバージョンのものを、どれか1つ受講すれば良いですが、初心者向けの1.を受講することが多いようです。 VMware vSphere: Install, Configure, Manage [V5.x] VMware vSphere: Fast Track [V5.x] VMware vSphere: Optimize and Scale [V5.x] VMware vSphere: Troubleshooting Workshop [V5.x] vSphere with Operations Management: Fast Track [V5.x] ただし、日本で開催されているトレーニングは3つしかないようですが...。 試験内容 試験時間 120分 (+非英語圏の人は、+30分? 聞いている話だと日本語で受験できるはずなので、受験してみて確認します) 問題数 135問 合格点 300/500点 試験勉強に役立つもの pre-test VMware Certification vCenter5.5 + ESXi環境 実機検証が何より役立つと思う。 あと5.5だとWeb Clientからしか使えない機能も増えているので、Web Clientを使っておいたほうが良いと予想している。 書籍 マスタリングVMware vSphere 5.5 個人的には、この本が一番良いと思っている。試験勉強中は、参照用に置いておくとはかどる。 実務でも役立つこと請け合いなので、試験が終わっても、とりあえず手元に持っておくと便利。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/30/0130.html","loc":"https://www.hitsumabushi.org/blog/2014/12/30/0130.html"},{"title":"fluentd-plugin-secure-forward のソースを読んでみる(Input プラグイン編)","text":"本当は全部読もうと思っていたけど、想像以上に疲れたので、Inputだけにしました。 ただ、整理されているコードなので、Ruby知らなくても読みやすいのは読みやすいと思います。 基本情報 lib/fluent/plugin/{TYPE}_{NAME}.rb 以下がプラグインの本体。 TYPE : in, out, buf,... etc NAME : プラグインの名前 pluginを書く時のお約束 Input(Output)プラグインは、module Fluentd内でInput(Output)プラグインを継承してクラスを定義する 設定ファイル中で type hoge と書きたいなら、 Fluent::Plugin.register_input('hoge', self) をクラス定義中に書く config_param を使うと、インスタンス変数が宣言できる。(実装はまだ読めていない) Fluentdの起動順序 : See docs.fluentd.org - plugin-development Fluent::Supervisor#run_configure require new configure(conf) Fluent::Supervisor#run_engine start shutdown fluentd-plugin-secure-forward を読み始める 準備 何はともあれ、クローンしてきます。 $ git clone https://github.com/tagomoris/fluent-plugin-secure-forward.git $ cd fluent-plugin-secure-forward さて、ライブラリの本体は、以下の通りです。 $ ls lib/fluent/plugin/ in_secure_forward.rb input_session.rb openssl_util.rb out_secure_forward.rb output_node.rb これを見ると、 in_secure_forward.rb out_secure_forward.rb があるので、このプラグインは、 input, outputについてプラグインを作っているようです。 in_secure_forward を読む require & new さっそく中を見てみます。 require 'fluent/mixin/config_placeholders' これは、 github.com/tagomoris/fluent-mixin-config-placeholders を読んでいます。 動作については、 http://d.hatena.ne.jp/tagomoris/20120820/1345455837 に書いてありますが、fluentdの設定ファイル中のプレースホルダ(${...}みたいなの)を展開した状態で、変数に格納してくれるものらしいです。 secure_forwardを使うときには、 self_hostname で自分のホスト名を宣言しますが、その時にhostnameコマンドの結果で定義したいのが人情というものなので、その時に使われていそうです。 とりあえず、設定ファイル中の値を参照できることとして(configureメソッドの super が呼ばれてから参照できます)、次に進みます。 module Fluent class SecureForwardInput < Input end end これはお約束みたいなやつで、Inputプラグインは、Fluentモジュールの、Inputを継承したクラスとして定義するようです。中身は後で定義されています。 先に進みます。 require_relative 'input_session' これは暗号化するためのセッションを扱うものなので、後で考えます。 module Fluent class SecureForwardInput < Input DEFAULT_SECURE_LISTEN_PORT = 24284 Fluent :: Plugin . register_input ( 'secure_forward' , self ) config_param :self_hostname , :string include Fluent :: Mixin :: ConfigPlaceholders ... ここで大事そうなのは、 Fluent::Plugin.register_input('secure_forward', self) です。 これを書いておくと、fluentd.conf(のinputセクション)で type secure_forward と書いたとき、このプラグインを使います。 # Define `log` method for v0.10.42 or earlier unless method_defined? ( :log ) define_method ( \"log\" ) { $log } end 昔はlogを出すときに$logと書いていたけども、今はlogと書く、という差を吸収するための設定みたいです。 configure def configure ( conf ) super ... @clients . each do | client | ... @nodes . push ({ address : source_addr , shared_key : ( client . shared_key || @shared_key ), users : ( client . users ? client . users . split ( ',' ) : nil ) }) end @generate_cert_common_name ||= @self_hostname self . certificate ... end 最初の superで、fluent/mixin/config_placeholders を使って、hostnameのプレースホルダーを展開した値を参照できるようになりました。次に、 @clients になにが入ってるかといえば、 config_section :client , param_name : :clients do config_param :host , :string , default : nil config_param :network , :string , default : nil config_param :shared_key , :string , default : nil config_param :users , :string , default : nil # comma separated username list end というのがあるので、設定ファイル中 <client>セクションで定義している中身が入っていることになります。 source_addrというのは、クライアントのアドレス、またはネットワークアドレスで、shared_keyとuserと一緒にpushしています。 self.certificate というのを見てみると、@certと@keyが宣言されていないとき、証明書とキーを生成して、@cert, @keyとして定義するものです。このときコモンネームは、 @generate_cert_common_name ||= @self_hostname から決まっているので、指定がないならホスト名になります。 start & shutdown さて、configureは読んだので、実行時の挙動を調べます。 def start super OpenSSL :: Random . seed ( File . read ( \"/dev/urandom\" , 16 )) @sessions = [] @sock = nil @listener = Thread . new ( & method ( :run )) end def shutdown @listener . kill @listener . join @sessions . each { | s | s . shutdown } @sock . close end 特に、変なところはないですが、 @listener, @session の中身が気になるところです。早速 runの中身を見てみましょう。 def run # sslsocket server thread log . trace \"setup for ssl sessions\" cert , key = self . certificate ctx = OpenSSL :: SSL :: SSLContext . new ctx . cert = cert ctx . key = key log . trace \"start to listen\" , :bind => @bind , :port => @port server = TCPServer . new ( @bind , @port ) log . trace \"starting SSL server\" , :bind => @bind , :port => @port @sock = OpenSSL :: SSL :: SSLServer . new ( server , ctx ) @sock . start_immediately = false begin log . trace \"accepting sessions\" loop do while socket = @sock . accept log . trace \"accept tcp connection (ssl session not established yet)\" @sessions . push Session . new ( self , socket ) # cleanup closed session instance @sessions . delete_if ( & :closed? ) log . trace \"session instances:\" , :all => @sessions . size , :closed => @sessions . select ( & :closed? ) . size end end rescue OpenSSL :: SSL :: SSLError => e raise unless e . message . start_with? ( 'SSL_accept SYSCALL' ) # signal trap on accept end end あまりOpenSSL:SSL::SSLserverについて調べていないですが、パッと見、 設定されている証明書, キーでSSLサーバーを立てて、loopで待ち続ける テキトーなポート,バインドでリッスンする TCPコネクションが確立されるごとに @sessions に格納していく ( @sock.start_immediately = false としているので、 SSLはまだハンドシェイクできてない ) セッションが切れるたびに、@sessionsから削除する という動作みたいです。特に3.について、なぜこの実装なのかはわかっていないです。 あと、ここで @sessions.push Session.new(self, socket) という部分がありますが、ここのSessionは、input_session.rb で定義されているものです。 こっちの方がこのプラグインの肝な感じですが、Fluentdのプラグインの作り方とはあまり関係ないので、見ないことにします。(最初にping pongをやって、それが終わってから、ソケットを読みにいってはon_messageを呼ぶ、みたいなことをしているみたいです) とりあえず、以上でざっくりと Inputプラグインの中身がわかりました。 まとめ Inputプラグインの場合は、 configure(config) start stop に集中して読み始めるとわかりやすいと思いました。 その他は割と雑多なので、困った時に読めば良さそうです。 追記(2014-12-18) @_hitsumabushi_ acceptするのはFluentd in_secure_forwardのメインスレッドだけど SSL handshake は割とコストが高い処理なので、それを input_session 側のスレッドにやらせたい、という理由ですね — tagomoris (@tagomoris) December 17, 2014","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/17/2345.html","loc":"https://www.hitsumabushi.org/blog/2014/12/17/2345.html"},{"title":"VMware 環境でのMACアドレス割当て","text":"1. まとめ vSphere環境上のMACアドレス割当の方式は、複数あります。自分のOUIを割当てたい!!、という場合には、 vCenter環境でプレフィックス指定 固定割当て といった方法を使いましょう 2. MACアドレスの割当て方式 vCenterによる自動割当て VMware OUI プレフィックスベース 範囲ベース vCenterに接続されていないESXiによる自動割当て 手動での割当て vCenterによる自動割当て 設定の変更は、vSphere Web Clientから行うか、vpxd.cfgから行います。 VMware OUI これがデフォルトの設定ですが、00:50:56:{XX}:{YY}:{ZZ} という割当てを行います。 ただし、XX = {vCenterのid} + 0x80 で決めています。 そういうわけで、XXはvCenterごとに固定なので、65536個だけ利用可能だということになります。 普通は、こんなにMACアドレスを使う前に、他の上限値(VM作成上限数やvCPU上限など)にぶち当たって悲しみを抱えるので、大丈夫だと思われます。 複数のvCenterを利用している人は、vCenter間でIDがかぶっていないことは確認する必要があります。 この割当て方式では、00:50:56:80:{YY}:{ZZ} - 00:50:56:BF:{YY}:{ZZ} までの範囲が割当てられるので、見ればわかります。 以下が vpxd.cfgでの例です。 <vpxd> <macAllocScheme> <VMwareOUI> true </VMwareOUI> </macAllocScheme> </vpxd> プレフィックスベース 5.1以降の環境では、デフォルトの00:50:56以外のプレフィックスを指定できます。 また、個数が足りない時には LAA(ローカル管理アドレス)プレフィックスを使うこともできます。 自分のOUIを使いたいときとか、複数vCenterがある時にLAA使う場合に使いそうです。 以下の設定は、00:50:26または、00:50:27から始まるMACアドレスを割当てる例です。 <vpxd> <macAllocScheme> <prefixScheme> <prefix> 005026 </prefix> <prefixLength> 23 </prefixLength> </prefixScheme> </macAllocScheme> </vpxd> 範囲ベース 開始と終了のMACアドレスを指定するものです。 LAAの複数の範囲を指定できるので、便利かもしれません。 これも複数vCenterを使うときには、分割して使えるので便利だと思います。 たぶん、こっちの方が、後で拡張したいときには便利なので、(そんなことあるのか知らないけど、)自由に使えるMACアドレス数が少ない場合には、後で追加しやすくて便利だと思います。 以下が、範囲ベース割当ての例です。 range idは0から始まります。 以下の例では、00:50:67:00:00:01のみの範囲1つだけを利用する例です。 <vpxd> <macAllocScheme> <rangeScheme> <range id= \"0\" > <begin> 005067000001 </begin> <end> 005067000001 </end> </range> </rangeScheme> </macAllocScheme> </vpxd> vCenterに接続されていないESXiにより自動割当て これが利用されるのは、 1. vCenterに接続されていない 2. vmxファイルに、MACアドレスやMACアドレス割当てタイプが書かれていない 場合です。 割当てられるMACアドレスは 00:0C:29 + {UUIDの最後の3オクテット} です。 固定割当て デフォルトのOUIは 00:50:56 です。 このOUIを利用する場合、上で見たように他の用途で使われる箇所は予約されています。そのため、 00:50:56:00:{YY}:{ZZ} - 00:50:56:3F:{YY}:{ZZ} のみが利用可能です。 これを利用するためには、以下を削除して、 ethernetN . generatedAddress ethernetN . addressType ethernetN . generatedAddressOffset 以下を記述します。 ethernetN . addressType = static ethernetN . address = { MAC ADDRESS } 3. MACアドレスが足りなくなった時の動作 起動されるときに、MACアドレスが生成されます。 基本的には、起動中のMACアドレスとの衝突しか検知しないので、仮想マシンを一時的に停止している場合、再起動したときに、たまにMACアドレスが変更されてしまうことがありえます。 4. 資料 vSphere ネットワークガイド - MACアドレスの管理","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/17/1440.html","loc":"https://www.hitsumabushi.org/blog/2014/12/17/1440.html"},{"title":"ソースコードリーディングをするときにctagsを使いたい","text":"きっかけ とある事情によって、Fluentdプラグインを自作する or 世間の良い実装のプラグインを見つける必要がありました。 でも、Fluentdを真面目に使ったことがないので、いまいち眺めていても難しいなー、と思っていました。 そもそも、Rubyもまともに書いていないため、ソースを見ても、Rubyのものなのか、Fluentdで定義されているのかが、 ぱっと見でわからず、いちいち時間がかかっていた感じです。 ソースコードを読む上で支援してくれるものがあったらなー、と思っていたら、タグをつけると良いと聞いたので、やってみます。 流れ 今回はmacでやることにします。 brew がインストールしてあることにしています。 Exuberant Ctagsのインストール brew install ctags tagの生成(Emacsから使うには、etagsと呼ばれる形式で生成) # 読みたいソースのトップに移動 cd /home/me/app # タグの生成 (Emacs の場合) ctags -Re # タグの生成 (Vim の場合) ctags -R コードリーディング Emacs の場合 キー 意味 M-. ジャンプ(関数, クラス) C-u M-. 次の検索 C-u – M-. 前の検索 M-* 元の場所 Vim の場合 いつも以下のサイトを参考にして生きている。 http://archiva.jp/web/tool/vim_ctags.html","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/16/2000.html","loc":"https://www.hitsumabushi.org/blog/2014/12/16/2000.html"},{"title":"drone.ioを使って、pelicanをビルドする","text":"動機 最近はWordpressでブログをやっていたけど、 あくまで Wordpressを使う人の気持ちがわかりたかったので、使ってたのでした。 そろそろ、Wordpressの便利さもわかってきたし、vimとかemacsから書きやすいものを使いたいなー、と思ってました。 そこで pelican ですよ。 OctpressとかSphinxとかで書いても良いんですが、なんとなく微妙な修正だったり、追加のプラグインが必要だったので、気分がのらなかったのです。 pelicanは割と軽量っぽく見えたのと、reStructuredTextでもMarkdownでも書けるのが、自分には気楽だったから採用しました。 やり方 drone.io で New Project -> Github -> 適当なリポジトリを選ぶ drone.io で Settings -> Repository -> View Key をクリックすると、SSH keyが表示される。 Githubの自分のリポジトリで、Settings -> Deploy keys に追加する 後は、自分のビルドの設定をして、ビルドするなり、git pushするなりするだけ。 困った...?と思ったこと SSHキーがどこから見るのかわからなくて、git pushできないかと思って焦りました。 いい感じにgit pushするスクリプトがなかったので、変なデプロイ用のシェルを書くことになった。しかも、毎回 gh-pages にforce pushするという感じになってしまった。 感想 まだテーマとかいじってないので、テーマをいじるかも。 テーマいじらないとタグクラウドの設定ができないっぽいので。 使ってみた感じ、割とブログには便利だと思うなー。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/12/15/2300.html","loc":"https://www.hitsumabushi.org/blog/2014/12/15/2300.html"},{"title":"Consul 使ってみる","text":"Consul とは 特徴 Service Discovery Consulのクライアントは、\"api\"や\"mysql\"といった与えられた名前を持つサービスを提供 他のクライアントは、Consulを使ってサービスを検出 アプリケーションはConsulが検出したサービスを、DNSやHTTP経由で検出 Health Check 多くのヘルスチェックの提供 実行されているサービスやノード上の情報などと連携 この情報を元に、クラスターの状態を監視 サービス検出コンポーネントを使って、良くない状態のホストを迂回できる Key/Value Store アプリケーションはConsulの階層的なKey/Valueストアを利用できる 動的な設定変更や、フラグを立てたり、色々使える HTTP APIで簡単に使える Multi Datacenter リージョンのことは気にしなくても、Consulが上手いことやる (複数のgpssipプールを持っている云々書いているので、LANとWANで変えているのだと思う) ステータスの確認のためには、謎のキレイなWebUIもある。 個人的に良いと思っているところ DNSインターフェースでホストのIPを取得できるので、クラスター用のDNSサーバーのメンテナンスがいらないのが嬉しい ホストの情報が散らばらずに、各サーバーで書かれている。それでいて、問い合わせは分散させられるのが嬉しい。 一貫性は正義 登場するサーバー Agent Server or Client HTTP, DNS, RPCのインターフェースを持ちうるやつ。 (実際に、持つかどうかは、起動時のclientオプションで決まる) Server クラスタ状況の管理 クライアントからのRPCへの応答 WAN側gossip, リモートデータセンターのリーダーへクエリ クラスタ内ではそれほど多くなくて良いはず Client サーバーにRPCを送ってアピール (gossipプールへの参加に関することを除いて、)割とステートレス 通信料もそんなに多くない いくらでもいて良い インストール バイナリを持ってきて、パス通ってるとこに置く。 http://www.consul.io/downloads.html 起動方法 Agentの起動 # server : serverオプションを指定 # - clientオプションは、RPC, DNS, HTTPのためのもの # - bindオプションは、cluster情報のやりとりに使うっぽい # - dcはデータセンター名らしい # - nodeはホスト名 # - bootstrapは最初の1つだけにつける。他は大体joinしとけば良い。 consul agent -server -bootstrap -client = 192 .168.2.5 -dc = local \\ -node = con -data-dir = /tmp/consul -bind = 192 .168.2.5 # client # - joinオプションで、agentのアドレスを指定。何個か書けるらしい consul agent -dc = local -node = consul2 -data-dir = /tmp/consul2 \\ -bind = 192 .168.39.6 -join = 192 .168.39.5 Agentの再起動 Agentの再起動時には、 SIGHUP を送れば良い。 使い方 clusterからの情報を取得 Consulメンバー # 自分の胸に聞くときは、\"-rpc-addr\"は不要 consul members # クラスタの外にいたりして、誰かに教えてもらうとき consul members -rpc-addr = 192 .168.2.5:8400 HTTPインターフェース # JSON形式で情報が返る curl 192 .168.2.5:8500/v1/catalog/nodes DNS # 聞く先は適当に。 anyリクエスト投げとけば良いと思う # <hostname>.node.consul (データセンターはlocalになる)という形式か、 # <hostname>.node.<datacenter>.consul という形式で問い合わせ。 dig @192.168.2.5 -p 8600 con.node.local.consul Key/Valueストア put curl -X PUT -d 'first object' 192 .168.2.5:8500/v1/kv/namespaces/keyname # Check-And-Set : atomicなkey変更をするときに使うパラメータ # ModifyIndex の値を指定して更新できる。 # まず、GETしてModifyIndexを見てPUTする、という操作をatomicにできる curl -X PUT -d 'first object' 192 .168.2.5:8500/v1/kv/namespaces/keyname?cas = 97 get curl -s 192 .168.2.5:8500/v1/kv/namespaces/keyname # => jsonが改行もされず表示されて辛い気持ちになる curl -s 192 .168.2.5:8500/v1/kv/namespaces/keyname | python -mjson.tool # => 見やすくしてくれる curl -s 192 .168.2.5:8500/v1/kv/namespaces/?recurse # => 再帰的に見てくれる # index=101 : ModifyIndexが101より大きいものが帰ってくるまで聞き続ける # wait=5s などすれば、5秒までしか聞かない もっとConsul Service, Health Check を定義する 定義はjson形式で書く。 サービスを定義したいサーバーの例えば、/etc/consul.d/以下に # /e t c/co nsul .d/mysql.jso n : サービスを定義 { \"service\" : { \"name\" : \"mysql\" , \"tags\" : [ \"mysql\" , \"db\" ], \"port\" : 3306 }} # /e t c/co nsul .d/pi n g.jso n : pi n gステータスをチェック { \"check\" : { \"name\" : \"ping\" , \"script\" : \"ping -c1 google.com >/dev/null\" , \"interval\" : \"30s\" } } # /e t c/co nsul .d/web.jso n : サービスを定義しつつ、サービスレベルでヘルスチェック { \"service\" : { \"name\" : \"web\" , \"tags\" : [ \"rails\" ], \"port\" : 80 , \"check\" : { \"script\" : \"curl localhost:80 >/dev/null 2>&1\" , \"interval\" : \"30s\" }}} とか書いて、起動オプションに、 -config-dir=/etc/consul.d を加える。 Service, Halthの確認 他のサーバーから定義されたサービスやステータスを知りたい。 DNS NAME.service.consul or TAG.NAME.service.consul を使う。 さらに、SRVレコードを見れば、ポートもわかる # Serviceを知る dig @127.0.0.1 -p 8600 mysql.service.consul SRV dig @127.0.0.1 -p 8600 db.mysql.service.consul SRV # Health Checkに失敗してるやつ dig @127.0.0.1 -p 8600 web.service.consul HTTP # Serviceを知る curl -s http://192.168.2.5:8500/v1/catalog/service/mysql # Halth Checkに失敗してるやつ curl http://localhost:8500/v1/health/state/critical 参考資料 Introduction to Consul Consul vs. Serf ( Consul vs Serfの日本語訳 ) Serf という Orchestration ツール #immutableinfra Advanced http://www.consul.io/docs/internals/index.html","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/11/01/0243.html","loc":"https://www.hitsumabushi.org/blog/2014/11/01/0243.html"},{"title":"vSphere API事始め","text":"リファレンス https://www.vmware.com/support/developer/vc-sdk/ https:// \"vcenter_ip\" /mob/ 参考サイト http://thinkit.co.jp/story/2010/06/23/1617 Overview Types 4つのタイプがある。 vSphere APIの型 普通のプログラミングとの類推 managed object type プリミティブ型、複合型 data object type 抽象データ型 enumerated type 定数 Fault type 例外型 1. Managed object types Managed object typeとは (サーバーサイドのオブジェクトモデルの基本的なデータを担っている。data objectも同じく。) managed objectには、大きく2つの種類がある。 ManagedEntity を拡張したもので、仮想コンポーネントのインベントリとして使われている。 \"managed entities\"と呼ばれることもある。 例えば以下のようなものがある。 | HostSystem | ホストシステムのインスタンス | | VirtualMachine | VM | | Datastore | データストア | システム全体のための、サービスを提供するもの。 例えば、以下のもの。 | PerfomanceManager | パフォーマンス管理 | | LicenseManager | VMware製品のライセンス管理 | | VirtualDiskManager | 仮想ストレージの管理 | Managed object typeの使い方 使いはじめるには、 connect : サーバーに接続 authenticate :認証を通す session : セッションを得る というステップがある。 セッションを貼った後は、 ServiceInstance という managed objectを得る。 ServiceInstance の下には、 Data object typesがぶら下がっている。 以下の図で、\"MOR\"というのは、 ManagedObjectRefarence の略で、 これは、server-side object へのリファレンスを与えるdata objectである。 ※図は vmwareのドキュメント から。 凡例 managed obejectは、property と operation(method)を持っている。 2. Data object types Data object typeとは Javaでいう抽象データ型や、C++でいうstructデータ型のようなもの。 managed object typeが、プリミティブデータ型だったり、複合データ型だったりする。 (vSphere API的な意味での、プリミティブデータ型は、 XML Schemeプリミティブで書かれている。例えば、xsd:string とか。) ※図は vmwareのドキュメント から。 Data object typeの例 data object 説明 AlarmInfo AlarmSpecから継承するプロパティ + ManagedObjectReference(Alarm managed objectへのリファレンス) ManagedObjectReference MOR, reference, MoRefとか呼ばれる。server-side managed objectのリファレンス 3. Enumerated types (enumeration) 予め定義されている値や予め定義されたオブジェクトが保管されているオブジェクト。 つまり、定数のように使われる。 | VirtualMachinePowerState | VMの起動状態を持っている。 poweredOff,poweredOn,suspended | 4. Fault types サーバーによってあげられる例外。 Fault 説明 NotSuported 呼ばれたメソッドがサーバーでサポートされていなかったとき。vCetnerのメソッドをESXiで呼んだりとかの場合も。 NoPermission clientユーザーの権限が足りないとき。権限が必要だった、MORを含んで返してくれる","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/08/14/1639.html","loc":"https://www.hitsumabushi.org/blog/2014/08/14/1639.html"},{"title":"IPv6のことを調べ始めた","text":"IPv6 環境構築 ひとまず参照すべきRFC RFC3315(DHCPv6) とそれをupdateしているやつ RFC4861(Neighbor DiscoveryのIPv6バージョン) RFC5942(IPv6のsubnet model) IPv6 アドレス配布方式について 全体としては、下記の4パターンある。 アドレス配布方式 RAは必要? RAにプレフィックス情報は必要? m-flag o-flag 配布できるもの 手動 x x - - - ステートレスアドレス自動設定(SLAAC) o o off off プレフィックスのみ ステートフルDHCPv6 o o(たぶん。) on on IPアドレスとDUIDの組, DNSなど ステートレスDHCPv6 o x off on DNSなど よく使われそうな、SLAACとステートフルDHCPv6には、以下のようなメリット・デメリットがある。 方式 メリット デメリット SLAAC ルーター設定のみでOK IPを固定化しづらい ステートフルDHCPv6 DHCPDを立てる必要あり IP固定化できる。DNS情報もその気になれば配布できる","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/08/12/1638.html","loc":"https://www.hitsumabushi.org/blog/2014/08/12/1638.html"},{"title":"DebianにOzをインストールする","text":"ガウディ本読みはじめた。 困ったこと この本で使われるOzという言語をDebian上で実行するのに、少し困った。 morzart2をダウンロードして、適当にインストールしただけだと、 emacs上でコンパイルはできるのに、実行結果が表示できない状況になった。 正常な動作は、コンパイルできるとウィンドウが開いて、そこに結果が表示される。 このために、必要なパッケージがあるので、それを予めインストールしておくと良い。 インストール手順 必要なパッケージをインストール sudo apt-get install tcl tk8.5 最初は、tk8.5ではなく、tkを指定して、tk8.6だけがインストールされていた。 この状況では、結果が表示されない。 morzart2のダウンロード 以下のページからダウンロード。(Githubからでも良い) Source Forge morzart2の展開 tar xf \"ダウンロードしたファイル\" 実行 cd \"展開したディレクトリ\" bin/oz こんな感じ。 必要ならパスを通しておくと良い。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/07/27/1637.html","loc":"https://www.hitsumabushi.org/blog/2014/07/27/1637.html"},{"title":"[論文]Janus: Optimal Flash Provisioning for Cloud Storage Workloads","text":"読んだもの http://research.google.com/pubs/pub41179.html 主な内容 『2種類の異なるストレージ(フラッシュとディスク)を複数のワークロードで共用する』効率的な使い方を最適化問題への帰着 使い方に2つの戦略があって、『FIFO or LRU』。 LRUの方がキャッシュヒットレート的には良いけど、メトリクスを取るのが複雑になりがちなので、FIFOで良いのでは。 実システムでの計測結果 使われているシステム, 技術要素 Clossus (GFSの改良版) アルゴリズム: 貪欲法 大事なこと 最適化問題を特にあたって、ワークロード特性を表す関数を、convexで近似したことにすることがアイディアの1つ。 理論としては、piecewise-linearでなくても良いが、実測では有限個の点しか取れないので、piecewise-linearとしておくと計算が楽になる。 運用中もワークロード特性を測定してフィードバックすることで、特性の変化に対応することができる。 所感 よくまとまっているので、とても良い。 実際にこの論文と同じことを行う際には、現状のストレージをどういうふうに使っているかが問題になる。 GFSのようなマスターを持つ分散ストレージを使っているなら、この論文の内容は実践しやすい。 (もちろん、ワークロードのメトリクスを取るようにしたり、最適化問題を解くロジックを入れるなど、変更は必要。) 個人的にはマスターに色々聞かなくて済むような分散ストレージ(Cephとか)の方がアーキテクチャ的に好きなのだけど、 この論文を読むと、マスターがあったほうが均質じゃないシステムの制御は、扱いやすいように見える。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/07/17/1635.html","loc":"https://www.hitsumabushi.org/blog/2014/07/17/1635.html"},{"title":"DebianでZNCを使って、快適なIRC生活を送る","text":"やりたいこと 最近、自分の興味のあるソフトウェアのIRCによく引きこもっています。 自宅のPCはつけっぱなしにしておけば、全部のログを見れるのですが、 会社やスマホで見るときには、接続している間のことしか確認できず、若干もどかしいです。 そういうわけで、ZNCサーバーを立てて、IRCのログを出先からでも確認できるようにしましょう。 実現すること スマホからIRCのログを見る 作業環境 # lsb_release -a No LSB modules are available. Distributor ID: Debian Description: Debian GNU/Linux 7 .5 ( wheezy ) Release: 7 .5 Codename: wheezy インストール 色んなブログでは、build-depして自前でビルドするように書かれているが、今はパッケージがあるので、それを使おう。 $ sudo apt-get install znc 設定 ユーザーの作成 zncを動かすユーザーを作成しておく。 $ useradd -d /home/znc -s /bin/bash -m znc 設定ファイルの作成 適当に答えるだけです。 設定値もモジュールも後からどうにでもなると思うので、気にしない。 webadminを入れて、webから設定をできるようにしておきます。 $ znc --makeconf [ ** ] Building new config [ ** ] . [ ** ] First let 's start with some global settings... [ ** ]. [ ?? ] What port would you like ZNC to listen on? (1025 to 65535): 6667 [ ?? ] Would you like ZNC to listen using SSL? (yes/no) [no]:. [ ?? ] Would you like ZNC to listen using ipv6? (yes/no) [yes]: no [ ?? ] Listen Host (Blank for all ips):. [ ok ] Verifying the listener.... [ ** ]. [ ** ] -- Global Modules -- [ ** ]. [ ** ] +-----------+----------------------------------------------------------+ [ ** ] | Name | Description | [ ** ] +-----------+----------------------------------------------------------+ [ ** ] | partyline | Internal channels and queries for users connected to znc | [ ** ] | webadmin | Web based administration module | [ ** ] +-----------+----------------------------------------------------------+ [ ** ] And 13 other (uncommon) modules. You can enable those later. [ ** ]. [ ?? ] Load global module <partyline>? (yes/no) [no]:. [ ?? ] Load global module <webadmin>? (yes/no) [no]: yes [ ** ]. [ ** ] Now we need to set up a user... [ ** ] ZNC needs one user per IRC network. [ ** ]. [ ?? ] Username (AlphaNumeric): hitsumabushi [ ?? ] Enter Password:. [ ?? ] Confirm Password:. [ ?? ] Would you like this user to be an admin? (yes/no) [yes]:. [ ?? ] Nick [hitsumabushi]:. [ ?? ] Alt Nick [hitsumabushi_]:. [ ?? ] Ident [hitsumabushi]:. [ ?? ] Real Name [Got ZNC?]: <name> [ ?? ] Bind Host (optional):. [ ?? ] Number of lines to buffer per channel [50]:. [ ?? ] Would you like to keep buffers after replay? (yes/no) [no]:. [ ?? ] Default channel modes [+stn]:. [ ** ]. [ ** ] -- User Modules -- [ ** ]. [ ** ] +-------------+------------------------------------------------------------------------------------------------------------+ [ ** ] | Name | Description | [ ** ] +-------------+------------------------------------------------------------------------------------------------------------+ [ ** ] | admin | Dynamic configuration of users/settings through IRC. Allows editing only yourself if you' re not ZNC admin. | [ ** ] | chansaver | Keep config up-to-date when user joins/parts | [ ** ] | keepnick | Keep trying for your primary nick | [ ** ] | kickrejoin | Autorejoin on kick | [ ** ] | nickserv | Auths you with NickServ | [ ** ] | perform | Keeps a list of commands to be executed when ZNC connects to IRC. | [ ** ] | simple_away | Auto away when last client disconnects | [ ** ] +-------------+------------------------------------------------------------------------------------------------------------+ [ ** ] And 36 other ( uncommon ) modules. You can enable those later. [ ** ] . [ ?? ] Load module <admin>? ( yes/no ) [ no ] : yes [ ?? ] Load module <chansaver>? ( yes/no ) [ no ] :. [ ?? ] Load module <keepnick>? ( yes/no ) [ no ] :. [ ?? ] Load module <kickrejoin>? ( yes/no ) [ no ] :. [ ?? ] Load module <nickserv>? ( yes/no ) [ no ] : yes [ ?? ] Load module <perform>? ( yes/no ) [ no ] :. [ ?? ] Load module <simple_away>? ( yes/no ) [ no ] : yes [ ** ] . [ ** ] -- IRC Servers -- [ ** ] Only add servers from the same IRC network. [ ** ] If a server from the list can ' t be reached, another server will be used. [ ** ] . [ ?? ] IRC server ( host only ) : irc.freenode.net [ ?? ] [ irc.freenode.net ] Port ( 1 to 65535 ) [ 6667 ] :. [ ?? ] [ irc.freenode.net ] Password ( probably empty ) :.... [ ?? ] Does this server use SSL? ( yes/no ) [ no ] :. [ ** ] . [ ?? ] Would you like to add another server for this IRC network? ( yes/no ) [ no ] :. [ ** ] . [ ** ] -- Channels -- [ ** ] . [ ?? ] Would you like to add a channel for ZNC to automatically join? ( yes/no ) [ yes ] :. [ ?? ] Channel name: #ceph [ ?? ] Would you like to add another channel? ( yes/no ) [ no ] :. [ ** ] . [ ?? ] Would you like to set up another user ( e.g. for connecting to another network ) ? ( yes/no ) [ no ] :. [ ok ] Writing config [ /home/znc/.znc/configs/znc.conf ] .... [ ** ] . [ ** ] To connect to this ZNC you need to connect to it as your IRC server [ ** ] using the port that you supplied. You have to supply your login info [ ** ] as the IRC server password like this: user:pass. [ ** ] . [ ** ] Try something like this in your IRC client... [ ** ] /server <znc_server_ip> 6667 hitsumabushi:<pass> [ ** ] And this in your browser... [ ** ] http://<znc_server_ip>:6667/ [ ** ] . [ ?? ] Launch ZNC now? ( yes/no ) [ yes ] :... [ ok ] Opening Config [ /home/znc/.znc/configs/znc.conf ] .... [ ok ] Loading Global Module [ webadmin ] ... [ /usr/lib/znc/webadmin.so ] [ ok ] Binding to port [ 6667 ] .... [ ** ] Loading user [ hitsumabushi ] [ ok ] Adding Server [ irc.freenode.net 6667 ] .... [ ok ] Adding Server [ irc.oftc.net 6667 ] .... [ ok ] Loading Module [ admin ] ... [ /usr/lib/znc/admin.so ] [ ok ] Loading Module [ nickserv ] ... [ /usr/lib/znc/nickserv.so ] [ ok ] Loading Module [ simple_away ] ... [ /usr/lib/znc/simple_away.so ] [ ok ] Forking into the background... [ pid: 27494 ] [ ** ] ZNC 0 .206+deb2 - http://znc.in Webからzncに接続する ブラウザから http(s)://hostname:6667 にアクセスする ログインして、List Users→Editを開いて、log モジュールにチェックを入れておくと、IRCのログを書いてくれる。 クライアントから接続する xchatでやってみた。 SSLで接続する場合には、ネットワーク一覧の設定から、SSLで接続するように設定をし、SSL不正な証明書を受け入れるようにしておく必要がある。 例として、zncサーバーが blog.hitsumabushi.org:6667, ユーザー名が oftc, パスワードが 123456 というユーザーで接続したことにすると、以下のようになる。 おわり これで快適なIRC生活がおくれるぞ！","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/06/09/1634.html","loc":"https://www.hitsumabushi.org/blog/2014/06/09/1634.html"},{"title":"WebHDFSで詰まったこと","text":"困っていたこと HDFSでwebhdfsを使うとき、基本的にnamenodeにリクエストを投げればよいが、実データにアクセスするにはdatanodeにアクセスする。 ただ、使うときにはあんまり気にしなくて良くて、リクエストを投げればリダイレクト先を指定してくれる、らしい。 しかし、実際にやっているとうまくいかないケースがあった。 % LANG = C ; curl -i \"http://namenode:50070/webhdfs/v1/tmp/client.retry?op=open\" HTTP/1.1 307 TEMPORARY_REDIRECT Cache-Control: no-cache Expires: Mon, 02 Jun 2014 20 :27:09 GMT Date: Mon, 02 Jun 2014 20 :27:09 GMT Pragma: no-cache Expires: Mon, 02 Jun 2014 20 :27:09 GMT Date: Mon, 02 Jun 2014 20 :27:09 GMT Pragma: no-cache Content-Type: application/octet-stream Location: http://localhost:50075/webhdfs/v1/tmp/client.retry?op = OPEN & namenoderpcaddress = namenode:8020 & offset = 0 Content-Length: 0 Server: Jetty ( 6 .1.26 ) 解決 これは簡単で、ホスト名の設定がおかしい。 hostname # => hostname.example.com となるだけではダメで、 hostname -f # => (ダメな例)localhost # => (正しい例)hostname.example.com となるようにしないといけない。 CentOS初めて使ったけど、ドメイン名がどこで決まるかとか、難しいな。","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/06/08/1633.html","loc":"https://www.hitsumabushi.org/blog/2014/06/08/1633.html"},{"title":"LEDバックライトの調整","text":"ディスプレイの輝度が高すぎて目が痛いので、輝度を下げたい。 GUIでやっても良いけど、CLIの方が簡単そうだったので、CLIでやってみることにした。 環境 PC: ASUS 24A OS: Linux(Debian sid amd64) 調整可能な範囲 cat /sys/class/backlight/intel_backlight/max_brightness ディスプレイの明るさ変更 # 明るさを800にするとき echo 800 | sudo tee /sys/class/backlight/intel_backlight/brightness","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/06/02/1631.html","loc":"https://www.hitsumabushi.org/blog/2014/06/02/1631.html"},{"title":"Perl の正規表現","text":"マッチ演算子 評価の結果は真偽値としては、マッチすれば真、そうでなければ偽。 and, or, ! が役立つのは間違いない。 $_ にマッチさせる /regex/ m:regex: string にマッチさせる string =~ /regex/ string =~ m:regex: $_ の文字列置換 s/regex/new/ s:regex:new: string の文字列置換 string =~ s/regex/new/ string =~ s:regex:new: 特殊なメタキャラクタ &#94;, $ 行頭、末尾にマッチ . ニューライン以外の任意の文字にマッチ [ chars ] chars に含まれる任意の1文字にマッチ chars 内のバックスラッシュ付き英数字以外のメタキャラクタは、リテラル扱い [&#94; chars ] chars に含まれない任意の1文字にマッチ メタキャラクタの扱いは [ chars ] と同様 [ char1 - char2 ] char1 の char2 の間に入る任意の文字にマッチ char1 , char2 を含む。 $& 最後にマッチした内容を保持 'Foo' =~ /&#94;[A-Z]/ なら $& は F \\ メタキャラクタに使ったり、エスケープに使う \\b 単語の区切りにマッチする 'abc def' にマッチさせたいときに、'Zabc def' にマッチしないよう、'babc def' と書いておく。 アルファベットは、単語構成文字クラスなので、スペースや!は、b にマッチするが、Zはマッチしない。 \\n ニューライン \\r キャリッジリターン文字 カーソルを行頭に戻す \\t タブ \\f フォームフィード 改ページ \\e エスケープ文字 \\NNN 8進数が NNN となる文字 例えば、040 はスペース 備考 8進数や16進数を調べるには、man ascii \\xNN 16進数が NN となる文字 例えば、x20 はスペース \\cX X によって表される制御文字 例えば、 cC はCtrl-C \\Q, \\E メタキャラクタを文字として見せたい部分を、\\Q ... \\E で囲む POSIX の文字クラスとショートカットメタキャラクタ 文字クラス ショートカットメタキャラクタ 説明 [a-zA-Z0-9_] \\w 単語構成文字 [&#94;a-zA-Z0-9_] \\W 非単語構成文字 [\\040\\t\\r\\n\\cJ\\cL] \\s 空白文字 [&#94;\\040\\t\\r\\n\\cJ\\cL] \\S 非空白文字 [0-9] \\d 数字 [&#94;0-9] \\D 数字以外の文字 マッチ修飾子 / RE /i や m%RE%xs, s/ RE / new /e など、最後に修飾子をいくつかつけて、マッチングの動作を変更させることができる。 マッチ修飾子 修飾子 説明 i 大文字小文字の違いを無視 x 拡張モード REフィールドに空白文字とコメントを許す s シングルラインモード . をニューラインにもマッチさせる m マルチラインモード &#94;, $ を文字列の絶対的な先頭、末尾でなく、ターゲット文字列中の行の先頭、末尾にマッチさせる g すべてのマッチを、スカラーコンテキストかリストコンテキストかによって、連続的もしくは集合的に返す e 置換の時に使う。 new を Perl のコードとして評価して、 RE にマッチしたものをその結果で置き換える。 その他の構文 選択、グループ化、キャプチャ、後方参照 構文 説明 X|Y|Z X, Y, Zのいずれかにまっち (X) グループ化とキャプチャ a(X|Y)bc や (XY)+ といった感じで使う \\1, \\2, ... 後方参照。検索文字列フィールドで使用 $1, $2, ... 後方参照。検索文字列フィールド以外で使用 量指定子 量指定子 構文 説明 X* 0回以上の繰り返し X+ 1回以上の繰り返し X? 0 または 1回の出現 X{min, max} min回以上max回以下の繰り返し X{min, } min回以上の繰り返し X{count} count回の繰り返し X{, max} max回以下の繰り返し REP? 量指定子の直後に ? をつけると、最短マッチ 例えば、.*? という風な。 以下は、Perl で正規表現を扱うときのコツ grep grep -v perl で grep -v のように、マッチしないものを表示させるのは、次のように行う。 # 空行以外を表示 perl - wnl - e '/&#94;$/ or print;' file このように or を使うと便利。 grep -l # foo にマッチするもののファイル名を表示 # close することで、マッチした後は探索しない。 perl - wnl - e '/\\bfoo\\b/ and print $ARGV and close ARGV;' file カスケードフィルタ シェルで grep \"regex1\" file | grep \"regex2\" とやることに相当 # foo という単語と bar という単語を含む行を見つける perl - wnl - e '/\\bfoo\\b/ and /\\bbar\\b/ and print;' file コンテキスト表示 段落モード -00 と ファイルモード -0777 を上手く使う /regex/s も上手く使うと良い 行をまたがるマッチングを行うとき、間にどんな文字を許すか考える必要がある。ニューラインについて言えば、 マッチ修飾子の s を使って . をニューラインにマッチさせる [\\t\\n]+ や [_\\s]+ というように、明示的に指定したり、[&#94;aiueo]+ などを使う s を使う sed 置換のデリミタ 対応する括弧も使用可能 s/.../.../ s |...|...| s{...}{...} , s(...)(...) 行指定置換, コンテキストアドレス sed では、次のように、置換の構文の前に、アドレスを指定して、置換する場所を制限できる。 # 2行目のみ置換 2s/regex/new/g # 2〜5行目のみ置換 2 ,5s/regex/new/g # 行頭が fff の行について置換 /&#94;fff/s/regex/new/g これを Perl では、行数に関する条件式で書く必要がある # 2行目のみ置換 perl - wpl - e '$. == 2 and s/regex/new/g;' file # 2〜5行目のみ置換 perl - wpl - e '2 <= $. and $. <= 5 and s/regex/new/g;' file # 行頭が fff の行について置換 perl - wpl - e '/&#94;fff/ and s/regex/new/g;' file 後方参照 sed では、キャプチャするときに、 ? を使えなかった。 Perl では、? を使える。 sed では、どこでも 1, 2, ...で参照したが、Perlでは 1, 2, ... と $1, $2, ... とどこから参照するかによって異なる。 # Mr か Mr. を見る perl - wnl - e 's/(Mr.?) (Fo[oa])/$1 Bar $2/g;' file 計算結果で置換 sed では、置換文字列フィールドに計算結果を入れることが難しかった。(できるか知らない。) perl では、簡単にできる。マッチ修飾子の e が必要。 perl - wnl - e 's/\\d+/$& * 2.1/ge;' file AWK フィールドアクセス AWK では、フィールドアクセスが便利 # スペースorタブ区切りのフィールドの順番を入れ替える awk '{ print $2, $1}' file Perl で似たようなことをやるには、2つ方法がある。 コマンドラインオプション -a (と合わせて、-F)を使う 自分で各フィールドを変数に代入する フィールドアクセス 構文 コメント ($A, $B)=@F; それぞれの変数にセット ($A, undef, $B)=@F; 第2フィールドは変数に入れない $numfields=@F; レコードのフィールド数を格納 パターン範囲 パターン範囲 演算子 構文 コメント .. regex1 .. regex2 regex1を含む最初のレコードからregex2を含む最初のレコードまでの範囲 一旦regex1とregex2の組が見つかると次のregex1を見つけるまで、無視 regex1とregex2が同じレコードにある場合もマッチ。 ... regex1 ... regex2 .. とほぼ同じ。 ただし、regex2はregex1の次の行以降から探す。 find ファイル属性テスト ファイル属性テスト (○ は属性に対応する文字) 構文 コメント -○ filename filename が○という属性を有していることをテスト ! -○ filename filename が○という属性を有していないことをテスト -○ $_ が○という属性を有していることをテスト ! -○ $_ が○という属性を有していないことをテスト ファイル属性と対応する演算子 ファイル属性 演算子 通常ファイル -f ディレクトリ -d シンボリックリンク -l 名前付きパイプ -p キャラクタ -c ブロック -b ソケット -S 空 -z 空でない -s 実UID/GIDで読取り可 -R 実UID/GIDで書込み可 -W 実UID/GIDで実行可 -X 実UIDが所有 -O 実効UID/GIDで読取り可 -r 実効UID/GIDで書込み可 -w 実効UID/GIDで実行可 -x 実効UIDが所有 -o 指定のUID/GIDが所有 stat setuid -u setgid -g sticky -k テキスト -T バイナリ -B 別のファイルより新しい stat 別のファイルより後にアクセスされた stat リンクの数 stat inode番号 stat","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/07/0239.html","loc":"https://www.hitsumabushi.org/blog/2014/01/07/0239.html"},{"title":"Perlスクリプトメモ","text":"参考文献 Perl基礎文法最速マスター ミニマル perl スクリプトを書く時 プラグマ perldoc.jp 推奨 : スクリプト先頭に書く use strict ; use warnings ; 1文字でも減らしたい print のデフォルト引数は、 $_ print $_; したいだけなら書く必要ない。 用語 レコード 1つのまとまりとして読み書きされる文字の集合 入出力の際、ファイルはレコードの集まりと見ることができる。 この時、デフォルトでは、1行がレコード。 入力レコードセパレータ 入力レコードの終端区切りを表す文字、または文字シーケンス（文字の並び）。 Linuxとかでは、デフォルトではラインフィード。 Perl ではOS固有の入力レコードセパレータを n として、参照できる。 ニューライン n のこと。 OS固有の入力レコードセパレータ。 スイッチ引数 -s オプション 良い感じのコマンドラインオプションを取ることができる。 引数の与え方で2つの方法がある。 - foo = 'test' #=> $foo == 'test' - foo #=> $foo == true : -debugで、デバッグ用、みたい使う。 スイッチ引数を任意にしているスクリプトでは、冒頭にour関数を使うのが良い。 our ( $foo ); our ( $foo , $bar , ... ); 真偽値 偽になるのは、次の3パターンある。 数値 0 と同等な値 文字列 null文字列 0 を保持する文字列 ( '0' みたいな) 変数 : 値を持たない変数 まだ設定されていない変数とか。定義されているかは、 defined を使うとわかる。 特別な文字列 $/, $\\ 入力,出力レコードセパレータを指定する $. 今読んでいる入力レコードが何番目か、を表す 通常は行数 コマンドラインオプションや$/で、入力レコードセパレータを指定したときは、行数じゃなくなる 複数のファイルを読んでいる場合、ファイルをまたいで番号付けらられる 以下のようにすることで、ファイルごとに $. の値をリセットできる # 各ファイル終了時にレコードカウンタの $. をリセット eof and close ARGV $_ 最後に読み込んだ入力レコード。 $\" ダブルクォートで囲んだ配列の各要素の間に挿入される文字 デフォルトはスペース $, print 文の2種類の特殊なフォーマット処理に利用 print の引数を区切るコンマの代わりに出力で使用される クォートされていない配列やハッシュの要素の区切り デフォルトは空文字 $& マッチした文字列の表示 $` マッチしたレコード中で最後のマッチの開始部分の前の部分、を保持 $' マッチしたレコード中で最後のマッチの末尾の後の部分、を保持 ARGV 現在の入力ソースのファイルハンドル。 特にクローズのために使われる。 $ARGV n, p オプションや空の入力演算子を使用すると、ファイル名が格納される。 STDIN の時は、 - が表示される。 @ARGV n, p や入力演算子を使わないときに、自前でプログラムの引数にアクセスするために使う。 <> \\n OS 固有のレコードセパレータ 変数 $var : 変数 宣言と代入 my $var = 'aaa' '' と \"\" シングルクォートとダブルクォートの違いは、変数が展開されるかどうか。 join (.), split, length, substr, index @array : 配列 宣言と代入 my @array @array = ( 1 , 2 , 3 ) $array[0] などとして、各値を得る $array[-2] というようにすれば、最後から2番目の意味 pop, push, shift, unshift も使える 配列の大きさを得るのは、 $array_size = @array などとする %hash : ハッシュ 宣言と代入 my %hash ; %hash = ( a => 1 , b => 2 ); $hash{a} などとして、各値を得る keys, values, exists, delete 制御構文 if { ~ } if { ~ } else { ~ } if { ~ } elsif { ~ } while ( 条件 ) { ~ } for ( my $i =0 ; $i < 5 ; $i++ ) { ~ } foreach my $field (@fields) { ~ } 配列の各要素に対して何かを行う foreach は for のエイリアスらしい。 比較演算子 文字列比較 $s eq $t $s ne $t $s lt $t $s gt $t $s le $t $s ge $t サブルーチン : 関数 他の言語との違いは、 引数を関数宣言に書かない @_ を使って引数にアクセス sub sub { my ( $num1 , $num2 ) = @_ ; my $result = $num1 - $num2 ; return $result ; } BEGIN, END 入力の処理の前や後に、処理を行う。 BEGIN は、Usage の表示に使ったりする。 END は、全体の結果・統計の計算・表示に使ったりする。 BEGIN { statements ; } END { statements ; } 文字列修飾子 以下の2箇所で使える。 ダブルクォート中の文字列 置換後文字列フィールド 文字列修飾子 修飾子 効果 \\U 右側の文字列を、\\E または文字列の末尾まで、大文字に変換 \\u 右側の文字を大文字に変換 \\L 右側の文字列を、\\E または文字列の末尾まで、小文字に変換 \\l 右側の文字を小文字に変換 \\E 変換を終了 論理演算 and, or and の方が or より優先度が高い。 $one or $two and $three # => $one or ( $two and $three ) ! 論理否定。 関係演算子 関係演算子 数値 文字列 意味 == eq 等しい != ne 等しくない > gt より大きい >= ge 以上 < lt より小さい <= le 以下 <=> cmp 比較 -1 : 左辺が小さい 0 : 等しい 1 : 左辺が大きい 以下は、組み込み関数 IOに関連する関数 print STDOUT に出力する printf print のニューラインなしバージョン : -l の影響を受けない 他にもたくさん使い勝手が良い warn STDERR にメッセージを送る。 # 出力メッセージのあと、更にエラー行が付加される。 warn \"msg\\n\" # \\n を入れると簡潔なエラーメッセージ : エラー行などを表示しない。 warn \"msg\\n\" print STRERR \"a\" , \"b\" die STDERR にメッセージを送って、プロセスを終了する。 出力メッセージの形式は warn と同様。 デフォルトのエラーコードは255。 # BEGIN ブロックの外で die \"$0 : msg\\n\" # BEGIN ブロックの中で $success or warn \"msg\\n\" and exit 255 ; close ファイルハンドルを引数に取って、ファイルを閉じる。 close ARGV eof 現在のファイルを最後まで読み込んだら、真を返す。 # 各ファイル終了時にレコードカウンタの $. をリセット eof and close ARGV dbmopen, dbmclose flock format 文字列操作 chomp chop chr crypt hex index lc, lcfirst, uc, ucfirst それぞれ \\L, \\l, \\U, \\u length 文字列の長さ substr 文字列の切り出し 添字は 0 から始まる # 文字列先頭から、4文字目まで substr $_ , 0 , 3 ; その他 defined undef dump eval formline time, gmtime, localtime local pos scalar reset system","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/07/0238.html","loc":"https://www.hitsumabushi.org/blog/2014/01/07/0238.html"},{"title":"VPLSについてのメモ","text":"What is VPLS Virtual Private LAN MACフレームを、ルーターを含むネットワークを超えて、やり取りするための技術の1つ MPLSはIPにラベルをつけるが、VPLSはMACにラベルをつけるというイメージ ラベルを使う理由としては、元々、経路計算を高速にしたかったはずだけど、今はそんなに気にしなくて良いっぽい Why VPLS MPLSが便利なのと、同じ。 VPN TE(Trafic Engineering) 明示的な経路選択 回線使用率から選択したり。 ループフリー&自由なネットワークトポロジー 障害検知&切り替え FRR(Fast ReRoute) 代替経路へ即座に切り替えが起こる 1 sec以内で切り替えられるらしい 参考サイト RFC 4761, 4762 http://itpro.nikkeibp.co.jp/article/COLUMN/20070130/259589/ http://www.itbook.info/study/mpls2.html MPLSについては以下が参考になる http://www.atmarkit.co.jp/fnetwork/tokusyuu/11mpls/mpls01.html","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/06/0243.html","loc":"https://www.hitsumabushi.org/blog/2014/01/06/0243.html"},{"title":"便利なツール","text":"POD Plain Old Documentation man perldoc を参照 lwp-request Webサーバーにリクエストを送って、適当な形式に変換してくれる GET, POST, PUT も選べる。 出力形式としては、 text, ps, links, html, dump lwp - request - o text www . example . com Text::Autoformat Text::Tabs タブをスペースに変換 String::Approx あいまいなマッチをする Template-Toolkit Lingua::En::Inflect 'PL_N' 単数形と複数形を変換してくれる","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/02/0240.html","loc":"https://www.hitsumabushi.org/blog/2014/01/02/0240.html"},{"title":"PXEブートでインストーラを起動する","text":"最近、VMwareのESXiで仮想環境を作っています。 触っていて気づいたのですが、空の仮想マシンを起動するとPXEブートを試みてくれます。 この仕様をうまく使いたいなーと思って、PXEブートでインストールを自動化する方法を調べました。 PXEブートしてインストールを自動化するまでの手順を簡単に書いていきます。 Debianをインストールサーバーとして予め立てておき、DebianまたはCentOSをインストールすることにします。 1. Debian Install 2. DHCPサーバーを立てる 3. TFTPサーバーを立てる 4. PXEブート用のイメージを準備する 5. 中間チェック ブート画面でOSを選択できるようにする 6. CentOSのインストーラを準備 7. メニューの作成 Debianインストールの自動化 8. preseed ファイル作成 9. pxelinux.cfg/default 書き換え 10. Debianインストールのチェック Cent OSインストールの自動化 11. httpサーバー作成 12. CentOSイメージのマウント 13. kickstartファイルの準備 14. kickstartファイルの配置 15. CentOSインストールのチェック もっと自動化できるけど... さらに読もうと思ってるサイト 1. Debian Install 最初のDebianはCDやらで手動でインストールしてください。 Debianのインストール手順は省略します。 2. DHCPサーバーを立てる # Install DHCP Server apt-get install isc-dhcp-server # Config DHCP : INTERFACES vi /etc/default/isc-dhcp-server # Modify INTERFACES=\"\" to INTERFACES \"eth1\" # Config DHCP : Server setting vi /etc/dhcp/dhcpd.conf 編集する箇所は以下の通りです。 https://gist.github.com/10664868 DHCPサーバーを再起動して設定を反映させます。 # Restart DHCP Server /etc/init.d/isc-dhcp-server restart 3. TFTPサーバーを立てる デフォルトで大丈夫でした。 # Install TFTP Server apt-get install tftpd-hpa 4. PXEブート用のイメージを準備する 以下のページから適当にnetbootイメージを選んで、netboot/netboot.tar.gzを取って来ます。 http://www.debian.org/distrib/netinst#netboot # 展開する tar xvf netboot.tar.gz # move files to root dir of TFTP mv ./* /srv/tftp 5. 中間チェック この段階でPXEブートしてみると、Debianのインストーラが立ち上がるようになります。 ブート画面でOSを選択できるようにする 6. CentOSのインストーラを準備 mkdir -p /srv/tftp/centos/6.5/x86_64 CentOSのインストールDVDを持って来る mount -o loop <CentOS install ISO> /media /media/images/pxeboot/にある次の2つのファイルを/srv/tftp/centos/6.5/x86_64/以下にコピーしてくる initrd.img vmlinuz umount /media ここで、Debianのインストーラたちも整理して、 /srv/tftp/debian/7.4/amd64 以下にあるとします。: /srv/tftp/ | |-debian/7.4/amd64/ | | | |-boot-screens | | | |-initrd.gz | | | |-linux | | | |-pxelinux.0 | | | |-pxelinux.cfg/default | |-centos/6.5/x86_64 | | | |-initrd.img | | | |-vmlinuz | |-pxelinux.0 -(シンボリックリンク)-> debian/7.4/amd64/pxelinux.0 | |-pxelinux.cfg -(シンボリックリンク)-> debian/7.4/amd64/pxelinux.cfg 7. メニューの作成 pxelinux.cfg/defaultの内容を以下のように編集します。 後でもう少し修正します。 最終形はGistにあがっているので 、 そちらを見てください。 # D-I config version 2.0 default debian/7.4/amd64/boot-screens/vesamenu.c32 # Boot Menuに入りたい prompt 1 timeout 300 menu title - Boop Options Menu - label Debian-7.4 menu label &#94;0 Debian 7 .4 #include debian/7.4/amd64/boot-screens/menu.cfg kernel debian/7.4/amd64/linux append priority = critical vga = 788 initrd = debian/7.4/amd64/initrd.gz label CentOS-6.5 menu label &#94;1 CentOS 6 .5 kernel centos/6.5/x86_64/vmlinuz append initrd = centos/6.5/x86_64/initrd.img Debianインストールの自動化 ここまでで、PXEブートさえしていれば、複数のOSを選んでインストールできるようになりました。 いまのところの問題点としては、結局インストーラを起動した後は手作業になっていることです。 次は、PXEブートした後、放っておくと勝手にインストールが終わっているようにしましょう。 Debianの場合にこれを実現するには、preseedと呼ばれる設定ファイルを書けば良いです。 普通にインストールするときに色々な質問に答える必要があると思いますが、その答えを事前に書いておくものです。 ただ、preseedが読まれる前に、インストーラから質問される(インストールのタイプなど)ことがあるので、そのあたりはブート時のパラメータとして渡しましょう。 8. preseed ファイル作成 preseedファイルはかなり長いです。 昔はpreseedのドキュメントを読んでも、作るの大変だった印象があったのですが、 コメント付きのpreseedの例がある ので、これを元に書いて行けば特に大きくは困らないです。 ただ、例に書かれていないのですが、GRUBの設定を追加しないといけないのはわかりづらいかも知れません。 preseedで細かく設定しようとすると大変なので、それはAnsibleなどインストール終わってからの自動化に入れる方針です。 今回はパーティションなどは全部インストーラに任せています。 パーティションを変える場合などは、コメントアウトされているところを変更してください。 https://gist.github.com/10664868 の preseed.cfgを参照。 9. pxelinux.cfg/default 書き換え 今書いたpreseedファイルを debian/7.4/preseed.cfg として配置したことにします。 ブートオプションなどを追加した pxelinux.cfg/default ファイルは以下のようになります。 あとでどうせCentOSも自動化するので、そちらもあわせてオプションをつけておきます。 https://gist.github.com/10664868 の defaultを参照。 10. Debianインストールのチェック また実際にPXEブートしてみましょう。 Debianをメニューで選択すると、今度は勝手に画面が進んで行くと思います。 最後までインストールが終わり、勝手にリブートされてこれば、成功です。s Cent OSインストールの自動化 11. httpサーバー作成 後でやるように、kickstartファイルはhttp経由で配ろうと思います。(TFTPでできないっぽかった) なので、httpサーバーを予め立てておくことにします。 # apache install apt-get install apache2 12. CentOSイメージのマウント mkdir -p /var/www/images/centos mount -o loop <CentOS install ISO> /var/www/images/centos 13. kickstartファイルの準備 kickstartファイルの詳細については、 redhatのページ が参考になります。というか、コレ見たら大体で書ける。 今回使っているkickstartfileは以下の通りです。 https://gist.github.com/10664868 の kickstart.cfgを参照。 14. kickstartファイルの配置 pxelinux.cfg/defaultファイルの内容に合わせて、kickstart.cfgを配置します。 今回は、apacheのドキュメントルート直下に置けば良いようにしました。 mv kickstart.cfg /var/www/ 15. CentOSインストールのチェック 最後に、CentOSのインストールを確かめます。 実際にPXEブートして、CentOSをメニューで選択すると、勝手に画面が進んで行くと思います。 最後までインストールが終わり、勝手にリブートされてこれば、成功です。 もっと自動化できるけど... 今回は、インストールの自動化を行いました。 一応、インストールするOSの種類を選ぶところを手でやることにしましたが、普通はOSは一種類で良いか、MACアドレスでインストールするOSを分けてしまうのだと思います。 そうしておけば、OSの種類を選ぶところも自動化することができます。 自分としては、基本はDebianをインストールしたくて、たまにCentOSも選びたいという程度なので、ここまでで止めておきます。 機会があれば、preseedやkickstartでホスト依存な部分(例えば固定IPとか)を都度生成してインストールする方法も考えたいですね。 さらに読もうと思ってるサイト http://d.hatena.ne.jp/fujisan3776/20100630/1277861431 インストール中は別の作業をしているので、インストールが終わったことも知らせて欲しい","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/01/0244.html","loc":"https://www.hitsumabushi.org/blog/2014/01/01/0244.html"},{"title":"実UID, 実行UID","text":"プロセスはいくつかのIDを持っている。 あまり深く考えず、実UIDと実行UIDについて考える。 実UID(UID) 呼び出し元のプロセス(親プロセス)の実ユーザーID。 普通にログインして、何かプロセスをシェルから走らせる場合、ログインシェルのUIDがログインユーザーIDなので、ログインユーザーIDになる。 実効UID 呼び出し元のプロセス(親プロセス)の実効ユーザーID。 ファイルアクセスに影響。 普通にログインして、何かプロセスをシェルから走らせる場合、ログインシェルのUIDがログインユーザーIDなので、ログインユーザーIDになる。 ただし、SUID ビットがあるときは、それに従う。 SUID, SGID, スティッキービット SUID, SGID それぞれ setuid, setgidのこと。 まず、 SUID(SGID) ビットが立っているファイルについて見る。 $ ls -l -rwsr-xr-x 1 root root 51096 5月 26 2012 /usr/bin/passwd このように、sというビットがユーザーパーミッションを表す部分(SGIDのときは、グループパーミッション部分)に立っている。 さて、パーミッションを見ると passwd はどんなユーザーでも実行できる。 実行した場合、実効UIDがファイルの所有ユーザーとして実行される。(SGIDビットが立っているときは、所有グループが実効GIDになる。) この仕組みが必要な理由は、例えば、一般ユーザーが自身のパスワードを変更するために、/etc/shadow を書き換える必要がある。しかし、一般ユーザー権限では書き換えることができない。これを解決するためである。 SUID, SGID ビットの設定は以下のようにする。 # SUID ビットを立てる $ chmod 4770 foo # SGID ビットを立てる $ chmod 2770 bar # 別の方法として、 +sx を使う方法がある # SUID $ chmod u+sx foo # SGID $ chmod g+sx bar スティッキービット (元々は違うけど、)ディレクトリ配下のファイルの削除についての設定。 典型例は /tmp。 このディレクトリの持つ特徴を考えてみよう。 誰でもファイルを作成できる。 自分の作ったファイルを削除できる。 ここまでであれば、 # /tmp : 誰でもファイルを作成できる $ sudo chmod 777 tmp # 試しに、適当なファイルを作成・削除できるか試す $ cd tmp $ touch user $ rm user # root ユーザーでもファイルを作成してみる $ sudo touch root 他のユーザーのファイルは一般ユーザーは削除できない。 # 先ほどの tmp ではこれが満たされていない $ rm root #=> 消える!! これは当たり前で、tmp のパーミッションが 777。ディレクトリの write ができるということは、ファイルの新規作成・削除ができる。 では、 tmp はどのように実現すれば良いだろうか。 そんな時に、スティッキービットが役に立つ。 # スティッキービットを立てる $ sudo chmod 1777 ../tmp # 別の方法として、 +t を使う方法がある。 $ sudo chmod a+rwxt ../tmp スティッキービットをディレクトリに立てておくと、ファイルやディレクトリの所有者しか削除できなくなる。 まとめ 実効UID, 実効GID でファイルアクセスの管理がなされる SUID, SGIDビットを使って、実行したユーザーと実効UID, 実効GID を変更できる。 スティッキービットを使えば、誰でも新規にファイルを作れるが、消すのは本人のみ、というディレクトリを作成できる。 参考ページ UNIXの部屋 コマンド検索: chmod","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/01/0243.html","loc":"https://www.hitsumabushi.org/blog/2014/01/01/0243.html"},{"title":"DNS サーバー","text":"ニフティクラウド上でサーバーを立てた。 いろいろと設定していこうかと思っているが、まずはDNSから始める。 作業内容 1. /etc/bind/named.conf.options 以下のように書き換える: // v6の設定： v6は応答しない // listen-on-v6 { any; }; listen-on-v6 { none; }; // transfer を許すIPを制限 allow-transfer \"192.168.100.1\"; 2. /etc/bind/named.conf.local ゾーンの設定: zone \"example.com\" { type master; file \"example.db\"; }; zone \"100.168.192.in-addr.arpa\" { type master; file \"192.rev\" };","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/01/0241.html","loc":"https://www.hitsumabushi.org/blog/2014/01/01/0241.html"},{"title":"Perl のコンパイラオプション","text":"参考ページ man perlrun DebugIto's - Perl/コマンドライン 基本的なオプション -e ワンライナーを書くために必須。 複数並べられるので、それなりに色々書ける 直後にperlプログラムを書ける。 perl - e 'print 11/2' -l[8進数] 行末に指定された8進数に変える。-l のみの場合、改行になる。 perl - e 'print 22' #=> 22 perl - l - e 'print 22' #=> 22\\n perl - l101 - e 'print 22' #=> 22A -0[digits] -l の入力セパレータバージョン -00 とすれば、空行を区切り。段落モード。 $/ = \"\"; -777 とすればファイルごとの区切りになる。 使い勝手の良いオプション -n while (<>) { ...} の中にスクリプトを入れたと思って実行 -p -n みたいなもの。ただし、行($_)を出力する。 sed に似たやつ。 -a -n, -p と一緒に使用 $_ を自動的に改行で分割し、配列 @F に入れる perl - ane 'print $F[0], \"\\n\"' /etc/ passwd -F/pattern/ -a の分割のデリミタを指定する パターンは複数文字でも良い。そのときは、' ' または / / で囲む perl - F ':' - ane 'print $F[0],\":\",$F[2] ,\"\\n\"' /etc/ passwd -i[拡張子] 拡張子がないとき、<>のファイルを直接編集する 拡張子があるとき、<>の元ファイルは、拡張子をつけたファイルにバックアップされる。 $SECONDS を使うのがオススメ perl - i . back - pe `s/foo/bar/g' sample.txt perl -i.$SECONDS -pe ` s/foo/bar/g ' sample . txt デバッグに使えるオプション -c スクリプトのシンタックスチェックのみ行う -d[:debugger] debugger の下で走らせる -w より多くの Warning を出す -W (-X) すべての Warning を出す(出さない)","tags":"blog","url":"https://www.hitsumabushi.org/blog/2014/01/01/0237.html","loc":"https://www.hitsumabushi.org/blog/2014/01/01/0237.html"}]};